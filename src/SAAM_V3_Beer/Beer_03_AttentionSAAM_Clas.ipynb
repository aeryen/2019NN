{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;34m/home/aeryen/anaconda3/lib/python3.7/ast.py\u001b[0m, in \u001b[0;32mparse\u001b[0m:\nLine \u001b[0;34m35\u001b[0m:    \u001b[34mreturn\u001b[39;49;00m \u001b[36mcompile\u001b[39;49;00m(source, filename, mode, PyCF_ONLY_AST)\n",
      "\u001b[0;31mSyntaxError\u001b[0m: invalid syntax (<string>, line 1)\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/aeryen/2019nn-beer/5196a9f232624eb79d3870117a9f6ba7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "experiment = comet_ml.Experiment(project_name=\"2019nn_beer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from fastai.text import *\n",
    "from data_helpers.Data import *\n",
    "from fastai.text.transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    \"max_sequence_length\": 20*70,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs1\": 12,\n",
    "    \"num_epochs2\": 15,\n",
    "    \"num_aspect\": 5,\n",
    "    \"num_rating\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LM Databunch and LM Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_db = load_data(\"./data/\", \"hotel_lm_databunch.1001\")\n",
    "# lm_learn = language_model_learner(lm_db, AWD_LSTM)\n",
    "# lm_learn = lm_learn.load(\"lang_model_hotel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_learn.save_encoder('lang_model_hotel_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_db = load_data(\"./data/\", \"beer_clas_databunch_rint.TraValTes\")\n",
    "cls_db.batch_size = hyper_params[\"batch_size\"]\n",
    "cls_db.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj watching some football tonight ( xxup xxunk ) and having the last of my 2005 and 2006 xxmaj bigfoots ( xxmaj bigfeet ? ? ) xxperiod xxmaj lots of malt in there xxperiod toffee and caramel xxperiod very xxmaj english - like these days xxperiod quite tasty xxperiod xxmaj just one 2004 left xxperiod :-( xxmaj cheers ! ! xxperiod * * * * * * xxperiod xxmaj</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos i had tried this once before , but the bottle i had purchased was smothered in dust and might as well have been in a petrified state by the time i got to it xxperiod xxmaj despite its robust 8 xxperiod 3 % abv , it is still a bit too weak to age gracefully , and it was clear that the flavor had been compromised , calling for</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos i feel strange reviewing this brew first before reviewing any other xxmaj founders offerings , because they really do a great job and i feel like by not giving this an xxup a+ i 'm understating how fantastic this brewery is on the whole xxperiod xxmaj but i have to keep the style and other examples i 've had of the style in mind , so here 's my</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos i picked up a bottle of xxmaj bigfoot xxmaj barleywine xxmaj style xxmaj ale for $ 1 xxperiod 99 at xxmaj the xxmaj lager xxmaj mill xxperiod i have been wanting to try this beer for a while since i heard it was one the better xxmaj barleywines available , but every time i thought to pick one up they were sold out for the season , but i</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj re - review xxperiod xxmaj turns out the first bottles of this i had was from an infected batch xxperiod i had it before the infection really took off but it would explain the lactic quality i was picking up xxperiod 12 oz bottled 2nd week of xxmaj november 2010 xxperiod xxmaj served at cellar temperature xxperiod xxmaj pours dark brown / black with a large light brown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<display.HTML object at 0x7ffa6d59aa70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_db.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = cls_db.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1347])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Feature Combo Pooling (1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_combo(output, start, end):\n",
    "    avg_pool = output[start:end, :].mean(dim=0)\n",
    "    max_pool = output[start:end, :].max(dim=0)[0]\n",
    "    x = torch.cat([output[-1, :], max_pool, avg_pool], 0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def sentence_pool_1200(outputs, mask, p_index):\n",
    "    output = outputs[-1]\n",
    "    seq_max = output.size(1)\n",
    "    doc_start = mask.int().sum(dim=1)\n",
    "\n",
    "    batch = []\n",
    "    for doci in range(0, output.shape[0]):\n",
    "        pi = p_index[doci, :].nonzero(as_tuple=True)[0].int()\n",
    "        doc = []\n",
    "        for senti in range(len(pi)):\n",
    "            if senti == 0:\n",
    "                doc.append(pool_combo(output[doci, :, :], doc_start[doci], pi[senti]))\n",
    "            else:\n",
    "                doc.append(pool_combo(output[doci, :, :], pi[senti - 1] + 1, pi[senti]))\n",
    "\n",
    "        batch.append(torch.stack(doc, 0))\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def sentence_pool_400(output:Tensor, mask, p_index):\n",
    "    batch = []\n",
    "    for doci in range(0, output.shape[0]):\n",
    "        doc = output[doci, p_index[doci, :], :]\n",
    "        batch.append(doc)\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def masked_concat_pool(outputs, mask):\n",
    "    \"Pool MultiBatchEncoder outputs into one vector [last_hidden, max_pool, avg_pool].\"\n",
    "    output = outputs[-1]\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).mean(dim=1)\n",
    "    avg_pool *= (\n",
    "        output.size(1) / (output.size(1) - mask.type(avg_pool.dtype).sum(dim=1))[:, None]\n",
    "    )\n",
    "    max_pool = output.masked_fill(mask[:, :, None], -float(\"inf\")).max(dim=1)[0]\n",
    "    x = torch.cat([output[:, -1], max_pool, avg_pool], 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(Module):\n",
    "    \"Create an encoder over `module` that can process a full sentence.\"\n",
    "    \n",
    "    def __init__(self, bptt:int, max_len:int, module:nn.Module, vocab, pad_idx:int=1):\n",
    "        print(\"Encoder initing\")\n",
    "        self.max_len,self.bptt,self.module,self.pad_idx = max_len,bptt,module,pad_idx\n",
    "        print(\"max len \" + str(self.max_len))\n",
    "        print(\"bptt \" + str(self.bptt))\n",
    "        print(\"pad_idx \" + str(self.pad_idx))\n",
    "        self.vocab = vocab\n",
    "        self.period_index = self.vocab.stoi[\"xxperiod\"]\n",
    "        print(\"period index \" + str(self.period_index))\n",
    "\n",
    "    def concat(self, arrs:Collection[Tensor])->Tensor:\n",
    "        \"Concatenate the `arrs` along the batch dimension.\"\n",
    "        return [torch.cat([l[si] for l in arrs], dim=1) for si in range_of(arrs[0])]\n",
    "\n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()\n",
    "\n",
    "    def forward(self, input:LongTensor)->Tuple[Tensor,Tensor]:\n",
    "        bs,sl = input.size()\n",
    "        self.reset()\n",
    "        \n",
    "        bptt = self.bptt\n",
    "        if np.random.random() > 0.95:\n",
    "            bptt = 50\n",
    "        \n",
    "        raw_outputs,outputs,masks = [],[],[]\n",
    "        p_index = []\n",
    "        for i in range(0, sl, bptt):\n",
    "            r, o = self.module(input[:,i: min(i+bptt, sl)])\n",
    "            if i>(sl-self.max_len):\n",
    "                masks.append(input[:,i: min(i+bptt, sl)] == self.pad_idx)\n",
    "                raw_outputs.append(r)\n",
    "                outputs.append(o)\n",
    "                p_index.append( input[:,i: min(i+bptt, sl)] == self.period_index )\n",
    "\n",
    "#         print(\"number of sentences in docs:\")\n",
    "#         n_sent = torch.sum( x==self.vocab.stoi[\"xxperiod\"] , dim=1)\n",
    "#         print(n_sent)\n",
    "        \n",
    "        period_index = torch.cat(p_index,dim=1)\n",
    "        \n",
    "        return self.concat(raw_outputs),self.concat(outputs), \\\n",
    "               torch.cat(masks,dim=1),period_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BI_AWD_LSTM(AWD_LSTM):\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, n_hid:int, pad_token:int=1, hidden_p:float=0.2,\n",
    "                 input_p:float=0.6, embed_p:float=0.1, weight_p:float=0.5, bidir:bool=False):\n",
    "        self.bs,self.emb_sz,self.n_hid = 1,emb_sz,n_hid\n",
    "        self.n_dir = 2 if bidir else 1\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
    " \n",
    "        self.rnns = [\n",
    "            nn.LSTM(emb_sz, n_hid//self.n_dir, 1,\n",
    "                 batch_first=True, bidirectional=bidir),\n",
    "            nn.LSTM(n_hid, n_hid//self.n_dir, 1,\n",
    "                 batch_first=True, bidirectional=bidir)\n",
    "        ]\n",
    "        \n",
    "        self.rnns.extend( [ nn.LSTM(n_hid, emb_sz//self.n_dir, 1,\n",
    "                                    batch_first=True, bidirectional=bidir) ] * 2 )\n",
    "        \n",
    "        for i in range(len(self.rnns)):\n",
    "            self.rnns[i] = WeightDropout(self.rnns[i], weight_p)\n",
    "        \n",
    "        self.rnns = nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        if self.encoder.padding_idx is not None:\n",
    "            self.encoder.weight.data[self.encoder.padding_idx] = 0.\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(len(self.rnns))])\n",
    "        \n",
    "    def forward(self, input:Tensor, from_embeddings:bool=False)->Tuple[List[Tensor],List[Tensor]]:\n",
    "        if from_embeddings:\n",
    "            bs,sl,es = input.size()  #  batchsize, seqlen, embsize\n",
    "        else:\n",
    "            bs,sl = input.size()     #  batchsize, seqlen\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(input if from_embeddings else self.encoder_dp(input))\n",
    "        \n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns[:-2], self.hidden_dps[:-2])):  #  go through rnn and its dp\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])  #  use previous hidden state\n",
    "            new_hidden.append(new_h)                    #  store hidden for next batch\n",
    "            raw_outputs.append(raw_output)              #  raw_outputs = lstm out before drop\n",
    "            raw_output = hid_dp(raw_output)             #  drop outputs for next layer use\n",
    "            outputs.append(raw_output)                  #  outputs = after drop\n",
    "        \n",
    "        raw_output_1, hidden_1 = self.rnns[-2](raw_output, self.hidden[-2])  #  work on dropped 2nd layer\n",
    "        new_hidden.append(hidden_1)\n",
    "        raw_outputs.append(raw_output_1)\n",
    "        outputs.append(raw_output_1)\n",
    "        \n",
    "        raw_output_2, hidden_2 = self.rnns[-1](raw_output, self.hidden[-1])  #  work on dropped 2nd layer\n",
    "        new_hidden.append(hidden_2)\n",
    "        raw_outputs.append(raw_output_2)\n",
    "        outputs.append(raw_output_2)\n",
    "        \n",
    "        self.hidden = to_detach(new_hidden, cpu=False)  #  store state for stateful lstm\n",
    "        return raw_outputs, outputs\n",
    "    \n",
    "    def _one_hidden(self, l:int) -> Tensor:\n",
    "        \"Return one hidden state.\"\n",
    "        nh_list = [self.n_hid, self.n_hid, self.emb_sz, self.emb_sz]  #  1152, 1152, 400, 400\n",
    "        nh = nh_list[l] // self.n_dir\n",
    "        return one_param(self).new(self.n_dir, self.bs, nh).zero_()\n",
    "\n",
    "    def select_hidden(self, idxs):\n",
    "        self.hidden = [(h[0][:,idxs,:],h[1][:,idxs,:]) for h in self.hidden]\n",
    "        self.bs = len(idxs)\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
    "        #  (torch.Size([1, 32, 1152]), torch.Size([1, 32, 1152]))\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range( len(self.rnns) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emb_sz': 400, 'n_hid': 1152, 'n_layers': 3, 'pad_token': 1, 'qrnn': False, 'bidir': False, 'output_p': 0.4, 'hidden_p': 0.3, 'input_p': 0.4, 'embed_p': 0.05, 'weight_p': 0.5}\n",
      "{'emb_sz': 400, 'n_hid': 1152, 'pad_token': 1, 'bidir': False, 'hidden_p': 0.3, 'input_p': 0.4, 'embed_p': 0.05, 'weight_p': 0.5}\n"
     ]
    }
   ],
   "source": [
    "meta = text.learner._model_meta[AWD_LSTM].copy()\n",
    "config = meta[\"config_clas\"].copy()\n",
    "print(config)\n",
    "config.pop(\"output_p\")\n",
    "config.pop(\"qrnn\")\n",
    "config.pop(\"n_layers\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BI_AWD_LSTM(\n",
       "  (encoder): Embedding(31600, 400, padding_idx=1)\n",
       "  (encoder_dp): EmbeddingDropout(\n",
       "    (emb): Embedding(31600, 400, padding_idx=1)\n",
       "  )\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDropout(\n",
       "      (module): LSTM(400, 1152, batch_first=True)\n",
       "    )\n",
       "    (1): WeightDropout(\n",
       "      (module): LSTM(1152, 1152, batch_first=True)\n",
       "    )\n",
       "    (2): WeightDropout(\n",
       "      (module): LSTM(1152, 400, batch_first=True)\n",
       "    )\n",
       "    (3): WeightDropout(\n",
       "      (module): LSTM(1152, 400, batch_first=True)\n",
       "    )\n",
       "  )\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0): RNNDropout()\n",
       "    (1): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sz = len(cls_db.vocab.itos)\n",
    "m = BI_AWD_LSTM(vocab_sz, **config)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1347])\n",
      "torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "x, y = cls_db.one_batch()\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_outputs, outputs = m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1347, 400]), torch.Size([32, 1347, 400]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2].shape, outputs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder initing\n",
      "max len 1400\n",
      "bptt 70\n",
      "pad_idx 1\n",
      "period index 9\n"
     ]
    }
   ],
   "source": [
    "encoder = SentenceEncoder(70, hyper_params[\"max_sequence_length\"], m, cls_db.vocab, pad_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_outputs,outputs,mask,p_index = encoder( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1347, 1152])\n",
      "torch.Size([32, 1347, 1152])\n",
      "torch.Size([32, 1347, 400])\n",
      "torch.Size([32, 1347, 400])\n"
     ]
    }
   ],
   "source": [
    "for o in outputs:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLS 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.add_tag(\"CLAS02\")\n",
    "# experiment.add_tag(\"FULLIND400\")\n",
    "\n",
    "# LSTM ATTENTION\n",
    "class Cls02ATT_BILSTM(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int):\n",
    "        print(\"CLS init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp + 1\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        self.asp_hidden = 100\n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 400, self.asp_hidden, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( self.asp_hidden, self.n_asp, p=0.1, actn=nn.Sigmoid() ) # actn=torch.nn.ReLU(dim=1)\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        self.smt_hidden = 20\n",
    "        self.s0 = nn.Sequential(* (bn_drop_lin( 400, self.smt_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( self.smt_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s1 = nn.Sequential(* (bn_drop_lin( 400, self.smt_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( self.smt_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s2 = nn.Sequential(* (bn_drop_lin( 400, self.smt_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( self.smt_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s3 = nn.Sequential(* (bn_drop_lin( 400, self.smt_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( self.smt_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s4 = nn.Sequential(* (bn_drop_lin( 400, self.smt_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( self.smt_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s5 = nn.Sequential(* (bn_drop_lin( 400, self.smt_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( self.smt_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "\n",
    "        self.sentiments = []\n",
    "        self.sentiments.append( self.s0 )\n",
    "        self.sentiments.append( self.s1 )\n",
    "        self.sentiments.append( self.s2 )\n",
    "        self.sentiments.append( self.s3 )\n",
    "        self.sentiments.append( self.s4 )\n",
    "        self.sentiments.append( self.s5 )\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "        \n",
    "        batch_sent_emb_asp = sentence_pool_400(outputs[3], mask, p_index)  #  list of size batch, each [n_sent, emb]\n",
    "        batch_sent_emb_smt = sentence_pool_400(outputs[2], mask, p_index)  #  list of size batch, each [n_sent, emb]\n",
    "        \n",
    "        sent_emb_asp = torch.cat(batch_sent_emb_asp, dim=0)          # aspects [n_sentence, emb400]\n",
    "        sent_dist_asp = self.aspect(sent_emb_asp)                    # [n_sentence, aspect6]\n",
    "        \n",
    "        sent_emb_smt = torch.cat(batch_sent_emb_smt, dim=0)          # sentiments [n_sentence, emb400]\n",
    "\n",
    "        sent_bmm = torch.bmm(sent_dist_asp.unsqueeze(2), sent_emb_smt.unsqueeze(1))  # [n_sentence, asp, emb400]\n",
    "        \n",
    "        all_doc_emb = []\n",
    "        aspect_doc = []\n",
    "        sentim_doc = []\n",
    "        cur = 0\n",
    "        for doci in range(0, len(batch_sent_emb_asp)):\n",
    "            sn = batch_sent_emb_asp[doci].shape[0]                                       #  number of sent in this doc\n",
    "            doc_emb_avg = torch.sum(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True)  #  [1, 7, 400]\n",
    "            asp_w_sum = torch.sum(sent_dist_asp[cur:(cur+sn),:], dim=0, keepdim=True)    #  [1, 7]\n",
    "            doc_emb_avg = doc_emb_avg / asp_w_sum[:,:,None]                              #  [1, 7, 400]\n",
    "            all_doc_emb.append( doc_emb_avg )\n",
    "            aspect_doc.append( sent_dist_asp[cur:(cur+sn), :] )\n",
    "            \n",
    "            cur = cur + sn\n",
    "\n",
    "        all_doc_emb = torch.cat( all_doc_emb, dim=0 )          # [batch, asp, 400]\n",
    "        \n",
    "        result_senti = [ self.sentiments[aspi]( all_doc_emb[:,aspi,:] ) for aspi in range(0,self.n_asp) ]  #  [batch, ra]\n",
    "        \n",
    "        result = torch.stack(result_senti, dim=1)  # [batch, asp, sentiment5]\n",
    "        \n",
    "        return result,raw_outputs,outputs,aspect_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_tag(\"CLAS02\")\n",
    "experiment.add_tag(\"LIATLI\")\n",
    "\n",
    "# ATTENTIONAL AVERAGING, COMPLETELY INDEPENDENT SENTI OUT\n",
    "class Cls02_LIATLI(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int):\n",
    "        print(\"CLS init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp + 1\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        self.doc_mapper = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        self.asp_hidden = 40\n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 800, self.asp_hidden, p=0.5, actn=nn.LeakyReLU(inplace=True) )  #  inplace=True\n",
    "        mod_layers += bn_drop_lin( self.asp_hidden, self.n_asp, p=0.15, actn=nn.Sigmoid() )  #  actn=nn.Softmax(dim=1)\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        self.smt_hidden = 300\n",
    "        self.first_fn = nn.Sequential( * ( bn_drop_lin( 400, self.smt_hidden * self.n_asp, p=0.5,\n",
    "                                                       actn=nn.LeakyReLU(inplace=True) ) ) )\n",
    "#         self.second_fn = nn.ModuleList(\n",
    "#             [ nn.Sequential( * ( bn_drop_lin( self.smt_hidden, self.n_rat, p=0.3, actn=None) ) )\n",
    "#              for i in range(self.n_asp) ]\n",
    "#         )\n",
    "        self.second_fn = nn.Sequential(\n",
    "            * ( bn_drop_lin( self.smt_hidden, 80, p=0.4, actn=nn.LeakyReLU(inplace=True)) +\n",
    "                bn_drop_lin( 80, self.n_rat, p=0.2, actn=None) )\n",
    "        )\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor]) -> Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "        \n",
    "        batch = sentence_pool_400(outputs[-1], mask, p_index)  #  list of size batch, each [n_sent, emb]\n",
    "        \n",
    "        temp1 = batch[0].mean(dim=0)\n",
    "        temp1 = temp1.unsqueeze(0)\n",
    "        temp1 = temp1.expand(10,-1)\n",
    "        \n",
    "        doc_emb = [doc.mean(dim=0).unsqueeze(0).expand(doc.shape[0],-1) for doc in batch]\n",
    "        doc_emb = torch.cat(doc_emb, dim=0)\n",
    "        \n",
    "        allsent_emb = torch.cat(batch, dim=0)          #  [n_sentence, emb400]\n",
    "        asp_emb = torch.cat([allsent_emb,doc_emb], dim=1)\n",
    "        aspect_dist = self.aspect(asp_emb)         #  [n_sentence, aspect6]\n",
    "        sentim_dist = self.first_fn(allsent_emb)\n",
    "        sentim_dist = sentim_dist.view(-1, self.n_asp, self.smt_hidden)  #  [n_sentence, aspect6, emb100]\n",
    "        \n",
    "        sent_bmm = sentim_dist * aspect_dist.unsqueeze(2)                #  [n_sentence, asp, emb100]\n",
    "        \n",
    "        all_doc_emb = []\n",
    "        aspect_doc = []\n",
    "        sentim_doc = []\n",
    "        cur = 0\n",
    "        for doci in range(0, len(batch)):\n",
    "            sn = batch[doci].shape[0]\n",
    "            doc_emb_avg = torch.sum(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True)  #  [1, 7, 400]\n",
    "            asp_w_sum = torch.sum(aspect_dist[cur:(cur+sn),:], dim=0, keepdim=True)      # [1, 7]\n",
    "            doc_emb_avg = doc_emb_avg / asp_w_sum[:,:,None]                                  # [1, 7, 400]\n",
    "#             doc_emb_max = torch.max(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True)[0] # [1, 7, 400]\n",
    "#             all_doc_emb.append( torch.cat( [doc_emb_avg, doc_emb_max], dim=2 ) )\n",
    "            all_doc_emb.append( doc_emb_avg )\n",
    "            aspect_doc.append( aspect_dist[cur:(cur+sn), :] )\n",
    "            \n",
    "            cur = cur + sn\n",
    "\n",
    "        all_doc_emb = torch.cat( all_doc_emb, dim=0 )          # [batch, asp, 100]\n",
    "        \n",
    "#         all_doc_emb = nn.functional.elu_( all_doc_emb )\n",
    "        \n",
    "#         result_senti = [ self.second_fn[aspi]( all_doc_emb[:,aspi,:] ) for aspi in range(0,self.n_asp)] # [batch, ra]\n",
    "        result_senti = [ self.second_fn( all_doc_emb[:,aspi,:] ) for aspi in range(0,self.n_asp)] # [batch, ra]\n",
    "        \n",
    "        result = torch.stack(result_senti, dim=1)  # [batch, asp, sentiment5]\n",
    "        \n",
    "        return result,raw_outputs,outputs,aspect_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.add_tag(\"CLAS02\")\n",
    "# experiment.add_tag(\"FULLIND400\")\n",
    "\n",
    "# ATTENTIONAL AVERAGING, COMPLETELY INDEPENDENT SENTI OUT\n",
    "\n",
    "class Cls02ATT400(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int):\n",
    "        print(\"CLS init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp + 1\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        self.asp_hidden = 40\n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 400, self.asp_hidden, p=0.5, actn=nn.Tanh() )  #  inplace=True\n",
    "        mod_layers += bn_drop_lin( self.asp_hidden, self.n_asp, p=0.15, actn=nn.Softmax(dim=1) )  #  actn=torch.nn.ReLU(dim=1)\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        self.sent_hidden = 20\n",
    "#         self.senti_base = nn.Sequential(*bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) ) )\n",
    "        self.s0 = nn.Sequential(* (bn_drop_lin( 400, self.sent_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) +\n",
    "                                   bn_drop_lin( self.sent_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s1 = nn.Sequential(* (bn_drop_lin( 400, self.sent_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) +\n",
    "                                   bn_drop_lin( self.sent_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s2 = nn.Sequential(* (bn_drop_lin( 400, self.sent_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) +\n",
    "                                   bn_drop_lin( self.sent_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s3 = nn.Sequential(* (bn_drop_lin( 400, self.sent_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) +\n",
    "                                   bn_drop_lin( self.sent_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s4 = nn.Sequential(* (bn_drop_lin( 400, self.sent_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) +\n",
    "                                   bn_drop_lin( self.sent_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s5 = nn.Sequential(* (bn_drop_lin( 400, self.sent_hidden, p=0.5, actn=nn.ReLU(inplace=True) ) +\n",
    "                                   bn_drop_lin( self.sent_hidden, self.n_rat, p=0.1, actn=None ) ) )\n",
    "\n",
    "        self.sentiments = []\n",
    "        self.sentiments.append( self.s0 )\n",
    "        self.sentiments.append( self.s1 )\n",
    "        self.sentiments.append( self.s2 )\n",
    "        self.sentiments.append( self.s3 )\n",
    "        self.sentiments.append( self.s4 )\n",
    "        self.sentiments.append( self.s5 )\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "        \n",
    "        batch = sentence_pool_400(outputs[-1], mask, p_index)\n",
    "        \n",
    "        allsent_emb = torch.cat(batch, dim=0)          # [n_sentence, emb400]\n",
    "        aspect_dist = self.aspect(allsent_emb)         # [n_sentence, aspect6]\n",
    "\n",
    "        sent_bmm = torch.bmm(aspect_dist.unsqueeze(2), allsent_emb.unsqueeze(1))  # [319, 7, 400]\n",
    "        \n",
    "        all_doc_emb = []\n",
    "        aspect_doc = []\n",
    "        sentim_doc = []\n",
    "        cur = 0\n",
    "        for doci in range(0, len(batch)):\n",
    "            sn = batch[doci].shape[0]\n",
    "            doc_emb_avg = torch.sum(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True)  #  [1, 7, 400]\n",
    "            asp_w_sum = torch.sum(aspect_dist[cur:(cur+sn),:], dim=0, keepdim=True) # [1, 7]\n",
    "            doc_emb_avg = doc_emb_avg / asp_w_sum[:,:,None]                                 # [1, 7, 400]\n",
    "#             doc_emb_max = torch.max(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True)[0] # [1, 7, 400]\n",
    "#             all_doc_emb.append( torch.cat( [doc_emb_avg, doc_emb_max], dim=2 ) )\n",
    "            all_doc_emb.append( doc_emb_avg )\n",
    "            aspect_doc.append( aspect_dist[cur:(cur+sn), :] )\n",
    "            \n",
    "            cur = cur + sn\n",
    "\n",
    "        all_doc_emb = torch.cat( all_doc_emb, dim=0 )          # [batch, asp, 400]\n",
    "        \n",
    "#         result_senti_base = self.senti_base( all_doc_emb.view(-1, 400) ) # [batch*asp, 50]\n",
    "#         result_senti_base = result_senti_base.view(-1, self.n_asp, 50)    # [batch, asp, 50]\n",
    "        \n",
    "        result_senti = [ self.sentiments[aspi]( all_doc_emb[:,aspi,:] ) for aspi in range(0,self.n_asp)] # [batch, ra]\n",
    "        \n",
    "        result = torch.stack(result_senti, dim=1)  # [batch, asp, sentiment5]\n",
    "        \n",
    "        return result,raw_outputs,outputs,aspect_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(vocab_sz:int, vocab, n_class:int, bptt:int=70, max_len:int=20*70, config:dict=None,\n",
    "                        drop_mult:float=1., lin_ftrs:Collection[int]=None, ps:Collection[float]=None,\n",
    "                        pad_idx:int=1) -> nn.Module:\n",
    "    print(\"Creating Custom Model\")\n",
    "    meta = text.learner._model_meta[AWD_LSTM]\n",
    "    # if we specified config then we dont use default\n",
    "    config = ifnone(config, meta['config_clas']).copy()\n",
    "    config.pop(\"output_p\")\n",
    "#     config.pop(\"qrnn\")\n",
    "#     config.pop(\"n_layers\")\n",
    "#     print(config)\n",
    "    # Drop multiplier\n",
    "    for k in config.keys():\n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    \n",
    "    encoder = SentenceEncoder(bptt, max_len, AWD_LSTM(vocab_sz, **config), vocab, pad_idx=pad_idx)\n",
    "#     cls_layer = Cls02ATT400(n_asp=hyper_params[\"num_aspect\"], n_rat=hyper_params[\"num_rating\"])\n",
    "    cls_layer = Cls02_LIATLI(n_asp=hyper_params[\"num_aspect\"], n_rat=hyper_params[\"num_rating\"])\n",
    "    \n",
    "#     encoder = SentenceEncoder(bptt, max_len, BI_AWD_LSTM(vocab_sz, **config), vocab, pad_idx=pad_idx)\n",
    "#     cls_layer = Cls02ATT_BILSTM(n_asp=hyper_params[\"num_aspect\"], n_rat=hyper_params[\"num_rating\"])\n",
    "\n",
    "    model = SequentialRNN(encoder, cls_layer)\n",
    "    return model if init is None else model.apply(init)\n",
    "\n",
    "def load_pretrained(learn, wgts_fname:str, itos_fname:str, strict:bool=True):\n",
    "    \"Load a pretrained model and adapts it to the data vocabulary.\"\n",
    "    old_itos = pickle.load(open(itos_fname, 'rb'))\n",
    "    old_stoi = {v:k for k,v in enumerate(old_itos)}\n",
    "    wgts = torch.load(wgts_fname, map_location=lambda storage, loc: storage)\n",
    "    if 'model' in wgts: wgts = wgts['model']\n",
    "    wgts = convert_weights(wgts, old_stoi, learn.data.train_ds.vocab.itos)\n",
    "    \n",
    "#     wkeys = list( wgts.keys() )                                           #  for BI LSTM\n",
    "#     for wkey in wkeys:\n",
    "#         if wkey.startswith(\"0.rnns.2\"):\n",
    "#             wgts[\"0.rnns.3\"+wkey[len(\"0.rnns.3\"):]] = wgts[wkey].clone()  #  for BI LSTM\n",
    "\n",
    "    print(\"Loading Pre_Trained\")\n",
    "    learn.model.load_state_dict(wgts, strict=strict)\n",
    "    return learn\n",
    "\n",
    "def text_classifier_learner(data:DataBunch, bptt:int=70, max_len:int=20*70, config:dict=None,\n",
    "                            pretrained:bool=True, drop_mult:float=1., lin_ftrs:Collection[int]=None,\n",
    "                            ps:Collection[float]=None, **learn_kwargs) -> 'TextClassifierLearner':\n",
    "    \"Create a `Learner` with a text classifier from `data` and `arch`.\"\n",
    "    model = get_model(len(data.vocab.itos), data.vocab, data.c, bptt=bptt, max_len=max_len,\n",
    "                                config=config, drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps)\n",
    "    meta = text.learner._model_meta[AWD_LSTM]\n",
    "    learn = RNNLearner(data, model, split_func=meta['split_clas'], **learn_kwargs)\n",
    "    if pretrained:\n",
    "        if 'url' not in meta:\n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], data=False)\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn = load_pretrained(learn, *fnames, strict=False)\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelCEL(nn.CrossEntropyLoss):\n",
    "    def forward(self, input, target, nasp=5):\n",
    "        target = target.long()\n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(nasp):\n",
    "            loss = loss + super(MultiLabelCEL, self).forward(input[:,i,:], target[:,i])\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def multi_acc(preds, targs, nasp=hyper_params[\"num_aspect\"], nrat=5):\n",
    "    preds = preds[:,0:nasp,:]\n",
    "    preds = preds.contiguous().view(-1, nrat)\n",
    "    preds = torch.max(preds, dim=1)[1]\n",
    "    targs = targs.contiguous().view(-1).long()\n",
    "    return (preds==targs).float().mean()\n",
    "\n",
    "def get_clas_acc(asp_index):\n",
    "    def asp_acc(preds, targs):\n",
    "        preds = torch.max(preds, dim=2)[1]\n",
    "        targs = targs.contiguous().long()\n",
    "        return (preds[:,asp_index]==targs[:,asp_index]).float().mean()\n",
    "    return asp_acc\n",
    "\n",
    "def get_clas_mse(asp_index):\n",
    "    def asp_mse(preds, targs):\n",
    "        preds = torch.max(preds, dim=2)[1].float()[:,asp_index]\n",
    "        targs = targs.contiguous().float()[:,asp_index]\n",
    "        return torch.nn.functional.mse_loss(preds, targs)\n",
    "    return asp_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "macc = [get_clas_acc(ai) for ai in range(hyper_params[\"num_aspect\"])]\n",
    "for ai in range(hyper_params[\"num_aspect\"]):\n",
    "    macc[ai].__name__ = \"clas_acc_\"+str(ai)\n",
    "mmse = [get_clas_mse(ai) for ai in range(hyper_params[\"num_aspect\"])]\n",
    "for ai in range(hyper_params[\"num_aspect\"]):\n",
    "    mmse[ai].__name__ = \"clas_mse_\"+str(ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Custom Model\n",
      "Encoder initing\n",
      "max len 1400\n",
      "bptt 70\n",
      "pad_idx 1\n",
      "period index 9\n",
      "CLS init\n",
      "Num Aspect: 5\n",
      "Num Rating: 5\n",
      "Loading Pre_Trained\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): Embedding(31600, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(31600, 400, padding_idx=1)\n",
      "  )\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Cls02_LIATLI(\n",
      "    (aspect): Sequential(\n",
      "      (0): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=800, out_features=40, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (4): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.15, inplace=False)\n",
      "      (6): Linear(in_features=40, out_features=6, bias=True)\n",
      "      (7): Sigmoid()\n",
      "    )\n",
      "    (first_fn): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=1800, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "    (second_fn): Sequential(\n",
      "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.4, inplace=False)\n",
      "      (2): Linear(in_features=300, out_features=80, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.2, inplace=False)\n",
      "      (6): Linear(in_features=80, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mloss = MultiLabelCEL()\n",
    "cls_learn = text_classifier_learner(cls_db,\n",
    "                                    drop_mult=1.1,\n",
    "                                    loss_func=mloss,\n",
    "                                    metrics=[multi_acc]+macc+mmse,\n",
    "                                    bptt=70,\n",
    "                                    max_len=hyper_params[\"max_sequence_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 0 =====\n",
      "Sequential(\n",
      "  (0): Embedding(31600, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(31600, 400, padding_idx=1)\n",
      "  )\n",
      ")\n",
      "group 1 =====\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "group 2 =====\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "group 3 =====\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "group 4 =====\n",
      "Sequential(\n",
      "  (0): Cls02_LIATLI(\n",
      "    (aspect): Sequential(\n",
      "      (0): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=800, out_features=40, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (4): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.15, inplace=False)\n",
      "      (6): Linear(in_features=40, out_features=6, bias=True)\n",
      "      (7): Sigmoid()\n",
      "    )\n",
      "    (first_fn): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=1800, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "    (second_fn): Sequential(\n",
      "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.4, inplace=False)\n",
      "      (2): Linear(in_features=300, out_features=80, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.2, inplace=False)\n",
      "      (6): Linear(in_features=80, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cls_learn.layer_groups)):\n",
    "    print(\"group \" + str(i) + \" =====\")\n",
    "    print(cls_learn.layer_groups[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp: best model tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks.tracker import TrackerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_validate(model:nn.Module, dl:DataLoader, loss_func:OptLossFunc=None, n_batch:Optional[int]=None)->Iterator[Tuple[Union[Tensor,int],...]]:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_losses,nums = [],[]\n",
    "\n",
    "        for xb,yb in dl:\n",
    "            out = model(xb)[0]\n",
    "            val_loss = loss_func(out, yb)\n",
    "            val_loss = val_loss.detach().cpu()\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            if not is_listy(yb): yb = [yb]\n",
    "            nums.append(first_el(yb).shape[0])\n",
    "            if n_batch and (len(nums)>=n_batch): break\n",
    "            \n",
    "        nums = np.array(nums, dtype=np.float32)\n",
    "        return (to_np(torch.stack(val_losses)) * nums).sum() / nums.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestStepModel(TrackerCallback):\n",
    "    \"A `TrackerCallback` that saves the model when monitored quantity is best.\"\n",
    "    def __init__(self, learn:Learner, monitor:str='valid_loss', mode:str='auto', every:int=50, name:str='bestmodel'):\n",
    "        super().__init__(learn, monitor=monitor, mode=mode)\n",
    "        self.every, self.name = every, name\n",
    "        self.step = 0\n",
    "        self.records = []\n",
    "    \n",
    "    def on_train_begin(self, **kwargs:Any)->None:\n",
    "        super().on_train_begin(**kwargs)\n",
    "        self.step = 0\n",
    "        \n",
    "    def on_batch_end(self, **kwargs:Any)->None:\n",
    "        self.step += 1\n",
    "\n",
    "        if self.step % self.every == 0:\n",
    "            self.learn.model.eval()\n",
    "            current = fast_validate(self.learn.model, self.learn.data.valid_dl, self.learn.loss_func, n_batch=50)\n",
    "            self.learn.model.train()\n",
    "            \n",
    "            if isinstance(current, Tensor): current = current.cpu()\n",
    "            self.records.append(current)\n",
    "            \n",
    "            if current is not None and self.operator(current, self.best):\n",
    "                print(f'Better model found at step {self.step} with {self.monitor} value: {current}.')\n",
    "                self.best = current\n",
    "                self.learn.save(f'{self.name}')\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        pass\n",
    "#         \"Load the best model.\"\n",
    "#         if self.every==\"improvement\" and os.path.isfile(self.path/self.model_dir/f'{self.name}.pth'):\n",
    "#             self.learn.load(f'{self.name}', purge=False)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.callback_fns = [cls_learn.callback_fns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_learn.callback_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.callback_fns += [ partial(SaveBestStepModel, monitor=\"valid_loss\", mode=\"min\", every=100, name='beer.clas.attfullind400.best.learner') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained LM Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREEZING\n",
      "Sequential(\n",
      "  (0): Embedding(31600, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(31600, 400, padding_idx=1)\n",
      "  )\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Cls02_LIATLI(\n",
      "    (aspect): Sequential(\n",
      "      (0): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=800, out_features=40, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (4): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.15, inplace=False)\n",
      "      (6): Linear(in_features=40, out_features=6, bias=True)\n",
      "      (7): Sigmoid()\n",
      "    )\n",
      "    (first_fn): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=1800, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "    (second_fn): Sequential(\n",
      "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.4, inplace=False)\n",
      "      (2): Linear(in_features=300, out_features=80, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.2, inplace=False)\n",
      "      (6): Linear(in_features=80, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "weight_file = \"lm_enc_beer.1115\"\n",
    "\n",
    "encoder = cls_learn.model[0]\n",
    "if hasattr(encoder, 'module'):\n",
    "    encoder = encoder.module\n",
    "distrib_barrier()\n",
    "wgts = torch.load(cls_learn.path/cls_learn.model_dir/f'{weight_file}.pth', map_location=cls_learn.data.device)\n",
    "\n",
    "# wkeys = list( wgts.keys() )\n",
    "# print(wkeys)\n",
    "# for wkey in wkeys:\n",
    "#     if wkey.startswith(\"rnns.2\"):\n",
    "#         wgts[\"rnns.3\"+wkey[len(\"rnns.3\"):]] = wgts[wkey].clone()\n",
    "        \n",
    "encoder.load_state_dict(wgts)\n",
    "cls_learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1347])\n",
      "torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "x, y = cls_db.one_batch()\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.3173e-01, -1.0980e-01, -2.9269e-01, -2.0363e-01, -3.6730e-01],\n",
       "         [-8.4624e-01, -1.2755e+00, -8.1942e-02,  1.1910e+00, -1.4436e+00],\n",
       "         [-6.1380e-01, -1.2222e+00, -7.0505e-01,  2.9264e-01, -1.6582e+00],\n",
       "         [ 2.3532e-01,  3.0885e-01,  4.8738e-02,  4.7766e-01,  9.1404e-01],\n",
       "         [ 1.0431e+00, -5.8030e-01, -1.1487e-01,  1.3336e+00, -7.2721e-01],\n",
       "         [ 9.9959e-01, -1.5362e+00, -3.3646e-01,  3.4005e-01, -1.6669e+00]],\n",
       "\n",
       "        [[-3.6118e-01, -3.9932e-02, -1.0955e-01, -2.0324e-01, -4.7890e-01],\n",
       "         [ 2.6623e-01,  7.6900e-01,  5.7503e-01, -1.1667e-01,  2.6506e-02],\n",
       "         [-3.2350e-01, -2.9726e-01,  1.2579e-01, -1.5481e+00, -5.4925e-01],\n",
       "         [-1.7890e-02,  3.8358e-01,  7.1075e-01, -7.5148e-03, -1.6143e-01],\n",
       "         [-4.7205e-01,  3.2206e-02, -6.2659e-01, -3.7117e-01, -2.4333e-01],\n",
       "         [ 3.1445e-01,  2.7918e-01,  8.8261e-01, -2.8040e-01,  1.9001e-01]],\n",
       "\n",
       "        [[ 1.2695e+00,  5.7827e-01,  5.9132e-01, -2.7674e-01, -4.3576e-01],\n",
       "         [-1.2413e-01, -3.3328e-01,  1.0113e+00, -4.1488e-02,  3.4319e-01],\n",
       "         [ 6.1423e-02,  9.1358e-01, -7.2279e-03, -1.5454e-01,  2.5876e-01],\n",
       "         [ 8.8023e-02, -4.1567e-01,  7.8207e-01,  6.3784e-02,  4.1002e-01],\n",
       "         [-2.2433e-02, -2.5744e-01,  3.4601e-01, -3.9730e-01,  2.2621e-01],\n",
       "         [-5.0071e-03, -1.0919e+00,  4.1484e-03,  8.2459e-02,  3.3345e-01]],\n",
       "\n",
       "        [[-1.6476e-01,  9.1972e-01, -1.0098e+00,  4.9110e-01, -8.5149e-01],\n",
       "         [-1.0621e-01, -5.7396e-01, -4.2290e-01,  4.4025e-01, -4.0552e-01],\n",
       "         [ 7.8985e-01,  1.3111e+00,  8.8267e-02, -4.3893e-01, -2.2985e-01],\n",
       "         [ 1.5828e-01, -9.0076e-01, -2.1805e-01, -1.1488e+00,  8.4363e-01],\n",
       "         [ 1.9534e-01, -1.4572e-01,  4.9758e-01, -1.2132e+00, -8.0318e-02],\n",
       "         [-1.7433e-01,  1.3262e+00,  2.4581e-01,  3.6421e-01,  7.1163e-01]],\n",
       "\n",
       "        [[ 6.2863e-01, -2.8646e-01,  1.0363e-01,  4.5649e-01, -2.7468e-01],\n",
       "         [ 5.9955e-02, -5.1500e-01, -7.1334e-02,  7.0759e-01, -1.2744e+00],\n",
       "         [-1.2972e+00, -8.1321e-01,  3.1891e-01,  6.8462e-01,  3.8438e-01],\n",
       "         [ 3.8941e-01, -2.7643e-01, -1.4744e+00,  8.7739e-01,  2.0311e-01],\n",
       "         [ 5.5254e-01, -4.1833e-01, -2.7005e-01,  2.6989e-01, -9.7323e-01],\n",
       "         [-2.5590e-01, -6.8165e-01,  2.5502e-01,  5.0352e-01, -8.3524e-01]],\n",
       "\n",
       "        [[ 3.9527e-01,  4.4786e-01, -3.6670e-02, -5.8354e-01, -9.9865e-01],\n",
       "         [-1.5372e-01, -5.2083e-01, -5.1280e-01, -3.3018e-01,  3.8040e-01],\n",
       "         [-2.6918e-01, -1.1768e-01,  3.9134e-01,  3.0094e-02,  1.3245e-01],\n",
       "         [ 2.7887e-01,  1.0656e+00,  1.0965e-01,  1.0991e-01, -9.6041e-01],\n",
       "         [-2.9096e-01,  2.9212e-01,  5.4495e-01,  3.8224e-01, -6.6876e-01],\n",
       "         [-1.0098e+00,  3.0096e-02, -3.7633e-01, -4.3723e-01, -2.7418e-01]],\n",
       "\n",
       "        [[-4.2589e-01, -7.3061e-01, -3.9688e-01, -6.4220e-01, -1.2121e+00],\n",
       "         [ 3.8755e-01, -1.8323e-02, -2.0198e-01,  1.2703e+00, -1.2968e-02],\n",
       "         [-6.6164e-01,  1.1466e-01, -9.1024e-02,  5.1097e-02,  3.8582e-02],\n",
       "         [ 5.1461e-01, -8.7597e-01, -5.6817e-01,  6.5883e-01,  7.2278e-01],\n",
       "         [-5.3670e-01,  9.4395e-03,  6.6401e-01,  9.4216e-02,  2.7998e-01],\n",
       "         [-5.2903e-01, -1.2568e-01,  4.8681e-01,  1.3874e+00,  6.1179e-01]],\n",
       "\n",
       "        [[-9.1482e-01,  9.4169e-01, -7.1556e-01, -1.5364e-01,  2.1996e-01],\n",
       "         [ 4.2953e-01, -1.2588e+00, -1.0666e+00,  1.6580e-01,  1.0243e+00],\n",
       "         [ 1.6742e-01,  2.6500e+00,  1.1230e+00, -6.2235e-01, -6.6173e-01],\n",
       "         [ 1.0323e-01,  7.4433e-01, -3.0975e-01,  7.4746e-01,  2.5561e-01],\n",
       "         [ 9.9682e-01,  5.2862e-01, -2.4378e-01, -5.4577e-01, -7.5260e-01],\n",
       "         [-6.3408e-01,  7.9700e-02,  3.5314e-01,  1.7191e-01,  4.1974e-01]],\n",
       "\n",
       "        [[-4.3197e-01,  2.1019e-01, -9.5927e-02, -7.7655e-01,  5.8085e-01],\n",
       "         [ 7.8777e-01,  6.9928e-01,  1.0226e+00, -4.2240e-01,  2.4157e-01],\n",
       "         [-1.1380e-01,  1.1434e-01, -5.8020e-01,  2.3623e-01, -8.3833e-01],\n",
       "         [ 9.4947e-02,  3.2950e-01,  2.2355e-01, -2.9098e-01, -2.9837e-02],\n",
       "         [-1.3484e-01,  9.5768e-03,  4.5179e-01, -8.1881e-01,  5.1827e-02],\n",
       "         [ 5.9875e-02, -9.1349e-02, -3.3600e-01, -1.1358e+00, -7.4852e-01]],\n",
       "\n",
       "        [[-1.4337e+00,  1.2397e+00,  8.6749e-01,  3.6952e-01,  1.2398e-01],\n",
       "         [ 7.4498e-01,  4.2999e-01, -6.3266e-01, -4.2180e-01,  1.9874e-01],\n",
       "         [ 4.2415e-01,  4.3640e-02,  3.9747e-01,  2.0247e-02,  4.7516e-01],\n",
       "         [-1.4357e+00, -3.0543e-01,  3.4636e-01,  8.4706e-02,  8.4132e-01],\n",
       "         [-1.3225e-01,  5.1952e-01,  8.1934e-02, -4.0689e-01,  2.0135e-02],\n",
       "         [ 4.6097e-01,  1.0768e-01,  3.1868e-01, -3.4333e-01,  1.8172e-01]],\n",
       "\n",
       "        [[-8.8201e-02, -2.8243e-01,  6.6035e-01, -1.8300e-01,  1.6028e-01],\n",
       "         [-5.0094e-01, -2.3404e-01,  3.4693e-01,  1.7139e-02,  3.7329e-01],\n",
       "         [-1.9011e-01, -1.5532e-01,  4.9986e-01, -5.1462e-01,  8.8775e-01],\n",
       "         [ 4.9226e-01,  1.0111e+00, -7.5338e-01,  6.4253e-01,  2.1068e-01],\n",
       "         [-1.5964e+00, -4.5719e-01,  1.8624e-01,  1.0034e+00,  8.8399e-01],\n",
       "         [-8.3063e-01, -5.9549e-01, -5.6044e-01,  6.7679e-01, -7.7248e-01]],\n",
       "\n",
       "        [[-1.3697e+00, -4.7737e-01,  1.9830e-02,  1.2423e-01, -1.2333e+00],\n",
       "         [-1.1060e+00,  2.5138e-01, -1.8266e-01, -9.5984e-01,  6.5508e-01],\n",
       "         [-3.5733e-01,  7.2635e-01, -4.3148e-02,  6.5223e-02, -5.3926e-01],\n",
       "         [-5.9088e-01,  3.5435e-01,  1.9364e+00, -6.5980e-02,  4.1866e-02],\n",
       "         [-2.9588e-01,  1.2562e+00, -7.1001e-01,  2.2842e-03,  4.3828e-01],\n",
       "         [-2.2828e-01,  6.7889e-01, -2.4518e-01, -5.6946e-01,  8.5868e-01]],\n",
       "\n",
       "        [[ 3.2392e-01,  4.4452e-01,  1.9903e-01, -2.2890e-01, -7.0337e-01],\n",
       "         [ 2.3120e-01, -1.3128e+00, -6.5626e-01,  6.1910e-01, -1.3991e-01],\n",
       "         [ 4.8142e-01, -4.7618e-01, -5.5904e-01,  9.4621e-01, -1.6111e+00],\n",
       "         [-3.3365e-01,  5.3106e-01, -1.1464e+00,  2.1914e-01, -4.3410e-01],\n",
       "         [ 1.6278e+00,  7.5593e-01, -7.3737e-02,  6.3761e-01, -3.2507e-01],\n",
       "         [-4.8936e-01, -1.9409e+00, -6.5050e-01,  3.4064e-01, -1.4222e+00]],\n",
       "\n",
       "        [[-3.0569e-02, -2.0568e-01, -5.4037e-01, -5.3870e-01, -2.5478e-01],\n",
       "         [-8.0698e-01, -2.7777e-01,  8.6398e-01,  4.2843e-01,  4.0896e-01],\n",
       "         [ 2.6254e-01, -4.8934e-02, -6.1086e-02, -3.1807e-01,  5.5280e-02],\n",
       "         [ 3.5095e-01, -6.5610e-01,  3.9047e-02,  6.7152e-01, -4.3127e-02],\n",
       "         [-1.4379e+00,  1.0930e+00,  9.7597e-01, -6.3640e-01,  1.0847e+00],\n",
       "         [-9.9434e-01,  2.0768e-01,  6.5203e-01, -4.2481e-01,  4.0353e-01]],\n",
       "\n",
       "        [[-6.5617e-01, -8.9057e-01,  1.0373e+00, -4.1657e-01,  2.2299e-01],\n",
       "         [ 3.4481e-02,  1.6145e+00, -4.9822e-01,  5.6173e-02, -6.7344e-01],\n",
       "         [-4.3752e-01, -2.5401e-01, -3.3698e-01, -8.0087e-01, -1.6751e-01],\n",
       "         [ 5.5900e-01,  6.4893e-01,  8.0183e-01,  9.5050e-03,  1.2133e-03],\n",
       "         [-2.1163e-01,  9.0626e-02, -8.4169e-02,  7.0384e-01,  5.3282e-01],\n",
       "         [-2.7101e-01, -7.7851e-02,  8.6522e-01, -8.5342e-01,  1.0429e+00]],\n",
       "\n",
       "        [[-6.4898e-02,  6.8627e-01,  1.3312e+00, -6.2417e-01,  2.0117e-01],\n",
       "         [-1.4104e+00,  7.5004e-01,  9.1201e-01,  4.5795e-01,  8.5675e-01],\n",
       "         [ 9.8941e-01, -2.0806e-01, -6.1985e-02,  3.4069e-02,  3.8567e-02],\n",
       "         [-7.9822e-01,  7.7421e-01,  3.7767e-02, -9.7166e-01, -1.6433e-01],\n",
       "         [ 1.3489e-01, -7.0115e-01, -1.3845e-01, -1.5452e+00,  1.0419e+00],\n",
       "         [-2.6729e-01,  7.9743e-01,  8.3895e-01, -5.6485e-02,  6.2004e-01]],\n",
       "\n",
       "        [[ 1.3979e+00,  1.1235e+00, -2.6538e-01,  3.0515e-01, -8.3145e-01],\n",
       "         [-2.2202e-01, -6.4609e-01,  3.5091e-01, -2.4882e-01,  8.3181e-01],\n",
       "         [ 6.0385e-02,  2.7348e-01,  8.4773e-01, -7.9866e-01,  5.8140e-02],\n",
       "         [ 1.4792e+00,  4.4071e-01, -7.2043e-01, -7.9367e-01,  4.7559e-01],\n",
       "         [-2.0150e-01, -4.8911e-01, -7.2040e-02,  1.4732e-01, -1.7023e-01],\n",
       "         [-1.4804e+00, -1.2155e-01,  1.5502e-01, -7.5115e-01,  3.5317e-01]],\n",
       "\n",
       "        [[-1.9056e+00, -1.0629e+00,  3.4356e-01, -1.0363e-01,  2.5447e-01],\n",
       "         [ 8.3314e-01, -6.6834e-01, -4.8113e-01,  2.5676e-01, -1.6650e+00],\n",
       "         [-5.5975e-01, -1.4581e+00, -6.5271e-01,  2.6634e-01,  6.5010e-01],\n",
       "         [ 8.7873e-01,  9.3714e-01, -1.3229e+00,  1.1161e+00, -1.1027e+00],\n",
       "         [ 3.5602e-01, -1.8152e+00, -2.3798e-01,  4.3366e-01,  2.1139e-01],\n",
       "         [ 8.9587e-01, -4.0433e-01, -4.1654e-01, -3.6124e-01, -1.2164e+00]],\n",
       "\n",
       "        [[ 3.0162e-01,  3.8657e-02,  1.0455e+00, -1.9241e-01, -1.3066e-01],\n",
       "         [-5.0033e-01, -9.3097e-01, -5.5398e-01,  8.7873e-02, -8.8680e-01],\n",
       "         [-4.6744e-01,  9.1707e-01, -1.1905e+00,  1.2907e-01,  1.2671e+00],\n",
       "         [ 5.2618e-01,  4.6721e-01,  1.3740e+00, -8.5894e-02, -7.1942e-01],\n",
       "         [-3.9963e-01, -8.5341e-01,  1.6686e-01,  3.5486e-01, -7.0550e-01],\n",
       "         [ 3.0576e-01, -9.3536e-01,  1.1571e-02,  6.7455e-01,  1.0020e+00]],\n",
       "\n",
       "        [[-1.2615e+00, -1.3719e+00, -5.9594e-01,  6.3468e-01, -5.2959e-01],\n",
       "         [-4.5409e-01,  4.0945e-01, -4.5154e-01,  2.7144e+00, -4.3396e-01],\n",
       "         [ 2.9494e-01,  1.9008e-01, -3.7756e-01,  1.6005e+00, -1.9516e+00],\n",
       "         [-2.1388e-01, -5.4739e-01,  6.2955e-02, -5.7038e-01, -1.4207e-02],\n",
       "         [-6.9935e-01, -8.3337e-01, -2.2728e-01, -2.7290e-01, -5.3566e-01],\n",
       "         [ 2.7577e-01, -6.4180e-01, -2.5996e+00,  3.0109e-01, -9.6023e-01]],\n",
       "\n",
       "        [[ 6.8177e-01,  1.0178e+00,  4.8470e-01,  6.1614e-01,  9.6417e-01],\n",
       "         [-3.9944e-01, -4.7115e-01,  1.6055e+00,  3.2049e-01,  1.1140e+00],\n",
       "         [ 5.7718e-01, -1.0807e+00, -2.7084e-01, -9.7972e-01, -1.9799e-01],\n",
       "         [-5.2788e-02,  1.0976e-01,  6.8496e-02,  4.2293e-02, -1.8384e-01],\n",
       "         [ 3.7925e-01,  4.9840e-01, -3.3283e-01, -7.0291e-01,  2.9813e-01],\n",
       "         [-1.6104e-01, -3.1457e-01,  4.2814e-02, -4.9730e-01,  8.0167e-01]],\n",
       "\n",
       "        [[-6.4487e-02, -1.5690e+00,  2.6970e-01,  3.5365e-01,  1.0553e+00],\n",
       "         [ 2.4719e-01, -8.2672e-01, -4.0130e-01,  2.1348e-02,  2.7873e-01],\n",
       "         [-1.0545e+00,  2.0562e-01, -9.4271e-01,  1.2429e-01,  4.7594e-01],\n",
       "         [-7.5799e-01,  3.9265e-01, -5.8228e-01,  3.4200e-01, -1.1574e+00],\n",
       "         [-2.8538e-02,  5.6415e-01, -3.4304e-01, -2.0734e-01, -6.2435e-01],\n",
       "         [-4.2163e-01, -3.6413e-01,  1.9052e-01,  3.7760e-01,  1.3010e+00]],\n",
       "\n",
       "        [[ 9.2945e-02,  2.3446e-01,  7.3880e-03, -3.9846e-01,  9.7613e-01],\n",
       "         [-5.6440e-02,  4.6323e-01, -9.9987e-01, -3.1204e-01, -6.5495e-01],\n",
       "         [ 6.9765e-02,  9.8894e-01, -2.9439e-01, -1.0192e-01,  2.6754e-01],\n",
       "         [-2.2229e-01, -1.3801e+00,  4.8452e-02, -2.2591e-01, -5.1789e-01],\n",
       "         [ 1.2113e-01, -4.3731e-01, -3.8726e-01,  3.0797e-01,  4.3279e-01],\n",
       "         [ 3.7488e-01, -7.5473e-01, -9.3908e-01, -1.0651e-01, -1.0006e-01]],\n",
       "\n",
       "        [[ 5.4091e-01,  1.8336e+00, -1.2155e-02,  7.5511e-01, -2.3511e-01],\n",
       "         [-1.0504e+00, -8.3665e-02,  1.7568e+00, -6.1995e-01, -2.0212e-01],\n",
       "         [ 1.8930e-01, -6.7586e-01,  1.0017e-01,  3.3944e-01,  5.5483e-01],\n",
       "         [ 8.3139e-02, -5.2013e-01, -4.7269e-01, -1.3182e-02,  3.5904e-01],\n",
       "         [ 2.0595e-01,  8.8892e-01,  7.7659e-01,  4.1278e-01,  2.7178e-01],\n",
       "         [-6.3305e-01,  1.5458e+00,  1.0880e-01, -7.7153e-01,  1.0877e+00]],\n",
       "\n",
       "        [[-1.4754e-01,  3.3492e-01, -1.1677e+00,  1.5267e+00,  4.3708e-01],\n",
       "         [ 8.0623e-01, -5.0179e-01, -2.1495e-01,  6.1154e-01,  6.3636e-01],\n",
       "         [-9.0155e-01,  2.6652e-01,  7.8169e-01,  5.2563e-01,  6.8969e-01],\n",
       "         [-3.7679e-01, -8.2129e-01, -6.9442e-01, -5.5868e-01,  2.5979e-01],\n",
       "         [ 1.3036e+00, -1.7032e+00, -2.8658e-01,  6.5578e-01, -2.1940e-01],\n",
       "         [ 1.2031e+00,  5.9788e-01, -1.2395e+00,  6.7525e-01, -6.5357e-01]],\n",
       "\n",
       "        [[-2.9013e-01,  1.0307e+00,  6.1166e-01,  2.2958e-02, -4.5420e-01],\n",
       "         [-1.9041e-01, -1.2399e-01, -5.4938e-01,  1.4028e-01, -1.1015e+00],\n",
       "         [-4.0072e-01, -1.0320e-01, -3.8844e-01,  1.1323e+00, -6.3077e-01],\n",
       "         [-7.0496e-01,  6.6115e-01,  7.1586e-01,  7.1480e-01,  4.1714e-01],\n",
       "         [ 4.9639e-01,  5.5274e-01,  4.6104e-01,  3.1123e-02,  4.4006e-01],\n",
       "         [ 8.7419e-02, -4.5834e-02,  1.1706e+00, -4.3153e-01,  6.5306e-01]],\n",
       "\n",
       "        [[ 2.8342e-01, -1.2731e+00, -7.4168e-01,  2.3115e-01,  6.1038e-01],\n",
       "         [ 3.0284e-01,  4.5795e-01,  3.5013e-01, -4.2706e-02, -1.8588e-01],\n",
       "         [ 2.1840e-01, -1.4194e-02,  1.7306e-01,  6.8314e-02, -3.6819e-01],\n",
       "         [ 1.3984e+00, -1.6747e+00,  1.5712e-01, -4.2747e-01, -8.3463e-01],\n",
       "         [ 2.0116e-01, -9.8450e-01, -1.1995e+00, -3.5282e-01,  4.9442e-03],\n",
       "         [-3.8920e-01, -9.2147e-01, -6.0164e-01,  1.9017e+00,  2.7704e-01]],\n",
       "\n",
       "        [[-5.1795e-01, -3.8451e-01,  7.2129e-01, -2.7091e-01,  1.2239e+00],\n",
       "         [-5.3917e-01,  1.2377e-01,  1.1775e-01, -1.3849e-01, -1.0276e-01],\n",
       "         [-5.6796e-02,  1.3073e+00, -6.8773e-02, -3.6024e-01,  1.4837e+00],\n",
       "         [-1.6683e+00,  4.5095e-01,  1.6900e+00, -3.0063e-02,  4.8456e-01],\n",
       "         [-5.9070e-01,  5.4733e-01,  5.7263e-01,  5.6747e-01,  4.6402e-01],\n",
       "         [-3.1214e-01,  3.7083e-01,  2.2439e-01, -3.5173e-01, -9.6202e-02]],\n",
       "\n",
       "        [[ 1.1172e+00, -7.9008e-01,  2.6272e-01,  2.8545e-01,  1.7946e-01],\n",
       "         [ 4.5413e-01,  7.5213e-01,  7.9648e-01,  8.5225e-02, -6.6011e-01],\n",
       "         [-4.0775e-01, -2.1451e-01,  3.0322e-01,  5.1810e-02,  3.1320e-01],\n",
       "         [-8.6071e-01, -8.4357e-01,  4.6806e-01, -4.8303e-02,  2.7828e-01],\n",
       "         [ 2.0381e-01, -8.5781e-01,  8.6933e-01,  1.0071e-01, -4.6071e-01],\n",
       "         [-1.1343e-01,  1.2484e+00,  4.8935e-01, -6.1262e-01,  8.9771e-01]],\n",
       "\n",
       "        [[-2.1042e-01, -4.2185e-01, -1.0810e+00,  8.6216e-01, -4.0951e-01],\n",
       "         [-1.8657e-01, -2.2863e-02, -4.7704e-01, -5.0726e-01,  1.3315e+00],\n",
       "         [ 1.2906e+00, -2.3050e-02,  3.5855e-03,  1.1495e+00, -2.4067e-01],\n",
       "         [-1.5453e-01,  9.4840e-01,  8.6543e-01, -3.9784e-01,  3.4294e-01],\n",
       "         [ 3.9305e-01, -4.2929e-01, -2.4186e-01,  1.1025e-01,  4.2798e-01],\n",
       "         [-5.6074e-01,  7.7605e-01,  1.1566e+00,  6.0054e-01,  8.5924e-02]],\n",
       "\n",
       "        [[-6.7086e-01, -4.6247e-02,  4.9267e-01, -2.3349e-01,  1.1953e+00],\n",
       "         [-5.8250e-01,  5.7169e-01,  7.0453e-01, -4.3153e-01, -1.7932e-01],\n",
       "         [-3.3209e-02, -1.3474e+00,  4.2040e-03,  3.9573e-01, -2.6271e-01],\n",
       "         [-4.2327e-01,  1.4744e+00,  1.9487e+00, -1.3260e+00,  1.9255e-01],\n",
       "         [-6.0801e-01,  1.1519e+00, -4.2135e-01,  5.6880e-01, -5.7335e-01],\n",
       "         [-1.5156e-01, -2.5029e-01,  3.5465e-01, -3.0198e-01, -7.1676e-01]],\n",
       "\n",
       "        [[ 1.2764e-02, -6.9134e-01,  1.0479e-02,  4.2208e-01,  1.2536e+00],\n",
       "         [ 1.3520e+00, -1.0502e+00, -2.8763e-01,  9.6757e-02,  1.8052e-01],\n",
       "         [ 4.4745e-01, -9.0535e-01,  9.2602e-01,  5.8229e-01, -2.9868e-01],\n",
       "         [ 5.0542e-02, -1.3813e+00, -1.6300e+00,  4.7890e-01,  3.9243e-02],\n",
       "         [-9.8533e-01,  3.6736e-01,  1.8386e-01, -3.8115e-01,  6.3608e-01],\n",
       "         [ 5.4815e-01,  7.2750e-01, -6.1766e-02, -6.2302e-01, -9.4667e-01]]],\n",
       "       device='cuda:0', grad_fn=<StackBackward>), [tensor([[[ 7.2990e-05, -1.5217e-02, -2.5846e-01,  ..., -5.3721e-01,\n",
       "           1.3935e-03,  2.4868e-04],\n",
       "         [ 6.6906e-04,  1.6641e-07, -5.2901e-04,  ...,  1.7265e-02,\n",
       "           2.2008e-01,  5.1276e-03],\n",
       "         [ 2.2560e-01, -5.7267e-03,  5.2630e-03,  ..., -6.3233e-01,\n",
       "          -1.4256e-01, -1.8957e-03],\n",
       "         ...,\n",
       "         [ 3.0458e-04, -9.2675e-03,  1.0083e-02,  ..., -2.3919e-01,\n",
       "           1.1947e-02, -1.4096e-02],\n",
       "         [ 1.8863e-04, -2.9350e-03, -3.2837e-01,  ..., -1.2532e-03,\n",
       "           8.8778e-04, -1.6639e-03],\n",
       "         [ 6.2985e-03, -2.4369e-02, -7.5209e-02,  ..., -1.8032e-01,\n",
       "          -4.0230e-03, -7.5005e-03]],\n",
       "\n",
       "        [[ 6.2754e-02, -5.0107e-02, -3.2665e-02,  ...,  1.0165e-01,\n",
       "          -7.1006e-02, -4.0864e-03],\n",
       "         [ 5.6559e-02, -1.2747e-01,  9.0644e-02,  ...,  2.5011e-02,\n",
       "          -1.5593e-02,  9.7691e-03],\n",
       "         [ 5.2532e-02, -8.1359e-02,  5.1950e-02,  ...,  1.3085e-02,\n",
       "          -3.9666e-03, -4.3690e-02],\n",
       "         ...,\n",
       "         [ 2.4206e-01, -3.3796e-02,  1.2267e-02,  ..., -3.7025e-04,\n",
       "           3.2777e-04, -2.1297e-03],\n",
       "         [ 6.4728e-04, -2.8956e-02,  1.7156e-03,  ...,  9.4485e-05,\n",
       "          -1.3553e-05, -2.1862e-03],\n",
       "         [-3.2331e-06, -5.0957e-02,  1.7479e-02,  ..., -2.7084e-01,\n",
       "           1.6189e-05, -2.0841e-02]],\n",
       "\n",
       "        [[-4.1227e-03, -5.2863e-02, -6.2403e-02,  ...,  1.2431e-01,\n",
       "          -4.2025e-02, -2.7690e-02],\n",
       "         [-2.6579e-02, -1.7469e-01,  5.5894e-02,  ...,  1.4275e-02,\n",
       "          -7.4989e-03, -2.1211e-02],\n",
       "         [-1.8224e-02, -3.1035e-01,  9.7965e-02,  ...,  1.8447e-02,\n",
       "          -2.5474e-03, -2.3230e-02],\n",
       "         ...,\n",
       "         [-1.6388e-03,  1.1481e-03, -1.3586e-03,  ..., -1.4656e-03,\n",
       "           1.2948e-01, -1.4167e-03],\n",
       "         [ 2.7742e-01,  1.6805e-04, -5.5480e-03,  ...,  2.2106e-02,\n",
       "           9.1218e-03, -5.2276e-05],\n",
       "         [ 1.1351e-01, -2.3717e-04, -9.2022e-05,  ..., -1.8936e-02,\n",
       "           1.7600e-01, -3.3966e-04]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 9.9885e-02, -6.5567e-02, -1.3809e-01,  ...,  1.1198e-01,\n",
       "          -4.8535e-02, -3.4039e-02],\n",
       "         [ 5.8405e-02, -1.1757e-01,  5.7266e-02,  ...,  2.5535e-02,\n",
       "          -5.7673e-03, -2.5223e-02],\n",
       "         [-4.0742e-02, -1.7764e-01,  1.2488e-01,  ...,  2.6913e-02,\n",
       "          -3.2305e-03, -3.2381e-02],\n",
       "         ...,\n",
       "         [-2.9498e-03, -2.5128e-01,  5.6930e-03,  ..., -3.3100e-01,\n",
       "          -3.4092e-03, -2.3328e-02],\n",
       "         [-2.9274e-06,  4.1029e-02,  7.9900e-04,  ..., -9.2895e-01,\n",
       "          -2.6472e-03, -5.3005e-02],\n",
       "         [-6.4378e-03, -7.8448e-02,  2.3816e-02,  ..., -8.1680e-01,\n",
       "          -4.4952e-03, -7.9244e-02]],\n",
       "\n",
       "        [[ 1.1279e-01, -3.7908e-02, -5.3030e-03,  ...,  1.0909e-01,\n",
       "          -4.5247e-02, -3.0434e-02],\n",
       "         [ 1.3778e-01, -1.1949e-01,  1.5284e-01,  ...,  2.8758e-02,\n",
       "          -8.2916e-03, -5.5797e-03],\n",
       "         [ 1.5315e-01, -2.2765e-01,  1.5970e-01,  ...,  3.1577e-02,\n",
       "          -5.3418e-03, -2.4653e-02],\n",
       "         ...,\n",
       "         [-2.1591e-04,  3.3684e-03,  9.5525e-02,  ...,  2.2258e-02,\n",
       "           1.1304e-03, -3.6418e-03],\n",
       "         [-4.8815e-03,  1.2754e-03,  1.9039e-01,  ..., -3.4730e-03,\n",
       "           2.3236e-03, -3.4943e-03],\n",
       "         [-7.8417e-05, -3.2239e-05,  2.2615e-02,  ..., -1.8694e-01,\n",
       "           4.1486e-02, -1.2671e-02]],\n",
       "\n",
       "        [[-2.4604e-02, -5.5256e-02, -6.1238e-02,  ...,  8.0544e-02,\n",
       "          -5.2561e-02, -2.4280e-02],\n",
       "         [ 2.5445e-03, -1.8899e-01,  1.6643e-01,  ...,  2.9425e-02,\n",
       "          -5.7537e-03, -8.7329e-03],\n",
       "         [-1.0251e-02, -3.5786e-01,  2.9808e-01,  ...,  2.0933e-02,\n",
       "          -1.3566e-03, -8.7168e-04],\n",
       "         ...,\n",
       "         [ 1.5475e-01, -2.4332e-01, -6.1793e-02,  ..., -1.6814e-01,\n",
       "           3.8799e-04, -2.4292e-03],\n",
       "         [-2.5093e-03, -1.9859e-03, -1.8578e-03,  ..., -5.2219e-04,\n",
       "           4.4623e-03, -1.5700e-01],\n",
       "         [-9.5407e-05,  1.2604e-02,  3.9804e-05,  ..., -8.4337e-01,\n",
       "           7.0949e-02, -1.8615e-01]]], device='cuda:0'), tensor([[[-1.2331e-01,  2.0856e-03, -1.3092e-02,  ..., -1.5394e-03,\n",
       "           1.2882e-04, -7.4630e-02],\n",
       "         [-8.0286e-03,  1.3038e-04, -7.9769e-03,  ..., -1.2447e-03,\n",
       "           2.1634e-03,  2.9702e-02],\n",
       "         [-2.8623e-02, -3.5539e-02, -5.7741e-04,  ..., -1.4203e-03,\n",
       "          -1.8336e-02,  8.2871e-03],\n",
       "         ...,\n",
       "         [-6.0594e-04, -1.9383e-04, -7.4591e-03,  ..., -2.7935e-02,\n",
       "           1.8359e-02,  1.7161e-01],\n",
       "         [-3.0118e-04,  7.9245e-06, -2.1265e-03,  ..., -6.9348e-03,\n",
       "          -2.1366e-03,  2.0030e-01],\n",
       "         [-9.3597e-04, -4.6477e-04, -1.0109e-02,  ..., -1.8234e-02,\n",
       "           1.1443e-02,  3.8967e-01]],\n",
       "\n",
       "        [[ 1.6229e-01, -3.4048e-04,  4.1417e-02,  ...,  1.1840e-02,\n",
       "          -2.4820e-03,  1.0920e-01],\n",
       "         [ 2.1473e-01,  9.7639e-03,  4.8861e-02,  ...,  1.0526e-03,\n",
       "          -3.1896e-03,  4.4910e-02],\n",
       "         [ 1.3666e-01,  8.3058e-03,  2.5520e-02,  ...,  1.1683e-03,\n",
       "          -1.4776e-04,  8.3137e-02],\n",
       "         ...,\n",
       "         [-8.0160e-02, -1.6027e-02, -6.1236e-02,  ..., -2.7921e-02,\n",
       "           9.7780e-03,  3.2050e-03],\n",
       "         [-4.7428e-03, -8.9576e-03, -5.9857e-03,  ..., -3.8000e-01,\n",
       "           3.9695e-05,  8.1450e-02],\n",
       "         [-2.4059e-03, -4.0552e-02, -2.8734e-03,  ..., -3.7096e-01,\n",
       "          -8.7573e-04,  5.7489e-02]],\n",
       "\n",
       "        [[ 2.2593e-01,  3.4515e-02,  2.1671e-03,  ...,  6.4742e-02,\n",
       "           1.6700e-03,  1.0137e-01],\n",
       "         [ 1.6480e-01,  4.3961e-02,  1.7019e-03,  ...,  2.4280e-02,\n",
       "           1.3483e-03,  4.3495e-02],\n",
       "         [ 9.5234e-02,  5.2151e-02, -6.0699e-04,  ...,  1.7743e-02,\n",
       "           3.0525e-04,  3.0000e-02],\n",
       "         ...,\n",
       "         [-8.3960e-02,  9.6072e-03,  3.3287e-02,  ...,  3.3309e-03,\n",
       "          -6.8362e-03,  3.7515e-03],\n",
       "         [-7.2223e-04,  2.4843e-02,  5.4056e-02,  ...,  3.9996e-03,\n",
       "          -2.8376e-02,  3.4543e-03],\n",
       "         [-2.1787e-02, -1.2578e-01,  1.6505e-02,  ...,  2.0308e-02,\n",
       "           9.2199e-04,  5.5038e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.3246e-02, -1.0851e-01,  1.9161e-02,  ..., -1.8289e-03,\n",
       "           1.2681e-02,  6.8630e-02],\n",
       "         [ 8.1586e-05, -2.3785e-02,  1.5035e-02,  ..., -5.3716e-04,\n",
       "           7.8157e-03,  5.0023e-02],\n",
       "         [ 6.0811e-02, -1.2058e-02,  6.6284e-03,  ..., -4.2339e-04,\n",
       "           1.2844e-03,  7.0468e-02],\n",
       "         ...,\n",
       "         [-2.4065e-03, -7.7903e-03,  5.5772e-03,  ..., -1.2556e-01,\n",
       "           9.8052e-04,  4.3794e-02],\n",
       "         [-9.7206e-04, -5.6477e-02,  6.9823e-03,  ..., -6.2572e-02,\n",
       "           5.2395e-03,  4.8234e-03],\n",
       "         [-2.4061e-03, -5.1279e-02,  3.8738e-03,  ..., -1.3083e-01,\n",
       "           2.3953e-02,  1.3294e-02]],\n",
       "\n",
       "        [[ 1.2182e-01, -1.7976e-02, -3.6081e-02,  ..., -1.1711e-02,\n",
       "           6.3225e-03,  1.2627e-01],\n",
       "         [ 7.9161e-02,  5.8580e-04, -2.9459e-02,  ..., -4.3192e-03,\n",
       "           5.1956e-03,  1.1378e-01],\n",
       "         [ 4.0197e-02,  7.2461e-03, -2.8629e-02,  ..., -4.1557e-03,\n",
       "           3.9109e-03,  7.4050e-02],\n",
       "         ...,\n",
       "         [ 2.7218e-02,  7.0022e-01, -5.3606e-02,  ...,  2.3592e-03,\n",
       "          -2.1964e-02,  2.6710e-03],\n",
       "         [ 1.4234e-01,  2.2192e-01, -3.5960e-02,  ...,  8.1012e-04,\n",
       "          -1.1196e-01,  8.7822e-04],\n",
       "         [-1.3446e-02, -9.8524e-03, -9.8245e-03,  ...,  8.0704e-02,\n",
       "           2.9432e-04,  8.6707e-02]],\n",
       "\n",
       "        [[-4.4210e-02,  6.1774e-03,  8.4057e-02,  ...,  5.1804e-02,\n",
       "           1.1020e-03,  1.1990e-01],\n",
       "         [-7.9867e-02,  1.5031e-02,  8.8657e-02,  ...,  8.0404e-03,\n",
       "           1.2287e-03,  6.8275e-02],\n",
       "         [-2.9883e-02,  1.1851e-02,  7.4506e-02,  ...,  5.9510e-03,\n",
       "           1.0563e-03,  4.8153e-02],\n",
       "         ...,\n",
       "         [-6.5514e-03,  2.5414e-02, -5.0077e-03,  ...,  4.6726e-05,\n",
       "          -4.3094e-02,  1.8464e-03],\n",
       "         [-2.4280e-01,  5.9190e-03, -9.3253e-03,  ...,  9.0717e-04,\n",
       "          -4.8095e-03,  1.5509e-03],\n",
       "         [-1.5368e-02, -1.4595e-03, -1.5166e-02,  ...,  2.7113e-02,\n",
       "           1.8312e-02,  4.6577e-04]]], device='cuda:0'), tensor([[[-1.2919e-01, -2.1932e-02, -4.5082e-04,  ..., -1.0603e-02,\n",
       "           1.4069e-02,  1.1889e-02],\n",
       "         [ 2.8419e-02, -2.8611e-03,  2.3472e-02,  ..., -4.7602e-02,\n",
       "           3.6627e-04,  1.5887e-02],\n",
       "         [ 1.2475e-01,  5.3548e-02,  2.0131e-01,  ..., -4.4011e-02,\n",
       "           1.9278e-02, -4.5495e-02],\n",
       "         ...,\n",
       "         [-2.6706e-02, -1.3804e-02,  2.3922e-01,  ..., -8.6332e-02,\n",
       "           3.3503e-02,  2.0578e-02],\n",
       "         [ 2.1288e-03,  2.1788e-01,  7.5887e-02,  ..., -8.4140e-02,\n",
       "          -2.5553e-02,  1.2757e-02],\n",
       "         [-1.4982e-02, -2.7845e-01,  2.0224e-01,  ..., -2.9595e-02,\n",
       "           7.1107e-02,  6.5311e-02]],\n",
       "\n",
       "        [[ 1.9499e-01,  1.1266e-01, -1.3396e-01,  ...,  2.8359e-02,\n",
       "           6.0726e-02,  1.2190e-01],\n",
       "         [ 1.9574e-01, -1.1273e-01, -5.5097e-02,  ...,  2.6595e-02,\n",
       "           2.1196e-03,  7.0395e-02],\n",
       "         [ 3.7261e-01, -1.9924e-01, -8.8316e-02,  ...,  4.1164e-02,\n",
       "          -4.9273e-03,  5.7505e-02],\n",
       "         ...,\n",
       "         [ 3.1973e-01,  1.3892e-03, -1.1955e-02,  ...,  3.8841e-02,\n",
       "           1.6763e-01, -1.1292e-01],\n",
       "         [ 2.1716e-01,  5.6376e-02, -1.6248e-01,  ...,  1.2371e-03,\n",
       "          -1.6321e-02,  7.7345e-02],\n",
       "         [ 1.4162e-03, -5.0357e-02,  2.5524e-04,  ...,  7.7663e-02,\n",
       "           4.5050e-02,  1.9471e-02]],\n",
       "\n",
       "        [[ 3.6848e-01,  3.1851e-01, -1.5211e-01,  ..., -9.3613e-03,\n",
       "          -1.5281e-01,  1.0674e-01],\n",
       "         [ 3.3681e-01, -1.6101e-01, -1.7395e-01,  ...,  4.7965e-03,\n",
       "          -1.7303e-01,  3.9620e-02],\n",
       "         [ 2.0666e-01, -8.1755e-02, -7.7578e-02,  ...,  5.4817e-02,\n",
       "          -1.3572e-01, -9.6070e-03],\n",
       "         ...,\n",
       "         [ 4.8067e-02,  1.6063e-01, -1.6051e-01,  ...,  1.4993e-01,\n",
       "           2.5302e-02, -1.0553e-01],\n",
       "         [ 6.0674e-02, -5.6108e-03, -2.4124e-02,  ...,  2.3901e-01,\n",
       "           1.4772e-02,  8.5772e-02],\n",
       "         [ 2.1981e-02, -3.8529e-02, -3.8571e-02,  ...,  9.1595e-02,\n",
       "           1.9997e-02,  7.7280e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.2126e-01,  3.6670e-01,  1.2069e-01,  ..., -4.0099e-02,\n",
       "          -1.8399e-01, -1.3405e-01],\n",
       "         [ 4.4693e-01, -1.8203e-02,  4.3047e-02,  ..., -1.3794e-02,\n",
       "          -2.5943e-01, -2.1818e-02],\n",
       "         [ 2.6644e-01, -9.4693e-02,  4.9351e-03,  ..., -6.2133e-02,\n",
       "          -7.5132e-02,  4.2256e-02],\n",
       "         ...,\n",
       "         [ 6.0405e-02,  1.0960e-02, -7.4542e-03,  ...,  1.0566e-01,\n",
       "           1.5526e-02, -8.9656e-02],\n",
       "         [ 6.4023e-02,  2.8964e-02, -4.3304e-02,  ...,  1.4300e-01,\n",
       "           3.7368e-03, -1.3434e-01],\n",
       "         [ 9.7949e-03, -4.0539e-03, -4.8839e-02,  ...,  2.4714e-02,\n",
       "          -2.4780e-02,  1.8520e-03]],\n",
       "\n",
       "        [[ 4.2491e-01,  3.9651e-01, -1.3471e-01,  ...,  7.9670e-02,\n",
       "           8.2702e-02, -2.1488e-01],\n",
       "         [ 5.7318e-01,  1.7269e-01, -4.0177e-02,  ...,  1.4540e-01,\n",
       "           1.6232e-01, -3.5918e-01],\n",
       "         [ 5.3253e-01,  8.2858e-02,  1.3912e-02,  ...,  1.5539e-01,\n",
       "           2.5867e-01, -2.1593e-01],\n",
       "         ...,\n",
       "         [-2.7771e-02,  1.1951e-01, -1.2347e-02,  ..., -5.5179e-02,\n",
       "          -1.3758e-02, -7.1947e-02],\n",
       "         [ 1.0936e-01,  1.4132e-01, -5.3433e-02,  ..., -1.1191e-01,\n",
       "           3.9082e-02, -2.0979e-01],\n",
       "         [ 1.4049e-01, -2.0949e-03, -2.0555e-02,  ..., -1.2471e-01,\n",
       "           5.1647e-02,  9.8916e-02]],\n",
       "\n",
       "        [[ 2.9925e-01, -9.7026e-02,  2.1008e-01,  ..., -1.5488e-03,\n",
       "          -1.6668e-01,  7.6129e-02],\n",
       "         [ 4.9138e-01, -2.9006e-01, -7.2477e-02,  ...,  1.8162e-02,\n",
       "          -9.8449e-02,  6.8968e-02],\n",
       "         [ 5.1945e-01, -1.7288e-01, -1.6923e-01,  ...,  6.2215e-03,\n",
       "          -5.0952e-02,  8.8868e-02],\n",
       "         ...,\n",
       "         [ 1.9846e-01,  8.5117e-02,  1.1869e-01,  ..., -1.3118e-02,\n",
       "           3.7326e-02, -5.6655e-02],\n",
       "         [ 9.0734e-02,  7.9024e-02,  3.1454e-01,  ...,  8.4315e-02,\n",
       "           8.1148e-03,  9.2897e-03],\n",
       "         [ 3.5592e-02, -4.4577e-02,  1.3667e-01,  ...,  6.0404e-02,\n",
       "           3.6660e-02,  6.4333e-02]]], device='cuda:0')], [tensor([[[ 1.0894e-04, -2.2712e-02, -3.8575e-01,  ..., -8.0180e-01,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 9.9860e-04,  2.4838e-07, -7.8956e-04,  ...,  2.5769e-02,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 3.3671e-01, -8.5474e-03,  7.8552e-03,  ..., -9.4378e-01,\n",
       "          -0.0000e+00, -0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00, -1.3832e-02,  0.0000e+00,  ..., -3.5700e-01,\n",
       "           1.7831e-02, -2.1039e-02],\n",
       "         [ 0.0000e+00, -4.3806e-03, -0.0000e+00,  ..., -1.8705e-03,\n",
       "           1.3250e-03, -2.4834e-03],\n",
       "         [ 0.0000e+00, -3.6371e-02, -0.0000e+00,  ..., -2.6913e-01,\n",
       "          -6.0045e-03, -1.1195e-02]],\n",
       "\n",
       "        [[ 0.0000e+00, -0.0000e+00, -4.8753e-02,  ...,  0.0000e+00,\n",
       "          -1.0598e-01, -6.0992e-03],\n",
       "         [ 0.0000e+00, -0.0000e+00,  1.3529e-01,  ...,  0.0000e+00,\n",
       "          -2.3273e-02,  1.4581e-02],\n",
       "         [ 0.0000e+00, -0.0000e+00,  7.7537e-02,  ...,  0.0000e+00,\n",
       "          -5.9202e-03, -6.5209e-02],\n",
       "         ...,\n",
       "         [ 3.6128e-01, -0.0000e+00,  1.8309e-02,  ..., -5.5261e-04,\n",
       "           0.0000e+00, -0.0000e+00],\n",
       "         [ 9.6610e-04, -0.0000e+00,  2.5606e-03,  ...,  1.4102e-04,\n",
       "          -0.0000e+00, -0.0000e+00],\n",
       "         [-4.8256e-06, -0.0000e+00,  2.6089e-02,  ..., -4.0424e-01,\n",
       "           0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "        [[-6.1533e-03, -7.8900e-02, -9.3139e-02,  ...,  1.8554e-01,\n",
       "          -6.2724e-02, -0.0000e+00],\n",
       "         [-3.9670e-02, -2.6074e-01,  8.3423e-02,  ...,  2.1307e-02,\n",
       "          -1.1192e-02, -0.0000e+00],\n",
       "         [-2.7200e-02, -4.6320e-01,  1.4622e-01,  ...,  2.7532e-02,\n",
       "          -3.8022e-03, -0.0000e+00],\n",
       "         ...,\n",
       "         [-2.4459e-03,  1.7135e-03, -2.0278e-03,  ..., -0.0000e+00,\n",
       "           1.9326e-01, -0.0000e+00],\n",
       "         [ 4.1407e-01,  2.5082e-04, -8.2806e-03,  ...,  0.0000e+00,\n",
       "           1.3615e-02, -0.0000e+00],\n",
       "         [ 1.6942e-01, -3.5398e-04, -1.3735e-04,  ..., -0.0000e+00,\n",
       "           2.6268e-01, -0.0000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000e+00, -9.7861e-02, -2.0610e-01,  ...,  1.6714e-01,\n",
       "          -0.0000e+00, -5.0805e-02],\n",
       "         [ 0.0000e+00, -1.7548e-01,  8.5472e-02,  ...,  3.8113e-02,\n",
       "          -0.0000e+00, -3.7646e-02],\n",
       "         [-0.0000e+00, -2.6513e-01,  1.8639e-01,  ...,  4.0169e-02,\n",
       "          -0.0000e+00, -4.8329e-02],\n",
       "         ...,\n",
       "         [-4.4027e-03, -3.7505e-01,  8.4970e-03,  ..., -4.9404e-01,\n",
       "          -5.0883e-03, -3.4819e-02],\n",
       "         [-4.3692e-06,  6.1238e-02,  1.1925e-03,  ..., -1.3865e+00,\n",
       "          -3.9511e-03, -7.9112e-02],\n",
       "         [-9.6087e-03, -1.1709e-01,  3.5546e-02,  ..., -1.2191e+00,\n",
       "          -6.7092e-03, -1.1827e-01]],\n",
       "\n",
       "        [[ 0.0000e+00, -0.0000e+00, -7.9149e-03,  ...,  1.6282e-01,\n",
       "          -6.7533e-02, -4.5424e-02],\n",
       "         [ 0.0000e+00, -0.0000e+00,  2.2811e-01,  ...,  4.2922e-02,\n",
       "          -1.2376e-02, -8.3279e-03],\n",
       "         [ 0.0000e+00, -0.0000e+00,  2.3836e-01,  ...,  4.7130e-02,\n",
       "          -7.9728e-03, -3.6795e-02],\n",
       "         ...,\n",
       "         [-0.0000e+00,  5.0275e-03,  1.4257e-01,  ...,  3.3221e-02,\n",
       "           1.6872e-03, -0.0000e+00],\n",
       "         [-0.0000e+00,  1.9035e-03,  2.8416e-01,  ..., -5.1836e-03,\n",
       "           3.4680e-03, -0.0000e+00],\n",
       "         [-0.0000e+00, -4.8118e-05,  3.3754e-02,  ..., -2.7902e-01,\n",
       "           6.1919e-02, -0.0000e+00]],\n",
       "\n",
       "        [[-3.6722e-02, -8.2472e-02, -9.1399e-02,  ...,  1.2021e-01,\n",
       "          -0.0000e+00, -0.0000e+00],\n",
       "         [ 3.7977e-03, -2.8208e-01,  2.4841e-01,  ...,  4.3918e-02,\n",
       "          -0.0000e+00, -0.0000e+00],\n",
       "         [-1.5300e-02, -5.3411e-01,  4.4489e-01,  ...,  3.1243e-02,\n",
       "          -0.0000e+00, -0.0000e+00],\n",
       "         ...,\n",
       "         [ 2.3097e-01, -0.0000e+00, -9.2229e-02,  ..., -2.5095e-01,\n",
       "           5.7909e-04, -3.6256e-03],\n",
       "         [-3.7453e-03, -0.0000e+00, -2.7729e-03,  ..., -7.7939e-04,\n",
       "           6.6601e-03, -2.3432e-01],\n",
       "         [-1.4240e-04,  0.0000e+00,  5.9409e-05,  ..., -1.2588e+00,\n",
       "           1.0589e-01, -2.7784e-01]]], device='cuda:0'), tensor([[[-0.0000e+00,  0.0000e+00, -1.9540e-02,  ..., -0.0000e+00,\n",
       "           1.9227e-04, -1.1139e-01],\n",
       "         [-0.0000e+00,  0.0000e+00, -1.1906e-02,  ..., -0.0000e+00,\n",
       "           3.2290e-03,  4.4332e-02],\n",
       "         [-0.0000e+00, -0.0000e+00, -8.6181e-04,  ..., -0.0000e+00,\n",
       "          -2.7367e-02,  1.2369e-02],\n",
       "         ...,\n",
       "         [-0.0000e+00, -2.8929e-04, -1.1133e-02,  ..., -4.1694e-02,\n",
       "           0.0000e+00,  2.5613e-01],\n",
       "         [-0.0000e+00,  1.1828e-05, -3.1740e-03,  ..., -1.0350e-02,\n",
       "          -0.0000e+00,  2.9895e-01],\n",
       "         [-0.0000e+00, -6.9369e-04, -1.5088e-02,  ..., -2.7215e-02,\n",
       "           0.0000e+00,  5.8160e-01]],\n",
       "\n",
       "        [[ 2.4222e-01, -5.0819e-04,  6.1816e-02,  ...,  0.0000e+00,\n",
       "          -3.7045e-03,  1.6298e-01],\n",
       "         [ 3.2049e-01,  1.4573e-02,  7.2926e-02,  ...,  0.0000e+00,\n",
       "          -4.7606e-03,  6.7030e-02],\n",
       "         [ 2.0398e-01,  1.2397e-02,  3.8090e-02,  ...,  0.0000e+00,\n",
       "          -2.2054e-04,  1.2408e-01],\n",
       "         ...,\n",
       "         [-0.0000e+00, -2.3920e-02, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           1.4594e-02,  4.7835e-03],\n",
       "         [-0.0000e+00, -1.3370e-02, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           5.9247e-05,  1.2157e-01],\n",
       "         [-0.0000e+00, -6.0525e-02, -0.0000e+00,  ..., -0.0000e+00,\n",
       "          -1.3071e-03,  8.5804e-02]],\n",
       "\n",
       "        [[ 3.3721e-01,  0.0000e+00,  3.2345e-03,  ...,  9.6630e-02,\n",
       "           2.4925e-03,  1.5130e-01],\n",
       "         [ 2.4597e-01,  0.0000e+00,  2.5401e-03,  ...,  3.6238e-02,\n",
       "           2.0124e-03,  6.4918e-02],\n",
       "         [ 1.4214e-01,  0.0000e+00, -9.0595e-04,  ...,  2.6482e-02,\n",
       "           4.5560e-04,  4.4777e-02],\n",
       "         ...,\n",
       "         [-1.2531e-01,  1.4339e-02,  0.0000e+00,  ...,  4.9715e-03,\n",
       "          -0.0000e+00,  0.0000e+00],\n",
       "         [-1.0780e-03,  3.7079e-02,  0.0000e+00,  ...,  5.9695e-03,\n",
       "          -0.0000e+00,  0.0000e+00],\n",
       "         [-3.2517e-02, -1.8773e-01,  0.0000e+00,  ...,  3.0311e-02,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000e+00, -1.6196e-01,  2.8599e-02,  ..., -2.7297e-03,\n",
       "           0.0000e+00,  1.0243e-01],\n",
       "         [ 0.0000e+00, -3.5500e-02,  2.2440e-02,  ..., -8.0173e-04,\n",
       "           0.0000e+00,  7.4661e-02],\n",
       "         [ 0.0000e+00, -1.7997e-02,  9.8931e-03,  ..., -6.3193e-04,\n",
       "           0.0000e+00,  1.0518e-01],\n",
       "         ...,\n",
       "         [-3.5919e-03, -1.1627e-02,  8.3241e-03,  ..., -0.0000e+00,\n",
       "           1.4635e-03,  6.5365e-02],\n",
       "         [-1.4508e-03, -8.4294e-02,  1.0421e-02,  ..., -0.0000e+00,\n",
       "           7.8201e-03,  7.1991e-03],\n",
       "         [-3.5911e-03, -7.6536e-02,  5.7818e-03,  ..., -0.0000e+00,\n",
       "           3.5751e-02,  1.9841e-02]],\n",
       "\n",
       "        [[ 1.8182e-01, -2.6830e-02, -0.0000e+00,  ..., -1.7480e-02,\n",
       "           0.0000e+00,  1.8847e-01],\n",
       "         [ 1.1815e-01,  8.7432e-04, -0.0000e+00,  ..., -6.4465e-03,\n",
       "           0.0000e+00,  1.6982e-01],\n",
       "         [ 5.9996e-02,  1.0815e-02, -0.0000e+00,  ..., -6.2025e-03,\n",
       "           0.0000e+00,  1.1052e-01],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  1.0451e+00, -8.0009e-02,  ...,  0.0000e+00,\n",
       "          -3.2783e-02,  3.9865e-03],\n",
       "         [ 0.0000e+00,  3.3123e-01, -5.3672e-02,  ...,  0.0000e+00,\n",
       "          -1.6711e-01,  1.3108e-03],\n",
       "         [-0.0000e+00, -1.4705e-02, -1.4663e-02,  ...,  0.0000e+00,\n",
       "           4.3929e-04,  1.2941e-01]],\n",
       "\n",
       "        [[-0.0000e+00,  0.0000e+00,  1.2546e-01,  ...,  7.7319e-02,\n",
       "           0.0000e+00,  1.7895e-01],\n",
       "         [-0.0000e+00,  0.0000e+00,  1.3232e-01,  ...,  1.2001e-02,\n",
       "           0.0000e+00,  1.0190e-01],\n",
       "         [-0.0000e+00,  0.0000e+00,  1.1120e-01,  ...,  8.8821e-03,\n",
       "           0.0000e+00,  7.1870e-02],\n",
       "         ...,\n",
       "         [-0.0000e+00,  3.7931e-02, -0.0000e+00,  ...,  6.9741e-05,\n",
       "          -6.4319e-02,  2.7558e-03],\n",
       "         [-0.0000e+00,  8.8343e-03, -0.0000e+00,  ...,  1.3540e-03,\n",
       "          -7.1783e-03,  2.3148e-03],\n",
       "         [-0.0000e+00, -2.1784e-03, -0.0000e+00,  ...,  4.0467e-02,\n",
       "           2.7331e-02,  6.9518e-04]]], device='cuda:0'), tensor([[[-1.2919e-01, -2.1932e-02, -4.5082e-04,  ..., -1.0603e-02,\n",
       "           1.4069e-02,  1.1889e-02],\n",
       "         [ 2.8419e-02, -2.8611e-03,  2.3472e-02,  ..., -4.7602e-02,\n",
       "           3.6627e-04,  1.5887e-02],\n",
       "         [ 1.2475e-01,  5.3548e-02,  2.0131e-01,  ..., -4.4011e-02,\n",
       "           1.9278e-02, -4.5495e-02],\n",
       "         ...,\n",
       "         [-2.6706e-02, -1.3804e-02,  2.3922e-01,  ..., -8.6332e-02,\n",
       "           3.3503e-02,  2.0578e-02],\n",
       "         [ 2.1288e-03,  2.1788e-01,  7.5887e-02,  ..., -8.4140e-02,\n",
       "          -2.5553e-02,  1.2757e-02],\n",
       "         [-1.4982e-02, -2.7845e-01,  2.0224e-01,  ..., -2.9595e-02,\n",
       "           7.1107e-02,  6.5311e-02]],\n",
       "\n",
       "        [[ 1.9499e-01,  1.1266e-01, -1.3396e-01,  ...,  2.8359e-02,\n",
       "           6.0726e-02,  1.2190e-01],\n",
       "         [ 1.9574e-01, -1.1273e-01, -5.5097e-02,  ...,  2.6595e-02,\n",
       "           2.1196e-03,  7.0395e-02],\n",
       "         [ 3.7261e-01, -1.9924e-01, -8.8316e-02,  ...,  4.1164e-02,\n",
       "          -4.9273e-03,  5.7505e-02],\n",
       "         ...,\n",
       "         [ 3.1973e-01,  1.3892e-03, -1.1955e-02,  ...,  3.8841e-02,\n",
       "           1.6763e-01, -1.1292e-01],\n",
       "         [ 2.1716e-01,  5.6376e-02, -1.6248e-01,  ...,  1.2371e-03,\n",
       "          -1.6321e-02,  7.7345e-02],\n",
       "         [ 1.4162e-03, -5.0357e-02,  2.5524e-04,  ...,  7.7663e-02,\n",
       "           4.5050e-02,  1.9471e-02]],\n",
       "\n",
       "        [[ 3.6848e-01,  3.1851e-01, -1.5211e-01,  ..., -9.3613e-03,\n",
       "          -1.5281e-01,  1.0674e-01],\n",
       "         [ 3.3681e-01, -1.6101e-01, -1.7395e-01,  ...,  4.7965e-03,\n",
       "          -1.7303e-01,  3.9620e-02],\n",
       "         [ 2.0666e-01, -8.1755e-02, -7.7578e-02,  ...,  5.4817e-02,\n",
       "          -1.3572e-01, -9.6070e-03],\n",
       "         ...,\n",
       "         [ 4.8067e-02,  1.6063e-01, -1.6051e-01,  ...,  1.4993e-01,\n",
       "           2.5302e-02, -1.0553e-01],\n",
       "         [ 6.0674e-02, -5.6108e-03, -2.4124e-02,  ...,  2.3901e-01,\n",
       "           1.4772e-02,  8.5772e-02],\n",
       "         [ 2.1981e-02, -3.8529e-02, -3.8571e-02,  ...,  9.1595e-02,\n",
       "           1.9997e-02,  7.7280e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.2126e-01,  3.6670e-01,  1.2069e-01,  ..., -4.0099e-02,\n",
       "          -1.8399e-01, -1.3405e-01],\n",
       "         [ 4.4693e-01, -1.8203e-02,  4.3047e-02,  ..., -1.3794e-02,\n",
       "          -2.5943e-01, -2.1818e-02],\n",
       "         [ 2.6644e-01, -9.4693e-02,  4.9351e-03,  ..., -6.2133e-02,\n",
       "          -7.5132e-02,  4.2256e-02],\n",
       "         ...,\n",
       "         [ 6.0405e-02,  1.0960e-02, -7.4542e-03,  ...,  1.0566e-01,\n",
       "           1.5526e-02, -8.9656e-02],\n",
       "         [ 6.4023e-02,  2.8964e-02, -4.3304e-02,  ...,  1.4300e-01,\n",
       "           3.7368e-03, -1.3434e-01],\n",
       "         [ 9.7949e-03, -4.0539e-03, -4.8839e-02,  ...,  2.4714e-02,\n",
       "          -2.4780e-02,  1.8520e-03]],\n",
       "\n",
       "        [[ 4.2491e-01,  3.9651e-01, -1.3471e-01,  ...,  7.9670e-02,\n",
       "           8.2702e-02, -2.1488e-01],\n",
       "         [ 5.7318e-01,  1.7269e-01, -4.0177e-02,  ...,  1.4540e-01,\n",
       "           1.6232e-01, -3.5918e-01],\n",
       "         [ 5.3253e-01,  8.2858e-02,  1.3912e-02,  ...,  1.5539e-01,\n",
       "           2.5867e-01, -2.1593e-01],\n",
       "         ...,\n",
       "         [-2.7771e-02,  1.1951e-01, -1.2347e-02,  ..., -5.5179e-02,\n",
       "          -1.3758e-02, -7.1947e-02],\n",
       "         [ 1.0936e-01,  1.4132e-01, -5.3433e-02,  ..., -1.1191e-01,\n",
       "           3.9082e-02, -2.0979e-01],\n",
       "         [ 1.4049e-01, -2.0949e-03, -2.0555e-02,  ..., -1.2471e-01,\n",
       "           5.1647e-02,  9.8916e-02]],\n",
       "\n",
       "        [[ 2.9925e-01, -9.7026e-02,  2.1008e-01,  ..., -1.5488e-03,\n",
       "          -1.6668e-01,  7.6129e-02],\n",
       "         [ 4.9138e-01, -2.9006e-01, -7.2477e-02,  ...,  1.8162e-02,\n",
       "          -9.8449e-02,  6.8968e-02],\n",
       "         [ 5.1945e-01, -1.7288e-01, -1.6923e-01,  ...,  6.2215e-03,\n",
       "          -5.0952e-02,  8.8868e-02],\n",
       "         ...,\n",
       "         [ 1.9846e-01,  8.5117e-02,  1.1869e-01,  ..., -1.3118e-02,\n",
       "           3.7326e-02, -5.6655e-02],\n",
       "         [ 9.0734e-02,  7.9024e-02,  3.1454e-01,  ...,  8.4315e-02,\n",
       "           8.1148e-03,  9.2897e-03],\n",
       "         [ 3.5592e-02, -4.4577e-02,  1.3667e-01,  ...,  6.0404e-02,\n",
       "           3.6660e-02,  6.4333e-02]]], device='cuda:0')], [tensor([[0.5619, 0.6603, 0.6253, 0.5555, 0.2238, 0.3868],\n",
       "        [0.7464, 0.4550, 0.5889, 0.6342, 0.5413, 0.2464],\n",
       "        [0.4799, 0.8494, 0.7090, 0.5848, 0.3918, 0.5882],\n",
       "        [0.3643, 0.5696, 0.6261, 0.6400, 0.5990, 0.4001],\n",
       "        [0.4924, 0.7281, 0.5788, 0.4417, 0.5847, 0.3944],\n",
       "        [0.4672, 0.8150, 0.6786, 0.5441, 0.6372, 0.5597],\n",
       "        [0.3820, 0.7128, 0.3209, 0.1287, 0.6234, 0.6290],\n",
       "        [0.2610, 0.7822, 0.4309, 0.4868, 0.6796, 0.4002],\n",
       "        [0.3406, 0.9088, 0.8150, 0.6060, 0.5335, 0.3336],\n",
       "        [0.5348, 0.3228, 0.5667, 0.2642, 0.2664, 0.2975],\n",
       "        [0.3467, 0.8593, 0.8353, 0.3824, 0.3056, 0.5011],\n",
       "        [0.5936, 0.5813, 0.5930, 0.4592, 0.4053, 0.4703],\n",
       "        [0.4728, 0.4880, 0.5579, 0.4274, 0.4590, 0.3309],\n",
       "        [0.3054, 0.3321, 0.4565, 0.2917, 0.3074, 0.5521],\n",
       "        [0.2808, 0.4412, 0.5166, 0.4610, 0.5219, 0.5464],\n",
       "        [0.6208, 0.8441, 0.8590, 0.8274, 0.1777, 0.6898],\n",
       "        [0.7959, 0.7406, 0.8283, 0.8938, 0.4199, 0.8164],\n",
       "        [0.4165, 0.8865, 0.7014, 0.7968, 0.3656, 0.8636],\n",
       "        [0.3741, 0.8435, 0.4602, 0.6107, 0.3790, 0.4424],\n",
       "        [0.3528, 0.4285, 0.7698, 0.5426, 0.5312, 0.4070],\n",
       "        [0.2771, 0.6409, 0.5542, 0.6972, 0.5535, 0.3046],\n",
       "        [0.3019, 0.2887, 0.5552, 0.3354, 0.4496, 0.3727],\n",
       "        [0.5162, 0.5416, 0.3952, 0.5601, 0.7675, 0.5401],\n",
       "        [0.2977, 0.6586, 0.7237, 0.2827, 0.4252, 0.8056],\n",
       "        [0.4084, 0.3337, 0.4124, 0.5039, 0.4649, 0.4201],\n",
       "        [0.5066, 0.7335, 0.2692, 0.4649, 0.6818, 0.6353],\n",
       "        [0.6191, 0.6765, 0.4093, 0.5701, 0.5835, 0.5698],\n",
       "        [0.1408, 0.6007, 0.4249, 0.4273, 0.4307, 0.5455],\n",
       "        [0.4597, 0.6111, 0.4704, 0.6063, 0.2599, 0.2637],\n",
       "        [0.5692, 0.7981, 0.4718, 0.4569, 0.7977, 0.3619],\n",
       "        [0.6374, 0.8379, 0.4511, 0.8554, 0.8231, 0.6604],\n",
       "        [0.6190, 0.8416, 0.7472, 0.6033, 0.3248, 0.4137],\n",
       "        [0.2289, 0.5661, 0.3856, 0.4650, 0.3678, 0.3110],\n",
       "        [0.7230, 0.5485, 0.1929, 0.1697, 0.4872, 0.1979],\n",
       "        [0.3228, 0.2558, 0.0947, 0.4384, 0.1871, 0.3610],\n",
       "        [0.2049, 0.1129, 0.2574, 0.4368, 0.3759, 0.4947],\n",
       "        [0.5362, 0.8743, 0.7252, 0.4742, 0.6678, 0.6881],\n",
       "        [0.4031, 0.6446, 0.3571, 0.6621, 0.5218, 0.5300],\n",
       "        [0.5367, 0.5061, 0.2983, 0.5759, 0.3514, 0.2640],\n",
       "        [0.2946, 0.5334, 0.4535, 0.4537, 0.3045, 0.4457],\n",
       "        [0.5211, 0.6229, 0.4603, 0.5079, 0.2273, 0.4956],\n",
       "        [0.6831, 0.6870, 0.6292, 0.6000, 0.4370, 0.2943],\n",
       "        [0.4889, 0.5728, 0.5323, 0.4226, 0.5566, 0.3825],\n",
       "        [0.5031, 0.8514, 0.8388, 0.5704, 0.3224, 0.7813],\n",
       "        [0.7193, 0.6455, 0.6922, 0.6866, 0.5519, 0.4575],\n",
       "        [0.3678, 0.5508, 0.6462, 0.3910, 0.3563, 0.5872],\n",
       "        [0.2845, 0.7903, 0.2469, 0.7494, 0.6214, 0.7492],\n",
       "        [0.4910, 0.3691, 0.4609, 0.4683, 0.4016, 0.3661],\n",
       "        [0.7987, 0.7162, 0.4037, 0.4879, 0.6965, 0.5936],\n",
       "        [0.4235, 0.6643, 0.5720, 0.2871, 0.5559, 0.3082],\n",
       "        [0.4525, 0.6424, 0.6104, 0.1325, 0.5282, 0.6723],\n",
       "        [0.4522, 0.7463, 0.3472, 0.5359, 0.2135, 0.5565],\n",
       "        [0.6050, 0.6129, 0.7860, 0.6459, 0.3358, 0.4123],\n",
       "        [0.5977, 0.5639, 0.4399, 0.6275, 0.6111, 0.5808],\n",
       "        [0.4719, 0.7547, 0.6047, 0.6128, 0.4224, 0.1827],\n",
       "        [0.2510, 0.6365, 0.5069, 0.5815, 0.4190, 0.3942],\n",
       "        [0.1717, 0.6926, 0.4988, 0.5308, 0.5765, 0.3921],\n",
       "        [0.2482, 0.6473, 0.5991, 0.4979, 0.6188, 0.4857],\n",
       "        [0.2054, 0.5224, 0.5757, 0.4410, 0.2623, 0.7262],\n",
       "        [0.2467, 0.7789, 0.4447, 0.4903, 0.5032, 0.7351],\n",
       "        [0.3534, 0.4568, 0.5218, 0.3987, 0.5436, 0.5320],\n",
       "        [0.4862, 0.5310, 0.6919, 0.5323, 0.6102, 0.4554],\n",
       "        [0.4394, 0.6244, 0.5038, 0.5410, 0.4438, 0.4211],\n",
       "        [0.4850, 0.6060, 0.4434, 0.6493, 0.5316, 0.4668],\n",
       "        [0.4950, 0.6405, 0.4738, 0.4442, 0.6931, 0.2640],\n",
       "        [0.5768, 0.3730, 0.5207, 0.3693, 0.7081, 0.6191],\n",
       "        [0.3953, 0.5360, 0.5309, 0.7191, 0.6406, 0.4557],\n",
       "        [0.1981, 0.6210, 0.5548, 0.3811, 0.6562, 0.5657],\n",
       "        [0.4750, 0.6158, 0.2874, 0.5179, 0.6377, 0.2413],\n",
       "        [0.3262, 0.6357, 0.5777, 0.0933, 0.4220, 0.4781],\n",
       "        [0.4050, 0.8289, 0.4664, 0.3989, 0.4731, 0.3251],\n",
       "        [0.3396, 0.7633, 0.4428, 0.5077, 0.4542, 0.1586],\n",
       "        [0.3198, 0.7930, 0.3686, 0.4279, 0.2925, 0.6973],\n",
       "        [0.3593, 0.4271, 0.5228, 0.3800, 0.5336, 0.3800],\n",
       "        [0.2503, 0.5506, 0.5171, 0.6975, 0.5941, 0.5849],\n",
       "        [0.4175, 0.6474, 0.3638, 0.5279, 0.4668, 0.2314],\n",
       "        [0.7060, 0.6537, 0.5048, 0.4118, 0.6586, 0.4348],\n",
       "        [0.5177, 0.4553, 0.6798, 0.6729, 0.6914, 0.4510],\n",
       "        [0.4775, 0.4456, 0.6118, 0.5388, 0.6366, 0.2825],\n",
       "        [0.4111, 0.7236, 0.2083, 0.3384, 0.7382, 0.5904],\n",
       "        [0.3377, 0.4355, 0.5158, 0.3821, 0.3952, 0.5967],\n",
       "        [0.4286, 0.6433, 0.6295, 0.5309, 0.6801, 0.3215],\n",
       "        [0.5102, 0.6777, 0.8317, 0.5010, 0.6963, 0.2807],\n",
       "        [0.2611, 0.8381, 0.7257, 0.5849, 0.8025, 0.4151],\n",
       "        [0.3094, 0.5178, 0.5261, 0.4213, 0.4918, 0.5220],\n",
       "        [0.3741, 0.6550, 0.3789, 0.4124, 0.7886, 0.4343],\n",
       "        [0.5414, 0.7144, 0.3394, 0.5403, 0.6826, 0.4027],\n",
       "        [0.2134, 0.2604, 0.4396, 0.5506, 0.6062, 0.2671],\n",
       "        [0.2827, 0.5977, 0.4785, 0.4718, 0.4835, 0.3903],\n",
       "        [0.3803, 0.6854, 0.7056, 0.5156, 0.3871, 0.5217],\n",
       "        [0.3681, 0.4449, 0.4590, 0.3147, 0.4909, 0.3815],\n",
       "        [0.5430, 0.6509, 0.6667, 0.5104, 0.6706, 0.4141],\n",
       "        [0.3128, 0.6894, 0.6905, 0.4251, 0.6735, 0.3878],\n",
       "        [0.3576, 0.7529, 0.4443, 0.3301, 0.3110, 0.7533],\n",
       "        [0.3622, 0.8275, 0.5467, 0.6099, 0.4764, 0.2399],\n",
       "        [0.3616, 0.7112, 0.8373, 0.5454, 0.4765, 0.4490],\n",
       "        [0.2183, 0.5857, 0.3761, 0.5946, 0.4739, 0.5071],\n",
       "        [0.5215, 0.7104, 0.7386, 0.4195, 0.4715, 0.4500],\n",
       "        [0.3608, 0.7375, 0.6448, 0.6987, 0.6608, 0.4613],\n",
       "        [0.7292, 0.4290, 0.4524, 0.5433, 0.6602, 0.4129],\n",
       "        [0.2591, 0.2783, 0.6088, 0.3492, 0.7745, 0.1385],\n",
       "        [0.2910, 0.6833, 0.6755, 0.5191, 0.6585, 0.5738],\n",
       "        [0.3240, 0.4963, 0.6159, 0.3653, 0.6200, 0.5173],\n",
       "        [0.4689, 0.5679, 0.5800, 0.2521, 0.2356, 0.8272],\n",
       "        [0.4827, 0.4085, 0.4662, 0.4603, 0.4909, 0.3800],\n",
       "        [0.3396, 0.3677, 0.5180, 0.4521, 0.3214, 0.4482],\n",
       "        [0.4988, 0.5463, 0.4464, 0.4364, 0.5518, 0.3998],\n",
       "        [0.2458, 0.7263, 0.4721, 0.5059, 0.6846, 0.5946],\n",
       "        [0.6021, 0.8173, 0.3884, 0.7332, 0.4844, 0.5030],\n",
       "        [0.4272, 0.5649, 0.4890, 0.5703, 0.4640, 0.4900],\n",
       "        [0.3841, 0.3656, 0.3066, 0.4048, 0.6920, 0.4113],\n",
       "        [0.3861, 0.7593, 0.3021, 0.6775, 0.6440, 0.4522],\n",
       "        [0.5176, 0.9047, 0.5124, 0.6592, 0.8650, 0.8679],\n",
       "        [0.2405, 0.3181, 0.4156, 0.2741, 0.5247, 0.3610],\n",
       "        [0.2864, 0.7931, 0.7239, 0.4145, 0.3478, 0.6112],\n",
       "        [0.3499, 0.6239, 0.5793, 0.5860, 0.4032, 0.5798],\n",
       "        [0.5950, 0.7155, 0.4876, 0.4545, 0.7344, 0.2623],\n",
       "        [0.5806, 0.7314, 0.6952, 0.2697, 0.5684, 0.5653],\n",
       "        [0.5023, 0.9065, 0.4203, 0.4252, 0.5454, 0.6893]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.5157, 0.5848, 0.7006, 0.5357, 0.5384, 0.5101],\n",
       "        [0.5742, 0.5622, 0.7753, 0.5100, 0.5717, 0.5206],\n",
       "        [0.7443, 0.7587, 0.7225, 0.5280, 0.5626, 0.5468],\n",
       "        [0.5681, 0.7364, 0.5251, 0.4620, 0.5681, 0.5452],\n",
       "        [0.6343, 0.9343, 0.7069, 0.6798, 0.6868, 0.5693],\n",
       "        [0.6751, 0.8538, 0.6238, 0.5383, 0.4711, 0.3298],\n",
       "        [0.2991, 0.8835, 0.7191, 0.4192, 0.5560, 0.7141],\n",
       "        [0.7293, 0.7857, 0.4880, 0.6658, 0.5754, 0.4388],\n",
       "        [0.4975, 0.7265, 0.4674, 0.5177, 0.5725, 0.4172],\n",
       "        [0.5406, 0.6945, 0.6473, 0.3696, 0.3932, 0.2813],\n",
       "        [0.5934, 0.7833, 0.7854, 0.8128, 0.2096, 0.7299],\n",
       "        [0.5593, 0.7513, 0.7673, 0.2057, 0.2243, 0.7326],\n",
       "        [0.5499, 0.5718, 0.6538, 0.4956, 0.4972, 0.6308],\n",
       "        [0.4035, 0.5067, 0.6134, 0.4828, 0.5351, 0.4486],\n",
       "        [0.4828, 0.6890, 0.5466, 0.5940, 0.4183, 0.5499],\n",
       "        [0.6682, 0.8286, 0.6290, 0.4770, 0.3215, 0.5504],\n",
       "        [0.6526, 0.6540, 0.6701, 0.4703, 0.3043, 0.7082],\n",
       "        [0.5707, 0.7010, 0.7340, 0.4494, 0.1677, 0.8328],\n",
       "        [0.4904, 0.4401, 0.6201, 0.4059, 0.5033, 0.4425],\n",
       "        [0.4436, 0.6367, 0.6351, 0.3934, 0.4401, 0.6201],\n",
       "        [0.5362, 0.7282, 0.7287, 0.5961, 0.5602, 0.4219],\n",
       "        [0.4324, 0.7000, 0.4121, 0.5479, 0.6080, 0.5699],\n",
       "        [0.8271, 0.8121, 0.8233, 0.7258, 0.3159, 0.5293],\n",
       "        [0.4529, 0.7646, 0.5903, 0.7094, 0.5655, 0.7248],\n",
       "        [0.4738, 0.7686, 0.5592, 0.7261, 0.6108, 0.3908],\n",
       "        [0.3711, 0.7544, 0.5031, 0.5107, 0.5122, 0.4179],\n",
       "        [0.2893, 0.8522, 0.6835, 0.5660, 0.2628, 0.7366],\n",
       "        [0.5087, 0.5787, 0.5934, 0.5014, 0.5057, 0.5619],\n",
       "        [0.4961, 0.7091, 0.8324, 0.7092, 0.4655, 0.6106],\n",
       "        [0.5788, 0.6625, 0.6117, 0.5301, 0.4130, 0.6332],\n",
       "        [0.4883, 0.6092, 0.6592, 0.6450, 0.5254, 0.5761],\n",
       "        [0.5984, 0.6898, 0.6633, 0.6176, 0.5622, 0.5265],\n",
       "        [0.5419, 0.5418, 0.7076, 0.4755, 0.5526, 0.5992],\n",
       "        [0.4962, 0.6737, 0.4006, 0.5283, 0.6575, 0.6106],\n",
       "        [0.5124, 0.5428, 0.6556, 0.5421, 0.7494, 0.4093],\n",
       "        [0.3577, 0.7330, 0.5717, 0.4784, 0.6953, 0.4887],\n",
       "        [0.5044, 0.4992, 0.5693, 0.3906, 0.3665, 0.6031],\n",
       "        [0.4483, 0.8094, 0.7656, 0.5367, 0.3677, 0.6874],\n",
       "        [0.5615, 0.5501, 0.5919, 0.6555, 0.5124, 0.5126],\n",
       "        [0.4131, 0.3477, 0.6237, 0.3942, 0.3129, 0.8689],\n",
       "        [0.5885, 0.6889, 0.4908, 0.6364, 0.5142, 0.7459],\n",
       "        [0.3403, 0.6004, 0.5542, 0.3792, 0.8233, 0.4846],\n",
       "        [0.4887, 0.7232, 0.5499, 0.4001, 0.5240, 0.3075],\n",
       "        [0.7038, 0.5506, 0.6089, 0.3515, 0.4679, 0.4550],\n",
       "        [0.3270, 0.5349, 0.6108, 0.4728, 0.4930, 0.4999],\n",
       "        [0.3325, 0.5761, 0.5249, 0.5518, 0.5819, 0.5884],\n",
       "        [0.3388, 0.5829, 0.5561, 0.6164, 0.6593, 0.4598],\n",
       "        [0.3957, 0.5263, 0.6434, 0.2743, 0.3675, 0.5202],\n",
       "        [0.3135, 0.6756, 0.6882, 0.4710, 0.6010, 0.5574],\n",
       "        [0.3059, 0.5817, 0.4921, 0.5280, 0.5114, 0.6856],\n",
       "        [0.5153, 0.3794, 0.6049, 0.5729, 0.4578, 0.7954],\n",
       "        [0.6425, 0.6993, 0.6612, 0.5402, 0.5573, 0.6841]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.6276, 0.3814, 0.4473, 0.1364, 0.7514, 0.4364],\n",
       "        [0.5427, 0.5233, 0.4789, 0.4061, 0.4712, 0.3192],\n",
       "        [0.8231, 0.5105, 0.5222, 0.5643, 0.6153, 0.6160],\n",
       "        [0.5596, 0.3146, 0.4398, 0.3798, 0.5617, 0.7501],\n",
       "        [0.5445, 0.4740, 0.4751, 0.3020, 0.2940, 0.7104],\n",
       "        [0.6738, 0.4376, 0.3969, 0.5338, 0.4340, 0.6381],\n",
       "        [0.7548, 0.5729, 0.5061, 0.4152, 0.4577, 0.4493],\n",
       "        [0.4133, 0.3108, 0.3898, 0.2651, 0.4716, 0.3914],\n",
       "        [0.4933, 0.2833, 0.3930, 0.4072, 0.6524, 0.4801],\n",
       "        [0.3634, 0.5085, 0.1822, 0.5237, 0.6006, 0.6624],\n",
       "        [0.5572, 0.4803, 0.2496, 0.3340, 0.6378, 0.4593],\n",
       "        [0.4840, 0.3609, 0.5871, 0.3497, 0.3032, 0.5722],\n",
       "        [0.4046, 0.4177, 0.5399, 0.3333, 0.5084, 0.5238],\n",
       "        [0.4798, 0.4420, 0.4501, 0.5541, 0.6248, 0.6034],\n",
       "        [0.2432, 0.6353, 0.3953, 0.4575, 0.7000, 0.4982],\n",
       "        [0.3091, 0.2737, 0.2834, 0.3990, 0.5345, 0.4959],\n",
       "        [0.4657, 0.6642, 0.4313, 0.2950, 0.6036, 0.4459],\n",
       "        [0.4628, 0.3942, 0.5274, 0.3678, 0.4440, 0.7004],\n",
       "        [0.6036, 0.2775, 0.5067, 0.4387, 0.6657, 0.4548],\n",
       "        [0.4047, 0.4622, 0.2514, 0.2301, 0.7148, 0.3154],\n",
       "        [0.5153, 0.6786, 0.4889, 0.4958, 0.6564, 0.5370],\n",
       "        [0.4533, 0.4649, 0.5856, 0.4961, 0.5610, 0.4896],\n",
       "        [0.4828, 0.5781, 0.6479, 0.5068, 0.5427, 0.4804],\n",
       "        [0.3412, 0.3499, 0.5339, 0.4629, 0.4490, 0.7467],\n",
       "        [0.5550, 0.3391, 0.6060, 0.4485, 0.5437, 0.6176],\n",
       "        [0.6459, 0.5414, 0.6117, 0.3848, 0.4842, 0.4138],\n",
       "        [0.6743, 0.2182, 0.3732, 0.3248, 0.6271, 0.4159],\n",
       "        [0.6442, 0.3395, 0.5323, 0.2932, 0.4771, 0.4869],\n",
       "        [0.6087, 0.4166, 0.4249, 0.2732, 0.6565, 0.4375],\n",
       "        [0.4983, 0.3734, 0.5097, 0.4102, 0.5007, 0.5221],\n",
       "        [0.5784, 0.3501, 0.4513, 0.2922, 0.5278, 0.5728],\n",
       "        [0.4221, 0.3698, 0.4765, 0.2979, 0.6085, 0.4705],\n",
       "        [0.5156, 0.5206, 0.6260, 0.3469, 0.4416, 0.5601],\n",
       "        [0.2866, 0.3754, 0.4963, 0.3579, 0.6654, 0.5356],\n",
       "        [0.5403, 0.3558, 0.4176, 0.4410, 0.5601, 0.3959],\n",
       "        [0.4803, 0.3081, 0.5014, 0.2664, 0.5487, 0.5472],\n",
       "        [0.3376, 0.5389, 0.5771, 0.4103, 0.5109, 0.5658],\n",
       "        [0.5097, 0.3088, 0.3952, 0.3857, 0.5734, 0.4086],\n",
       "        [0.4601, 0.4120, 0.4504, 0.4120, 0.6649, 0.5446],\n",
       "        [0.4523, 0.2751, 0.4631, 0.2523, 0.5210, 0.6322],\n",
       "        [0.4497, 0.2412, 0.5090, 0.2412, 0.5428, 0.4681],\n",
       "        [0.5133, 0.2446, 0.5185, 0.4183, 0.5953, 0.7445],\n",
       "        [0.4815, 0.4585, 0.5173, 0.4224, 0.4174, 0.5535],\n",
       "        [0.4108, 0.3873, 0.5814, 0.3133, 0.5308, 0.4869],\n",
       "        [0.3896, 0.3234, 0.6316, 0.2300, 0.5911, 0.4744],\n",
       "        [0.3101, 0.4398, 0.4732, 0.4544, 0.5456, 0.5792],\n",
       "        [0.4795, 0.2558, 0.5835, 0.5453, 0.6524, 0.3940],\n",
       "        [0.5020, 0.3549, 0.6116, 0.3992, 0.5494, 0.4903],\n",
       "        [0.5034, 0.2838, 0.3072, 0.5565, 0.5028, 0.4807],\n",
       "        [0.5256, 0.3884, 0.5056, 0.3720, 0.6023, 0.4976],\n",
       "        [0.4617, 0.3793, 0.5692, 0.3442, 0.6064, 0.5641],\n",
       "        [0.4192, 0.4394, 0.4971, 0.3264, 0.5640, 0.7011],\n",
       "        [0.4422, 0.3771, 0.4253, 0.3066, 0.7384, 0.4116],\n",
       "        [0.5197, 0.3895, 0.5108, 0.3914, 0.5438, 0.4721],\n",
       "        [0.5246, 0.2796, 0.4413, 0.3852, 0.6542, 0.5564],\n",
       "        [0.3806, 0.4074, 0.5848, 0.4147, 0.5524, 0.4471]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.5941, 0.5301, 0.5278, 0.5349, 0.4700, 0.7375],\n",
       "        [0.5279, 0.3135, 0.5715, 0.3351, 0.2393, 0.3428],\n",
       "        [0.6682, 0.5037, 0.4250, 0.5275, 0.4852, 0.4922],\n",
       "        [0.4584, 0.6333, 0.3373, 0.6165, 0.4022, 0.3187],\n",
       "        [0.5816, 0.6272, 0.3766, 0.4274, 0.6209, 0.2818],\n",
       "        [0.4846, 0.4327, 0.4674, 0.5273, 0.3623, 0.1922],\n",
       "        [0.7207, 0.6116, 0.6508, 0.6199, 0.1876, 0.4346],\n",
       "        [0.5956, 0.5573, 0.6294, 0.5610, 0.4279, 0.4434],\n",
       "        [0.6397, 0.5072, 0.6213, 0.6036, 0.4068, 0.3984],\n",
       "        [0.3621, 0.5172, 0.4494, 0.3907, 0.3566, 0.5634],\n",
       "        [0.5847, 0.6343, 0.3679, 0.5965, 0.6120, 0.4627],\n",
       "        [0.3591, 0.6058, 0.6537, 0.6757, 0.5572, 0.4534],\n",
       "        [0.3687, 0.4205, 0.6034, 0.6021, 0.3312, 0.5741],\n",
       "        [0.5208, 0.4223, 0.6239, 0.2478, 0.4869, 0.4883],\n",
       "        [0.5009, 0.2354, 0.4657, 0.2210, 0.4621, 0.4956],\n",
       "        [0.4138, 0.5649, 0.3596, 0.4468, 0.3767, 0.2069],\n",
       "        [0.5799, 0.5515, 0.5744, 0.6518, 0.4168, 0.5060],\n",
       "        [0.3557, 0.3207, 0.4999, 0.4400, 0.6243, 0.4406],\n",
       "        [0.6261, 0.2612, 0.5219, 0.3730, 0.6032, 0.4633],\n",
       "        [0.5333, 0.3782, 0.4016, 0.5703, 0.6697, 0.4663],\n",
       "        [0.7220, 0.6562, 0.4021, 0.4325, 0.6497, 0.6107],\n",
       "        [0.4705, 0.5753, 0.5350, 0.5546, 0.5842, 0.6091],\n",
       "        [0.1892, 0.4029, 0.4447, 0.2783, 0.6129, 0.6404],\n",
       "        [0.3897, 0.3183, 0.4575, 0.4116, 0.6879, 0.5209],\n",
       "        [0.4463, 0.5260, 0.6769, 0.3649, 0.5806, 0.5798],\n",
       "        [0.5022, 0.6201, 0.6183, 0.4682, 0.3887, 0.3841],\n",
       "        [0.5503, 0.3454, 0.5160, 0.3198, 0.7004, 0.4744],\n",
       "        [0.5051, 0.3621, 0.4875, 0.3416, 0.5826, 0.5510],\n",
       "        [0.4795, 0.2466, 0.3721, 0.2979, 0.3818, 0.5296],\n",
       "        [0.7023, 0.4880, 0.6433, 0.6184, 0.4331, 0.6601],\n",
       "        [0.3743, 0.3076, 0.5158, 0.3458, 0.3679, 0.6475],\n",
       "        [0.3502, 0.2354, 0.4472, 0.2136, 0.6657, 0.7511],\n",
       "        [0.4822, 0.5090, 0.4695, 0.3590, 0.5848, 0.5832],\n",
       "        [0.3020, 0.2806, 0.4881, 0.4249, 0.4277, 0.5354],\n",
       "        [0.4206, 0.4176, 0.6576, 0.4406, 0.4750, 0.4601],\n",
       "        [0.6114, 0.5028, 0.4844, 0.4432, 0.5491, 0.3936],\n",
       "        [0.2814, 0.8229, 0.6969, 0.3793, 0.5180, 0.6600],\n",
       "        [0.6651, 0.6015, 0.6673, 0.4765, 0.2393, 0.6864],\n",
       "        [0.4908, 0.6935, 0.6568, 0.4261, 0.5282, 0.5986],\n",
       "        [0.3570, 0.4178, 0.4317, 0.4182, 0.6836, 0.5461],\n",
       "        [0.6078, 0.1804, 0.4309, 0.2533, 0.6159, 0.4733],\n",
       "        [0.6633, 0.5665, 0.7043, 0.6513, 0.4547, 0.5218],\n",
       "        [0.5257, 0.3726, 0.5590, 0.3685, 0.4551, 0.7410]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4779, 0.7039, 0.4729, 0.4179, 0.5502, 0.7202],\n",
       "        [0.2795, 0.3076, 0.3972, 0.2625, 0.7557, 0.2157],\n",
       "        [0.2823, 0.3750, 0.2627, 0.6210, 0.7194, 0.1844],\n",
       "        [0.1676, 0.5164, 0.3376, 0.4225, 0.4529, 0.5097],\n",
       "        [0.7201, 0.7761, 0.4590, 0.2998, 0.7254, 0.6800],\n",
       "        [0.6251, 0.6772, 0.5220, 0.5636, 0.3990, 0.5157],\n",
       "        [0.3005, 0.7704, 0.5959, 0.2496, 0.2406, 0.5972],\n",
       "        [0.1741, 0.5751, 0.5776, 0.4885, 0.2957, 0.3374],\n",
       "        [0.5123, 0.6538, 0.6592, 0.4791, 0.4105, 0.4166],\n",
       "        [0.4504, 0.6242, 0.5804, 0.6322, 0.4648, 0.4977],\n",
       "        [0.5910, 0.5013, 0.3434, 0.2904, 0.5285, 0.4516],\n",
       "        [0.7005, 0.4561, 0.5031, 0.4199, 0.5222, 0.4656],\n",
       "        [0.5534, 0.6096, 0.3231, 0.4785, 0.3720, 0.6155],\n",
       "        [0.6003, 0.6858, 0.6532, 0.5253, 0.6600, 0.7688],\n",
       "        [0.3452, 0.8308, 0.3423, 0.6239, 0.4175, 0.4124],\n",
       "        [0.4573, 0.6500, 0.6686, 0.3204, 0.4481, 0.7339],\n",
       "        [0.4940, 0.7350, 0.5227, 0.4711, 0.4193, 0.7584],\n",
       "        [0.2872, 0.6726, 0.5456, 0.3654, 0.5278, 0.5797],\n",
       "        [0.4131, 0.5513, 0.4924, 0.3788, 0.3568, 0.3808],\n",
       "        [0.4019, 0.5334, 0.4487, 0.5343, 0.4929, 0.4752],\n",
       "        [0.2811, 0.2756, 0.4840, 0.2774, 0.3418, 0.3278],\n",
       "        [0.4250, 0.6561, 0.6795, 0.4396, 0.3983, 0.3317],\n",
       "        [0.6730, 0.8476, 0.4720, 0.5636, 0.5447, 0.5613],\n",
       "        [0.3064, 0.3188, 0.5983, 0.3347, 0.4325, 0.4728],\n",
       "        [0.2957, 0.8209, 0.6195, 0.7636, 0.5424, 0.2689],\n",
       "        [0.2220, 0.8461, 0.7221, 0.4792, 0.5325, 0.6915],\n",
       "        [0.4273, 0.6667, 0.5097, 0.3884, 0.5344, 0.4401],\n",
       "        [0.3487, 0.4326, 0.5993, 0.4440, 0.4979, 0.6290],\n",
       "        [0.5055, 0.6073, 0.7435, 0.4815, 0.3460, 0.4943],\n",
       "        [0.3893, 0.4985, 0.3997, 0.3994, 0.4123, 0.3297],\n",
       "        [0.3733, 0.7265, 0.6776, 0.6366, 0.4393, 0.3320],\n",
       "        [0.5776, 0.6529, 0.7431, 0.4698, 0.3857, 0.5976],\n",
       "        [0.6046, 0.6863, 0.6704, 0.3912, 0.3389, 0.5517],\n",
       "        [0.3961, 0.5463, 0.5036, 0.4544, 0.2838, 0.4080],\n",
       "        [0.4591, 0.6309, 0.5962, 0.5216, 0.2556, 0.4182],\n",
       "        [0.4394, 0.5579, 0.7908, 0.4469, 0.1685, 0.6979],\n",
       "        [0.4561, 0.4549, 0.6127, 0.4276, 0.5254, 0.5133],\n",
       "        [0.4208, 0.5055, 0.4197, 0.3762, 0.4183, 0.4293],\n",
       "        [0.3630, 0.8394, 0.5058, 0.5339, 0.4562, 0.4941],\n",
       "        [0.3406, 0.4009, 0.5220, 0.5266, 0.5436, 0.1852],\n",
       "        [0.4689, 0.8027, 0.5585, 0.5520, 0.7192, 0.4257],\n",
       "        [0.4646, 0.7137, 0.6375, 0.5586, 0.4002, 0.3514],\n",
       "        [0.4153, 0.5009, 0.6223, 0.6420, 0.7177, 0.4979],\n",
       "        [0.3219, 0.2572, 0.6486, 0.2955, 0.5252, 0.4869],\n",
       "        [0.5704, 0.5587, 0.5752, 0.5857, 0.6481, 0.4354],\n",
       "        [0.2166, 0.3466, 0.3974, 0.2092, 0.7543, 0.3475],\n",
       "        [0.3488, 0.8011, 0.3982, 0.1701, 0.3940, 0.4464],\n",
       "        [0.5844, 0.5894, 0.1582, 0.4628, 0.6051, 0.2795],\n",
       "        [0.4143, 0.7626, 0.3638, 0.2958, 0.7536, 0.4360],\n",
       "        [0.3977, 0.4807, 0.6077, 0.5860, 0.5173, 0.4913],\n",
       "        [0.4980, 0.6246, 0.4319, 0.1827, 0.8042, 0.3781],\n",
       "        [0.7343, 0.4151, 0.5009, 0.6852, 0.7113, 0.5638],\n",
       "        [0.3314, 0.7911, 0.6824, 0.7332, 0.7094, 0.3826],\n",
       "        [0.2479, 0.6718, 0.2210, 0.4984, 0.6204, 0.3495],\n",
       "        [0.3683, 0.7804, 0.8325, 0.5021, 0.2833, 0.5828],\n",
       "        [0.4952, 0.6318, 0.4789, 0.6717, 0.5599, 0.3730],\n",
       "        [0.4748, 0.7571, 0.6284, 0.3841, 0.6635, 0.4004],\n",
       "        [0.2738, 0.5878, 0.4611, 0.2664, 0.6852, 0.4955],\n",
       "        [0.7168, 0.8543, 0.6585, 0.6039, 0.4331, 0.5455],\n",
       "        [0.5176, 0.6794, 0.6530, 0.6522, 0.4570, 0.6590],\n",
       "        [0.3214, 0.6721, 0.5483, 0.4282, 0.6416, 0.4037],\n",
       "        [0.1832, 0.7192, 0.3193, 0.6372, 0.6147, 0.3663]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.3175, 0.5846, 0.3875, 0.2531, 0.4186, 0.4847],\n",
       "        [0.6299, 0.3728, 0.4906, 0.5069, 0.7714, 0.7384],\n",
       "        [0.3672, 0.3039, 0.0709, 0.2416, 0.5419, 0.3223],\n",
       "        [0.3186, 0.2014, 0.3988, 0.4192, 0.3460, 0.4222],\n",
       "        [0.4551, 0.4521, 0.3421, 0.4017, 0.4698, 0.5857],\n",
       "        [0.5831, 0.1841, 0.4810, 0.4528, 0.3787, 0.6031],\n",
       "        [0.3504, 0.3354, 0.3937, 0.2683, 0.4528, 0.6463],\n",
       "        [0.5429, 0.3855, 0.5393, 0.4115, 0.5427, 0.5301],\n",
       "        [0.4494, 0.6869, 0.3656, 0.5397, 0.2435, 0.6436],\n",
       "        [0.3131, 0.3448, 0.3000, 0.2637, 0.4685, 0.7201],\n",
       "        [0.4307, 0.4994, 0.3259, 0.6943, 0.6220, 0.6276],\n",
       "        [0.3915, 0.5244, 0.3397, 0.5805, 0.2507, 0.6535],\n",
       "        [0.7998, 0.5548, 0.2206, 0.3776, 0.7532, 0.2844],\n",
       "        [0.5807, 0.3593, 0.3791, 0.5365, 0.5497, 0.6255],\n",
       "        [0.2860, 0.2420, 0.4570, 0.3840, 0.7610, 0.4744],\n",
       "        [0.5910, 0.3182, 0.3938, 0.4231, 0.6978, 0.6651],\n",
       "        [0.5472, 0.5368, 0.3433, 0.5476, 0.6225, 0.4254],\n",
       "        [0.4956, 0.3799, 0.2371, 0.5332, 0.6476, 0.6697],\n",
       "        [0.3871, 0.2677, 0.2054, 0.4294, 0.6152, 0.6752],\n",
       "        [0.4603, 0.3593, 0.3783, 0.3846, 0.5567, 0.7862],\n",
       "        [0.6049, 0.5326, 0.3792, 0.5935, 0.6661, 0.6228],\n",
       "        [0.6037, 0.4076, 0.5936, 0.5961, 0.5487, 0.5818],\n",
       "        [0.4242, 0.4790, 0.5347, 0.4786, 0.4205, 0.5462],\n",
       "        [0.5108, 0.4457, 0.4280, 0.4428, 0.6043, 0.7145],\n",
       "        [0.5283, 0.3393, 0.5701, 0.5387, 0.6886, 0.7368],\n",
       "        [0.5459, 0.4749, 0.3417, 0.3120, 0.7476, 0.4829],\n",
       "        [0.5283, 0.5367, 0.5259, 0.2959, 0.4487, 0.3814],\n",
       "        [0.4850, 0.3457, 0.3763, 0.3997, 0.3719, 0.4128],\n",
       "        [0.3123, 0.4102, 0.4930, 0.3927, 0.7043, 0.6443],\n",
       "        [0.4801, 0.3397, 0.4899, 0.5384, 0.7228, 0.5331],\n",
       "        [0.5312, 0.5868, 0.6052, 0.6132, 0.4648, 0.6951],\n",
       "        [0.6812, 0.4615, 0.3014, 0.3755, 0.5442, 0.3011],\n",
       "        [0.2727, 0.6025, 0.5412, 0.4801, 0.8175, 0.3620],\n",
       "        [0.3546, 0.3510, 0.2430, 0.4201, 0.6220, 0.5342],\n",
       "        [0.6185, 0.4878, 0.4780, 0.4423, 0.4566, 0.6700],\n",
       "        [0.3577, 0.3989, 0.3248, 0.5077, 0.8222, 0.4602],\n",
       "        [0.4571, 0.3524, 0.3444, 0.3279, 0.6850, 0.5003],\n",
       "        [0.3884, 0.3897, 0.2944, 0.1921, 0.7157, 0.5509],\n",
       "        [0.3452, 0.5391, 0.6138, 0.3576, 0.4423, 0.4388],\n",
       "        [0.3727, 0.2465, 0.3289, 0.3659, 0.7879, 0.5836],\n",
       "        [0.4810, 0.3062, 0.4670, 0.6164, 0.5119, 0.6691],\n",
       "        [0.5103, 0.2946, 0.5780, 0.3975, 0.5512, 0.4088],\n",
       "        [0.4385, 0.3887, 0.5012, 0.6204, 0.7787, 0.5915],\n",
       "        [0.3721, 0.2039, 0.4820, 0.4365, 0.6405, 0.4708],\n",
       "        [0.4383, 0.1910, 0.4410, 0.6581, 0.7570, 0.6260],\n",
       "        [0.3815, 0.5351, 0.4395, 0.5154, 0.4845, 0.6481]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4786, 0.6660, 0.7623, 0.5876, 0.6851, 0.3168],\n",
       "        [0.7467, 0.6368, 0.3920, 0.6308, 0.4740, 0.2830],\n",
       "        [0.3680, 0.7133, 0.3547, 0.6897, 0.2517, 0.4015],\n",
       "        [0.5862, 0.5363, 0.3677, 0.4034, 0.5062, 0.4385],\n",
       "        [0.5530, 0.6368, 0.6581, 0.4554, 0.2621, 0.5483],\n",
       "        [0.4909, 0.3212, 0.3992, 0.2495, 0.2396, 0.2949],\n",
       "        [0.3366, 0.4043, 0.3398, 0.2998, 0.2761, 0.5450],\n",
       "        [0.5686, 0.3262, 0.4745, 0.3911, 0.3126, 0.5981],\n",
       "        [0.4972, 0.4545, 0.5135, 0.3300, 0.2550, 0.4110],\n",
       "        [0.4641, 0.3238, 0.7329, 0.5243, 0.3859, 0.5181],\n",
       "        [0.3596, 0.5749, 0.4961, 0.3878, 0.5113, 0.5115],\n",
       "        [0.4086, 0.3463, 0.4078, 0.4418, 0.4209, 0.2633],\n",
       "        [0.5093, 0.6048, 0.4228, 0.4679, 0.4242, 0.2778],\n",
       "        [0.5404, 0.4502, 0.5304, 0.5533, 0.3476, 0.5390],\n",
       "        [0.4395, 0.2798, 0.4775, 0.3496, 0.3217, 0.3807],\n",
       "        [0.4486, 0.5305, 0.3914, 0.4758, 0.4335, 0.5255],\n",
       "        [0.5218, 0.5471, 0.4191, 0.4038, 0.5117, 0.4387],\n",
       "        [0.4853, 0.5429, 0.5568, 0.4758, 0.6684, 0.6539],\n",
       "        [0.3540, 0.6285, 0.4723, 0.4308, 0.5256, 0.4367],\n",
       "        [0.4514, 0.4488, 0.4890, 0.2751, 0.3843, 0.2923],\n",
       "        [0.3359, 0.2650, 0.4590, 0.4709, 0.7008, 0.5360],\n",
       "        [0.3252, 0.3627, 0.5140, 0.3221, 0.7527, 0.5660],\n",
       "        [0.4056, 0.5500, 0.4544, 0.4182, 0.4353, 0.2549],\n",
       "        [0.3896, 0.5802, 0.4645, 0.3878, 0.6620, 0.4764],\n",
       "        [0.4140, 0.4998, 0.6076, 0.3895, 0.2513, 0.6187],\n",
       "        [0.2490, 0.6386, 0.3802, 0.5045, 0.4503, 0.5300],\n",
       "        [0.3008, 0.3889, 0.2665, 0.4573, 0.3497, 0.5941],\n",
       "        [0.3624, 0.4258, 0.5798, 0.4530, 0.6293, 0.4611],\n",
       "        [0.2900, 0.6375, 0.4579, 0.5536, 0.4605, 0.4248],\n",
       "        [0.5398, 0.6883, 0.5420, 0.3972, 0.5104, 0.5401],\n",
       "        [0.3079, 0.5190, 0.5553, 0.4497, 0.5252, 0.4572]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4306, 0.8400, 0.5216, 0.2777, 0.3741, 0.3771],\n",
       "        [0.7380, 0.6751, 0.6636, 0.3513, 0.4546, 0.1141],\n",
       "        [0.7765, 0.9091, 0.8605, 0.5604, 0.3147, 0.3648],\n",
       "        [0.3869, 0.8085, 0.6457, 0.4959, 0.4748, 0.5059],\n",
       "        [0.5351, 0.4974, 0.4554, 0.3621, 0.2345, 0.2770],\n",
       "        [0.5178, 0.7378, 0.5762, 0.4208, 0.3695, 0.3779],\n",
       "        [0.5300, 0.6033, 0.2748, 0.3302, 0.7952, 0.0836],\n",
       "        [0.3319, 0.3181, 0.4808, 0.3147, 0.1632, 0.2307],\n",
       "        [0.9046, 0.8133, 0.8015, 0.5504, 0.3409, 0.0696],\n",
       "        [0.5963, 0.7789, 0.5933, 0.1182, 0.1880, 0.6652],\n",
       "        [0.6903, 0.5602, 0.6497, 0.4593, 0.5813, 0.3265],\n",
       "        [0.6879, 0.7896, 0.6685, 0.5336, 0.4798, 0.1926],\n",
       "        [0.5685, 0.5839, 0.4936, 0.2998, 0.6120, 0.3337],\n",
       "        [0.6319, 0.4606, 0.6633, 0.5603, 0.5156, 0.4064],\n",
       "        [0.6080, 0.5469, 0.5561, 0.1462, 0.5927, 0.4362],\n",
       "        [0.6364, 0.6594, 0.5673, 0.5462, 0.5115, 0.4451],\n",
       "        [0.7387, 0.6780, 0.4256, 0.5226, 0.6912, 0.3534],\n",
       "        [0.6013, 0.4820, 0.6768, 0.4940, 0.3905, 0.4876],\n",
       "        [0.2944, 0.6121, 0.6277, 0.3634, 0.4289, 0.3858],\n",
       "        [0.5082, 0.6732, 0.4508, 0.4338, 0.7659, 0.4824],\n",
       "        [0.3007, 0.2526, 0.5956, 0.5241, 0.5373, 0.4366],\n",
       "        [0.4856, 0.5212, 0.5594, 0.4364, 0.5399, 0.4695],\n",
       "        [0.5851, 0.6356, 0.7525, 0.6651, 0.5420, 0.3510],\n",
       "        [0.6337, 0.8231, 0.6991, 0.6787, 0.3776, 0.4009],\n",
       "        [0.4781, 0.3768, 0.4038, 0.3124, 0.6592, 0.2330],\n",
       "        [0.6813, 0.6703, 0.6925, 0.3788, 0.4865, 0.3943],\n",
       "        [0.4706, 0.5408, 0.4569, 0.3454, 0.5764, 0.2656],\n",
       "        [0.6245, 0.6189, 0.6507, 0.5170, 0.3837, 0.1851],\n",
       "        [0.4432, 0.8010, 0.4802, 0.5889, 0.5310, 0.3303],\n",
       "        [0.3321, 0.5739, 0.4754, 0.2880, 0.8252, 0.4618],\n",
       "        [0.5102, 0.3348, 0.6439, 0.6216, 0.3934, 0.6577],\n",
       "        [0.4802, 0.7570, 0.5648, 0.6217, 0.5172, 0.1839],\n",
       "        [0.5868, 0.4732, 0.5732, 0.5747, 0.6696, 0.5226],\n",
       "        [0.2429, 0.2831, 0.3825, 0.2420, 0.6389, 0.4302],\n",
       "        [0.5320, 0.5299, 0.6073, 0.5982, 0.3717, 0.4745],\n",
       "        [0.3493, 0.3441, 0.3842, 0.4201, 0.5297, 0.5859],\n",
       "        [0.6477, 0.4173, 0.4852, 0.4600, 0.5651, 0.4767],\n",
       "        [0.4597, 0.4940, 0.4517, 0.5819, 0.5951, 0.3187],\n",
       "        [0.4197, 0.4938, 0.5990, 0.3890, 0.6111, 0.4421],\n",
       "        [0.6913, 0.6059, 0.7384, 0.7563, 0.3017, 0.4807],\n",
       "        [0.5450, 0.6867, 0.7968, 0.4444, 0.4449, 0.5323],\n",
       "        [0.6217, 0.5583, 0.7550, 0.3418, 0.4574, 0.6012],\n",
       "        [0.5964, 0.6867, 0.7074, 0.6173, 0.6453, 0.3326],\n",
       "        [0.5554, 0.3797, 0.6355, 0.6751, 0.5539, 0.4966],\n",
       "        [0.5569, 0.5160, 0.5211, 0.4627, 0.5383, 0.2035],\n",
       "        [0.5524, 0.3166, 0.7205, 0.6472, 0.4818, 0.4904]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4850, 0.3017, 0.4334, 0.5544, 0.7545, 0.4409],\n",
       "        [0.4393, 0.5157, 0.4941, 0.5331, 0.7253, 0.4454],\n",
       "        [0.5689, 0.6711, 0.2831, 0.6506, 0.6853, 0.2748],\n",
       "        [0.4611, 0.4305, 0.5283, 0.6192, 0.6397, 0.4205],\n",
       "        [0.6128, 0.4719, 0.3815, 0.2163, 0.4258, 0.5802],\n",
       "        [0.5866, 0.5476, 0.4605, 0.5905, 0.5884, 0.2465],\n",
       "        [0.3945, 0.5486, 0.6688, 0.2967, 0.2907, 0.2873],\n",
       "        [0.4199, 0.3801, 0.4751, 0.4587, 0.7358, 0.6766],\n",
       "        [0.4379, 0.7064, 0.6304, 0.4437, 0.5544, 0.4067],\n",
       "        [0.6455, 0.4304, 0.5332, 0.7019, 0.4521, 0.5136],\n",
       "        [0.5838, 0.5203, 0.7215, 0.6008, 0.5633, 0.4106],\n",
       "        [0.3305, 0.3782, 0.3988, 0.6462, 0.7807, 0.2873],\n",
       "        [0.2112, 0.5286, 0.5123, 0.8258, 0.5992, 0.3103],\n",
       "        [0.2794, 0.3233, 0.3899, 0.4536, 0.7287, 0.3899],\n",
       "        [0.4180, 0.2533, 0.5518, 0.6932, 0.6594, 0.5296],\n",
       "        [0.5343, 0.5534, 0.5830, 0.6467, 0.6290, 0.5722],\n",
       "        [0.5428, 0.7507, 0.3140, 0.8651, 0.6198, 0.2162],\n",
       "        [0.8095, 0.2939, 0.4679, 0.5869, 0.1732, 0.7699],\n",
       "        [0.4022, 0.3007, 0.3766, 0.4707, 0.5279, 0.2906],\n",
       "        [0.2351, 0.5450, 0.6845, 0.4823, 0.4469, 0.6570],\n",
       "        [0.2082, 0.6183, 0.4389, 0.3792, 0.5835, 0.3625],\n",
       "        [0.4537, 0.5713, 0.5091, 0.5752, 0.7191, 0.6250],\n",
       "        [0.3247, 0.4624, 0.5641, 0.4780, 0.6046, 0.5637],\n",
       "        [0.3670, 0.5832, 0.5256, 0.4864, 0.6561, 0.4878],\n",
       "        [0.7444, 0.8843, 0.6304, 0.5091, 0.4013, 0.3732],\n",
       "        [0.8178, 0.5653, 0.6824, 0.4946, 0.6639, 0.5265],\n",
       "        [0.4988, 0.3828, 0.4407, 0.4725, 0.5261, 0.3163],\n",
       "        [0.5484, 0.2845, 0.5394, 0.5348, 0.7009, 0.5035],\n",
       "        [0.3315, 0.5204, 0.5287, 0.5522, 0.6276, 0.3206],\n",
       "        [0.5350, 0.4351, 0.4793, 0.3323, 0.6259, 0.5360],\n",
       "        [0.4879, 0.7094, 0.2506, 0.5304, 0.6889, 0.4391],\n",
       "        [0.5334, 0.4094, 0.5324, 0.5369, 0.6822, 0.4974],\n",
       "        [0.4317, 0.3756, 0.5501, 0.4726, 0.7500, 0.4150],\n",
       "        [0.5260, 0.7257, 0.5743, 0.6481, 0.5567, 0.3735],\n",
       "        [0.6154, 0.6450, 0.6228, 0.4620, 0.5678, 0.5111],\n",
       "        [0.7221, 0.2546, 0.3539, 0.5523, 0.6355, 0.5168],\n",
       "        [0.5063, 0.3014, 0.4344, 0.3489, 0.6945, 0.4205],\n",
       "        [0.3219, 0.3958, 0.3330, 0.5461, 0.7263, 0.2273],\n",
       "        [0.5330, 0.4508, 0.6214, 0.4094, 0.6094, 0.3572],\n",
       "        [0.6090, 0.3973, 0.4736, 0.5667, 0.5749, 0.5419],\n",
       "        [0.6889, 0.4750, 0.4877, 0.3788, 0.5256, 0.4689],\n",
       "        [0.4825, 0.2555, 0.4704, 0.5418, 0.6338, 0.4000],\n",
       "        [0.4039, 0.6675, 0.5950, 0.4418, 0.7726, 0.2337],\n",
       "        [0.3453, 0.5001, 0.6593, 0.4013, 0.6099, 0.3473]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.7722, 0.7335, 0.5873, 0.4289, 0.5552, 0.8038],\n",
       "        [0.5542, 0.5128, 0.6435, 0.4308, 0.5732, 0.7999],\n",
       "        [0.4862, 0.5212, 0.4667, 0.2387, 0.2764, 0.6386],\n",
       "        [0.5640, 0.7061, 0.4874, 0.2844, 0.4829, 0.5900],\n",
       "        [0.6033, 0.5748, 0.5236, 0.3947, 0.6585, 0.6436],\n",
       "        [0.7643, 0.2690, 0.4127, 0.3903, 0.4100, 0.3843],\n",
       "        [0.5828, 0.7493, 0.6014, 0.4831, 0.6564, 0.4340],\n",
       "        [0.5166, 0.7447, 0.3715, 0.2569, 0.6526, 0.5734],\n",
       "        [0.4603, 0.7589, 0.4019, 0.4722, 0.1176, 0.7553],\n",
       "        [0.4106, 0.5391, 0.5431, 0.4355, 0.4307, 0.3224],\n",
       "        [0.5730, 0.4418, 0.5984, 0.5046, 0.4839, 0.6096],\n",
       "        [0.5348, 0.7079, 0.6942, 0.2736, 0.4399, 0.4577],\n",
       "        [0.5452, 0.4163, 0.5047, 0.1760, 0.5200, 0.4150],\n",
       "        [0.5193, 0.7337, 0.7699, 0.1440, 0.3640, 0.6570],\n",
       "        [0.2925, 0.6727, 0.6108, 0.1745, 0.2508, 0.6686],\n",
       "        [0.4177, 0.7429, 0.5543, 0.4368, 0.4326, 0.6648],\n",
       "        [0.7296, 0.4253, 0.3936, 0.3941, 0.6645, 0.5095],\n",
       "        [0.5314, 0.6191, 0.7678, 0.2726, 0.4809, 0.6512],\n",
       "        [0.4043, 0.5936, 0.6408, 0.3168, 0.5009, 0.7405],\n",
       "        [0.3128, 0.5011, 0.5143, 0.2333, 0.4675, 0.3399],\n",
       "        [0.6169, 0.4440, 0.7790, 0.3785, 0.3947, 0.5489],\n",
       "        [0.6401, 0.4378, 0.7534, 0.4552, 0.7727, 0.6057],\n",
       "        [0.5575, 0.6544, 0.7406, 0.4195, 0.5873, 0.4889],\n",
       "        [0.6197, 0.5479, 0.6443, 0.3699, 0.2550, 0.7605],\n",
       "        [0.8039, 0.3752, 0.6209, 0.5501, 0.3690, 0.7150],\n",
       "        [0.4621, 0.3634, 0.6499, 0.4255, 0.6357, 0.5044],\n",
       "        [0.6043, 0.3879, 0.5281, 0.4713, 0.5384, 0.5719],\n",
       "        [0.3419, 0.2332, 0.3514, 0.2924, 0.6714, 0.3662],\n",
       "        [0.5465, 0.3387, 0.7111, 0.4129, 0.5234, 0.7580],\n",
       "        [0.5036, 0.4017, 0.6304, 0.2229, 0.5667, 0.5052],\n",
       "        [0.3462, 0.5572, 0.6904, 0.2116, 0.4801, 0.6633]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4671, 0.3783, 0.5463, 0.4023, 0.3920, 0.3913],\n",
       "        [0.7690, 0.4573, 0.6382, 0.5128, 0.5163, 0.3565],\n",
       "        [0.5950, 0.3043, 0.6247, 0.4772, 0.6862, 0.6114],\n",
       "        [0.5034, 0.3693, 0.4965, 0.5426, 0.5521, 0.5800],\n",
       "        [0.4667, 0.7224, 0.5862, 0.3785, 0.5619, 0.2927],\n",
       "        [0.5189, 0.7044, 0.7039, 0.3644, 0.5149, 0.6525],\n",
       "        [0.5390, 0.5785, 0.6303, 0.4420, 0.5279, 0.4305],\n",
       "        [0.4987, 0.7144, 0.6373, 0.4421, 0.4009, 0.1354],\n",
       "        [0.3638, 0.5536, 0.5217, 0.4908, 0.4482, 0.3775],\n",
       "        [0.6221, 0.3525, 0.6120, 0.4943, 0.4876, 0.3897],\n",
       "        [0.6738, 0.5408, 0.6015, 0.5594, 0.3868, 0.3402],\n",
       "        [0.7256, 0.5937, 0.7575, 0.6372, 0.3600, 0.4492],\n",
       "        [0.4890, 0.5667, 0.7077, 0.6060, 0.5167, 0.4784],\n",
       "        [0.6600, 0.3391, 0.4962, 0.6314, 0.4918, 0.5014],\n",
       "        [0.5678, 0.4920, 0.6720, 0.5155, 0.2477, 0.3374],\n",
       "        [0.3591, 0.4545, 0.5348, 0.6501, 0.4320, 0.5543],\n",
       "        [0.6895, 0.3849, 0.5940, 0.4509, 0.5563, 0.4372],\n",
       "        [0.3476, 0.2650, 0.4902, 0.6028, 0.6237, 0.3921],\n",
       "        [0.3624, 0.5197, 0.4683, 0.7552, 0.6647, 0.3371],\n",
       "        [0.3824, 0.3275, 0.5044, 0.4065, 0.4446, 0.3986],\n",
       "        [0.3214, 0.5467, 0.5606, 0.6429, 0.5952, 0.6128],\n",
       "        [0.4969, 0.3268, 0.4787, 0.4932, 0.5125, 0.5549],\n",
       "        [0.5076, 0.5537, 0.6414, 0.3263, 0.7294, 0.4611],\n",
       "        [0.3893, 0.2899, 0.4218, 0.5698, 0.5736, 0.5083],\n",
       "        [0.4330, 0.7361, 0.4577, 0.5604, 0.4643, 0.4588],\n",
       "        [0.4488, 0.5666, 0.5404, 0.2797, 0.4090, 0.5409],\n",
       "        [0.3169, 0.3307, 0.4644, 0.2924, 0.6187, 0.7377],\n",
       "        [0.5361, 0.4881, 0.5173, 0.4194, 0.5278, 0.4153],\n",
       "        [0.3703, 0.3491, 0.4283, 0.3530, 0.4582, 0.3730],\n",
       "        [0.4612, 0.2847, 0.4008, 0.5807, 0.5571, 0.4247],\n",
       "        [0.4915, 0.3843, 0.2396, 0.3610, 0.4376, 0.6162],\n",
       "        [0.4840, 0.3353, 0.5449, 0.4646, 0.5228, 0.5091],\n",
       "        [0.5487, 0.5962, 0.6297, 0.5811, 0.6146, 0.6099],\n",
       "        [0.3977, 0.6171, 0.4613, 0.6112, 0.6055, 0.4940],\n",
       "        [0.5174, 0.3985, 0.5017, 0.7249, 0.2967, 0.4234],\n",
       "        [0.3762, 0.4702, 0.5112, 0.4045, 0.6284, 0.3844],\n",
       "        [0.4766, 0.4801, 0.7200, 0.4628, 0.5250, 0.3997]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4594, 0.5922, 0.3867, 0.6517, 0.7664, 0.4237],\n",
       "        [0.4952, 0.5571, 0.6736, 0.8110, 0.1522, 0.7062],\n",
       "        [0.4492, 0.2957, 0.4010, 0.4033, 0.2881, 0.2057],\n",
       "        [0.3498, 0.5364, 0.4332, 0.4690, 0.6035, 0.3625],\n",
       "        [0.6499, 0.5045, 0.4999, 0.5582, 0.5125, 0.5229],\n",
       "        [0.5663, 0.2551, 0.7398, 0.6717, 0.1978, 0.3570],\n",
       "        [0.6173, 0.4259, 0.5793, 0.4761, 0.3816, 0.4403],\n",
       "        [0.4235, 0.5822, 0.6728, 0.6641, 0.4904, 0.6759],\n",
       "        [0.5796, 0.5454, 0.4587, 0.7281, 0.4177, 0.5851],\n",
       "        [0.7094, 0.4380, 0.7388, 0.6011, 0.4690, 0.4509],\n",
       "        [0.7206, 0.2855, 0.5304, 0.6484, 0.4378, 0.5416],\n",
       "        [0.6248, 0.3485, 0.4985, 0.5727, 0.4247, 0.5258],\n",
       "        [0.5470, 0.4683, 0.5943, 0.6002, 0.7077, 0.4994],\n",
       "        [0.4755, 0.4071, 0.4567, 0.5031, 0.5552, 0.4621],\n",
       "        [0.6045, 0.3771, 0.6404, 0.5913, 0.5884, 0.4790],\n",
       "        [0.5825, 0.3511, 0.3522, 0.3830, 0.4717, 0.3767],\n",
       "        [0.3503, 0.5110, 0.3570, 0.4747, 0.5069, 0.4161],\n",
       "        [0.6957, 0.4559, 0.5296, 0.6516, 0.5672, 0.3314],\n",
       "        [0.4890, 0.6812, 0.6338, 0.4308, 0.4790, 0.5840]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.2425, 0.3520, 0.4203, 0.5264, 0.4860, 0.5392],\n",
       "        [0.7324, 0.3695, 0.2522, 0.4291, 0.8027, 0.6780],\n",
       "        [0.4569, 0.6299, 0.3704, 0.5470, 0.6162, 0.1976],\n",
       "        [0.7433, 0.5882, 0.5308, 0.7587, 0.1883, 0.7040],\n",
       "        [0.3128, 0.7826, 0.3084, 0.7758, 0.5158, 0.6829],\n",
       "        [0.4848, 0.3165, 0.4208, 0.6175, 0.3330, 0.6611],\n",
       "        [0.4263, 0.5627, 0.5380, 0.5131, 0.5127, 0.5214],\n",
       "        [0.6046, 0.5337, 0.5489, 0.7139, 0.4568, 0.7401],\n",
       "        [0.3751, 0.4261, 0.3699, 0.4864, 0.2967, 0.6074],\n",
       "        [0.5907, 0.3607, 0.2992, 0.3622, 0.3798, 0.4376],\n",
       "        [0.4263, 0.5772, 0.3652, 0.6834, 0.5395, 0.6274],\n",
       "        [0.6761, 0.7179, 0.6237, 0.5359, 0.1608, 0.7033],\n",
       "        [0.7233, 0.4751, 0.4005, 0.5015, 0.2650, 0.3798],\n",
       "        [0.6961, 0.8327, 0.5277, 0.4421, 0.4485, 0.8763],\n",
       "        [0.5223, 0.3124, 0.4037, 0.5377, 0.3427, 0.5157],\n",
       "        [0.6804, 0.6156, 0.7348, 0.5981, 0.3062, 0.3798],\n",
       "        [0.3205, 0.3666, 0.4877, 0.4235, 0.3716, 0.5509],\n",
       "        [0.6642, 0.6320, 0.5256, 0.4478, 0.3605, 0.6536],\n",
       "        [0.6083, 0.5470, 0.7096, 0.6789, 0.5462, 0.5664],\n",
       "        [0.5836, 0.5431, 0.3160, 0.7098, 0.5269, 0.6796],\n",
       "        [0.6012, 0.7570, 0.6551, 0.5364, 0.7123, 0.5776],\n",
       "        [0.5349, 0.2693, 0.4636, 0.3209, 0.5907, 0.4540],\n",
       "        [0.7413, 0.2120, 0.4053, 0.5723, 0.4512, 0.4138],\n",
       "        [0.3818, 0.5534, 0.4325, 0.8216, 0.4429, 0.6709],\n",
       "        [0.3829, 0.7776, 0.6896, 0.9065, 0.2976, 0.5890],\n",
       "        [0.6325, 0.7137, 0.5393, 0.4975, 0.3859, 0.4112],\n",
       "        [0.3080, 0.2676, 0.3709, 0.5568, 0.4954, 0.6222],\n",
       "        [0.5602, 0.3646, 0.5241, 0.5825, 0.5456, 0.4830],\n",
       "        [0.5925, 0.6166, 0.6151, 0.6040, 0.5029, 0.6541],\n",
       "        [0.2537, 0.2901, 0.5916, 0.3922, 0.4139, 0.2512],\n",
       "        [0.4040, 0.4885, 0.4900, 0.3892, 0.4619, 0.4997],\n",
       "        [0.5976, 0.5979, 0.6115, 0.6368, 0.5598, 0.6984],\n",
       "        [0.5060, 0.4640, 0.4961, 0.4272, 0.3685, 0.6526],\n",
       "        [0.4627, 0.3824, 0.7038, 0.2800, 0.3629, 0.5644],\n",
       "        [0.6431, 0.1847, 0.3281, 0.2651, 0.6968, 0.4399],\n",
       "        [0.2397, 0.4158, 0.4809, 0.6531, 0.2697, 0.4745],\n",
       "        [0.4132, 0.4239, 0.5155, 0.4501, 0.4949, 0.4043],\n",
       "        [0.5116, 0.4755, 0.3475, 0.4479, 0.4745, 0.4579],\n",
       "        [0.6541, 0.6092, 0.5689, 0.5809, 0.1840, 0.4106],\n",
       "        [0.6719, 0.6575, 0.7543, 0.4692, 0.3276, 0.5333],\n",
       "        [0.4508, 0.5052, 0.6149, 0.4855, 0.2406, 0.4925],\n",
       "        [0.5523, 0.4178, 0.6019, 0.5595, 0.5385, 0.5049]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.6206, 0.7878, 0.6391, 0.6702, 0.5341, 0.5187],\n",
       "        [0.8291, 0.4190, 0.5307, 0.7270, 0.5435, 0.6404],\n",
       "        [0.5138, 0.3230, 0.4502, 0.5888, 0.7254, 0.7034],\n",
       "        [0.4736, 0.5749, 0.5756, 0.7160, 0.6895, 0.6584],\n",
       "        [0.6720, 0.5528, 0.5733, 0.7293, 0.5137, 0.8509],\n",
       "        [0.7080, 0.7550, 0.3696, 0.6339, 0.7418, 0.5167],\n",
       "        [0.5631, 0.4523, 0.6737, 0.3757, 0.6165, 0.7944],\n",
       "        [0.5826, 0.5942, 0.4744, 0.5214, 0.6195, 0.4346],\n",
       "        [0.6522, 0.6979, 0.7107, 0.7259, 0.6424, 0.6437],\n",
       "        [0.5972, 0.7578, 0.7406, 0.7623, 0.4577, 0.5087],\n",
       "        [0.3949, 0.4994, 0.5282, 0.6381, 0.6613, 0.6413],\n",
       "        [0.4566, 0.5490, 0.4791, 0.3366, 0.5427, 0.5693],\n",
       "        [0.5647, 0.3704, 0.5524, 0.5537, 0.6370, 0.4897],\n",
       "        [0.6199, 0.4679, 0.7654, 0.4383, 0.6366, 0.4398],\n",
       "        [0.6448, 0.3982, 0.6703, 0.4817, 0.3940, 0.5785],\n",
       "        [0.3186, 0.2740, 0.5784, 0.4766, 0.8231, 0.5786],\n",
       "        [0.5103, 0.5830, 0.6218, 0.5192, 0.6111, 0.6338],\n",
       "        [0.5564, 0.4753, 0.7194, 0.5765, 0.5743, 0.6539],\n",
       "        [0.3169, 0.2431, 0.4050, 0.4289, 0.7089, 0.7030],\n",
       "        [0.2829, 0.2421, 0.4632, 0.6374, 0.6315, 0.7320],\n",
       "        [0.4925, 0.4618, 0.6294, 0.7255, 0.5376, 0.6233],\n",
       "        [0.6756, 0.3954, 0.6654, 0.6112, 0.5450, 0.6989],\n",
       "        [0.4513, 0.3493, 0.6150, 0.4491, 0.5827, 0.6800],\n",
       "        [0.2993, 0.5787, 0.5834, 0.4041, 0.6850, 0.6149],\n",
       "        [0.4932, 0.2922, 0.4615, 0.5598, 0.7034, 0.6351],\n",
       "        [0.3537, 0.2165, 0.5079, 0.7150, 0.5817, 0.7483],\n",
       "        [0.3913, 0.1731, 0.2804, 0.3505, 0.8164, 0.7109],\n",
       "        [0.4438, 0.4142, 0.3590, 0.5260, 0.7062, 0.6272],\n",
       "        [0.5103, 0.3951, 0.4917, 0.5593, 0.6724, 0.7507],\n",
       "        [0.5221, 0.3604, 0.5160, 0.4339, 0.7596, 0.4466],\n",
       "        [0.4552, 0.4090, 0.7000, 0.4017, 0.8085, 0.8059],\n",
       "        [0.1096, 0.2939, 0.5592, 0.5866, 0.9379, 0.5633],\n",
       "        [0.4901, 0.6935, 0.7278, 0.6066, 0.4272, 0.3104]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.6470, 0.5168, 0.3390, 0.3014, 0.4815, 0.4918],\n",
       "        [0.3026, 0.3779, 0.4865, 0.3166, 0.3673, 0.5889],\n",
       "        [0.7614, 0.5533, 0.5295, 0.4851, 0.6160, 0.5589],\n",
       "        [0.6167, 0.7367, 0.8437, 0.1671, 0.4407, 0.2642],\n",
       "        [0.3637, 0.5547, 0.4080, 0.4480, 0.4391, 0.5244],\n",
       "        [0.6227, 0.3718, 0.2961, 0.4693, 0.4497, 0.6710],\n",
       "        [0.5861, 0.7512, 0.7348, 0.6229, 0.6287, 0.7078],\n",
       "        [0.4605, 0.7509, 0.2556, 0.3470, 0.5317, 0.2713],\n",
       "        [0.3646, 0.2198, 0.3832, 0.2815, 0.3705, 0.7449],\n",
       "        [0.5830, 0.6648, 0.5950, 0.6888, 0.4776, 0.6397],\n",
       "        [0.6133, 0.2513, 0.6049, 0.5701, 0.3563, 0.5038],\n",
       "        [0.6948, 0.8650, 0.7019, 0.5122, 0.3703, 0.6916],\n",
       "        [0.5007, 0.5454, 0.8087, 0.2946, 0.2027, 0.4489],\n",
       "        [0.6751, 0.6701, 0.6732, 0.3273, 0.4379, 0.3451],\n",
       "        [0.7093, 0.6660, 0.3316, 0.2796, 0.7141, 0.5268],\n",
       "        [0.7085, 0.3503, 0.3258, 0.4145, 0.4935, 0.3648],\n",
       "        [0.6664, 0.3678, 0.4989, 0.3473, 0.3779, 0.3067],\n",
       "        [0.6308, 0.6053, 0.3809, 0.4462, 0.4216, 0.8177],\n",
       "        [0.7150, 0.6576, 0.5699, 0.2690, 0.3560, 0.6035],\n",
       "        [0.8495, 0.3697, 0.6713, 0.3535, 0.3786, 0.3168],\n",
       "        [0.8608, 0.5997, 0.5832, 0.3574, 0.5163, 0.6187],\n",
       "        [0.3326, 0.3451, 0.4291, 0.1461, 0.2283, 0.2932],\n",
       "        [0.6645, 0.2917, 0.4824, 0.6403, 0.2639, 0.3055],\n",
       "        [0.7016, 0.6845, 0.5350, 0.3912, 0.5740, 0.4413],\n",
       "        [0.6482, 0.5616, 0.5458, 0.3771, 0.4560, 0.6849],\n",
       "        [0.4302, 0.5706, 0.5989, 0.4670, 0.4589, 0.5862],\n",
       "        [0.7268, 0.4922, 0.5470, 0.4971, 0.4723, 0.4046],\n",
       "        [0.5814, 0.4069, 0.4333, 0.1233, 0.4045, 0.2842],\n",
       "        [0.5080, 0.8474, 0.6762, 0.4609, 0.4354, 0.6585],\n",
       "        [0.7949, 0.6600, 0.6214, 0.3286, 0.1725, 0.2610]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.3381, 0.5379, 0.4294, 0.4797, 0.7382, 0.4572],\n",
       "        [0.6434, 0.4161, 0.5574, 0.5305, 0.5274, 0.5128],\n",
       "        [0.3393, 0.4356, 0.4037, 0.4553, 0.8094, 0.3486],\n",
       "        [0.3438, 0.4965, 0.3642, 0.5033, 0.7463, 0.3673],\n",
       "        [0.5080, 0.6806, 0.6797, 0.4780, 0.4313, 0.4493],\n",
       "        [0.3858, 0.4247, 0.4941, 0.5864, 0.7083, 0.4715],\n",
       "        [0.4575, 0.6183, 0.3878, 0.6624, 0.6079, 0.3751],\n",
       "        [0.4013, 0.3804, 0.2980, 0.3324, 0.7223, 0.4663],\n",
       "        [0.2797, 0.3675, 0.5720, 0.6477, 0.7509, 0.5315],\n",
       "        [0.2987, 0.6013, 0.5001, 0.5349, 0.6695, 0.4210],\n",
       "        [0.2466, 0.5279, 0.6476, 0.6319, 0.7650, 0.6676],\n",
       "        [0.1445, 0.4268, 0.3971, 0.5947, 0.7525, 0.5639],\n",
       "        [0.6129, 0.3593, 0.4201, 0.3643, 0.6315, 0.5881],\n",
       "        [0.4619, 0.4032, 0.3446, 0.3743, 0.6970, 0.5146],\n",
       "        [0.5325, 0.5876, 0.5707, 0.5721, 0.6891, 0.4131],\n",
       "        [0.4593, 0.7141, 0.5203, 0.3748, 0.7177, 0.2936],\n",
       "        [0.4913, 0.7373, 0.7237, 0.5728, 0.5970, 0.5202],\n",
       "        [0.3487, 0.5777, 0.5310, 0.5239, 0.6722, 0.4252],\n",
       "        [0.4708, 0.7828, 0.7274, 0.5528, 0.6095, 0.2953],\n",
       "        [0.5366, 0.5009, 0.5506, 0.4947, 0.6565, 0.4639],\n",
       "        [0.4116, 0.7949, 0.6937, 0.5285, 0.7422, 0.5032],\n",
       "        [0.2672, 0.8082, 0.5352, 0.5229, 0.6471, 0.4616],\n",
       "        [0.4297, 0.6626, 0.6418, 0.5045, 0.6869, 0.6239],\n",
       "        [0.4111, 0.7196, 0.5814, 0.6826, 0.5981, 0.5825],\n",
       "        [0.6615, 0.7041, 0.8020, 0.6256, 0.4441, 0.4465],\n",
       "        [0.4024, 0.5459, 0.7439, 0.8010, 0.6947, 0.8040],\n",
       "        [0.4254, 0.3019, 0.5710, 0.6665, 0.6431, 0.5323],\n",
       "        [0.2415, 0.5663, 0.5775, 0.5276, 0.6745, 0.6255],\n",
       "        [0.3551, 0.5373, 0.4945, 0.6414, 0.7963, 0.5298],\n",
       "        [0.3112, 0.6336, 0.4544, 0.5076, 0.5853, 0.3052],\n",
       "        [0.1866, 0.7029, 0.3282, 0.4376, 0.6933, 0.6046]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.7379, 0.5171, 0.6294, 0.6642, 0.4386, 0.4304],\n",
       "        [0.4165, 0.3436, 0.3914, 0.2856, 0.4625, 0.6138],\n",
       "        [0.6258, 0.6369, 0.4579, 0.6749, 0.4728, 0.4062],\n",
       "        [0.2356, 0.4774, 0.3349, 0.5017, 0.5815, 0.3314],\n",
       "        [0.3181, 0.3712, 0.3468, 0.4435, 0.4458, 0.5466],\n",
       "        [0.5252, 0.2307, 0.4909, 0.5893, 0.2987, 0.3872],\n",
       "        [0.5627, 0.2634, 0.3777, 0.3672, 0.3971, 0.3292],\n",
       "        [0.6271, 0.3309, 0.3621, 0.5006, 0.6626, 0.5705],\n",
       "        [0.3550, 0.3390, 0.2975, 0.2538, 0.3837, 0.3785],\n",
       "        [0.5710, 0.4248, 0.3689, 0.4116, 0.6703, 0.4543],\n",
       "        [0.3055, 0.2499, 0.3284, 0.4563, 0.6031, 0.5176],\n",
       "        [0.2080, 0.2123, 0.4428, 0.4367, 0.7284, 0.5073],\n",
       "        [0.2824, 0.3581, 0.4052, 0.4281, 0.7382, 0.4744],\n",
       "        [0.4660, 0.4689, 0.3484, 0.5705, 0.7462, 0.3841],\n",
       "        [0.5620, 0.4956, 0.5182, 0.3369, 0.5814, 0.5408],\n",
       "        [0.4980, 0.3801, 0.5341, 0.4183, 0.4922, 0.3380],\n",
       "        [0.3158, 0.3077, 0.3038, 0.4241, 0.6504, 0.4095],\n",
       "        [0.3531, 0.5989, 0.5244, 0.4562, 0.4153, 0.4866],\n",
       "        [0.4467, 0.4093, 0.5524, 0.3896, 0.5594, 0.4509],\n",
       "        [0.5633, 0.4585, 0.3867, 0.5968, 0.6271, 0.5787],\n",
       "        [0.3494, 0.3031, 0.3880, 0.3737, 0.5411, 0.4796],\n",
       "        [0.4879, 0.3317, 0.3988, 0.4343, 0.5717, 0.2968],\n",
       "        [0.3776, 0.3016, 0.3936, 0.5540, 0.5528, 0.6491],\n",
       "        [0.2967, 0.3442, 0.4422, 0.4639, 0.5838, 0.3373],\n",
       "        [0.5247, 0.5581, 0.6064, 0.4651, 0.6887, 0.4458],\n",
       "        [0.3754, 0.5372, 0.5490, 0.5662, 0.7218, 0.3016],\n",
       "        [0.4837, 0.2939, 0.5719, 0.4772, 0.7302, 0.4277]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.7637, 0.3106, 0.6039, 0.7360, 0.4327, 0.6009],\n",
       "        [0.5155, 0.4603, 0.6005, 0.5692, 0.3277, 0.7338],\n",
       "        [0.3563, 0.6011, 0.5701, 0.6437, 0.5410, 0.3709],\n",
       "        [0.6670, 0.8622, 0.6860, 0.4103, 0.7431, 0.4527],\n",
       "        [0.3928, 0.5285, 0.5257, 0.6869, 0.4909, 0.5406],\n",
       "        [0.4217, 0.7232, 0.7046, 0.6092, 0.6799, 0.5399],\n",
       "        [0.3061, 0.6959, 0.4669, 0.5058, 0.6211, 0.7269],\n",
       "        [0.4274, 0.5007, 0.7129, 0.4317, 0.5839, 0.5695],\n",
       "        [0.4486, 0.4718, 0.5171, 0.6034, 0.5405, 0.5870],\n",
       "        [0.5076, 0.2800, 0.6225, 0.6362, 0.4733, 0.4211],\n",
       "        [0.2285, 0.5981, 0.5610, 0.5283, 0.5528, 0.4385],\n",
       "        [0.6406, 0.5764, 0.3729, 0.4944, 0.7225, 0.6203],\n",
       "        [0.4662, 0.5136, 0.5384, 0.5920, 0.5511, 0.2335],\n",
       "        [0.4435, 0.8298, 0.5634, 0.5945, 0.5354, 0.3423],\n",
       "        [0.4534, 0.6053, 0.5891, 0.3594, 0.6488, 0.3957],\n",
       "        [0.3866, 0.6178, 0.6303, 0.6612, 0.4127, 0.4790],\n",
       "        [0.2832, 0.7680, 0.6868, 0.6442, 0.4698, 0.6748],\n",
       "        [0.5490, 0.4318, 0.5348, 0.3850, 0.6235, 0.4984],\n",
       "        [0.4291, 0.7144, 0.5457, 0.4912, 0.4689, 0.5752],\n",
       "        [0.4373, 0.4213, 0.5340, 0.5658, 0.5315, 0.4927],\n",
       "        [0.3872, 0.7238, 0.5230, 0.7374, 0.3577, 0.5145],\n",
       "        [0.4002, 0.7583, 0.5926, 0.2351, 0.5862, 0.5631],\n",
       "        [0.2023, 0.7545, 0.6502, 0.5420, 0.7094, 0.4847],\n",
       "        [0.4507, 0.8852, 0.7591, 0.6196, 0.7178, 0.4019],\n",
       "        [0.6507, 0.6860, 0.6703, 0.7018, 0.6564, 0.4248],\n",
       "        [0.6565, 0.8333, 0.4058, 0.6399, 0.7133, 0.2604],\n",
       "        [0.5957, 0.5323, 0.5621, 0.2509, 0.8772, 0.5372],\n",
       "        [0.1637, 0.7662, 0.9238, 0.6977, 0.8048, 0.4711],\n",
       "        [0.2920, 0.7990, 0.7204, 0.7050, 0.4876, 0.7457],\n",
       "        [0.5126, 0.7742, 0.3539, 0.5992, 0.8528, 0.5322],\n",
       "        [0.2780, 0.7653, 0.4822, 0.2215, 0.6161, 0.5613],\n",
       "        [0.1814, 0.8044, 0.3356, 0.3226, 0.4837, 0.5980],\n",
       "        [0.1331, 0.4845, 0.3871, 0.3170, 0.4567, 0.3573],\n",
       "        [0.1391, 0.4496, 0.2890, 0.6864, 0.5355, 0.7304],\n",
       "        [0.1456, 0.7497, 0.3195, 0.2709, 0.6683, 0.4770],\n",
       "        [0.3272, 0.6291, 0.5732, 0.4728, 0.5075, 0.7349],\n",
       "        [0.3163, 0.7115, 0.6397, 0.6263, 0.4520, 0.4700],\n",
       "        [0.4118, 0.7973, 0.4370, 0.5817, 0.4972, 0.3439],\n",
       "        [0.3342, 0.5657, 0.6329, 0.5400, 0.4938, 0.4726],\n",
       "        [0.4946, 0.3446, 0.4726, 0.3347, 0.6637, 0.7351]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4100, 0.8524, 0.5785, 0.4421, 0.6381, 0.5789],\n",
       "        [0.3676, 0.4257, 0.5442, 0.5752, 0.7443, 0.2150],\n",
       "        [0.3705, 0.5605, 0.5287, 0.4853, 0.6256, 0.6039],\n",
       "        [0.6682, 0.7022, 0.3952, 0.7170, 0.3769, 0.7314],\n",
       "        [0.3114, 0.7599, 0.4682, 0.4692, 0.5122, 0.4783],\n",
       "        [0.3548, 0.8175, 0.4635, 0.3748, 0.5780, 0.8318],\n",
       "        [0.2585, 0.5630, 0.2751, 0.6148, 0.5540, 0.4359],\n",
       "        [0.1578, 0.4702, 0.2908, 0.6846, 0.7033, 0.6448],\n",
       "        [0.3118, 0.4748, 0.2851, 0.4909, 0.6159, 0.4652],\n",
       "        [0.3391, 0.3980, 0.4890, 0.5724, 0.6875, 0.4156],\n",
       "        [0.2958, 0.4078, 0.3678, 0.4604, 0.5095, 0.6557],\n",
       "        [0.3074, 0.5745, 0.4855, 0.4496, 0.4619, 0.4447],\n",
       "        [0.2706, 0.5684, 0.5780, 0.4750, 0.6110, 0.4620],\n",
       "        [0.2156, 0.5848, 0.3567, 0.3599, 0.7001, 0.5482],\n",
       "        [0.2685, 0.6787, 0.5165, 0.5010, 0.5306, 0.6136],\n",
       "        [0.3890, 0.6174, 0.4393, 0.3342, 0.6805, 0.6054],\n",
       "        [0.2239, 0.6991, 0.3894, 0.4538, 0.5676, 0.4067],\n",
       "        [0.3878, 0.4125, 0.4644, 0.2712, 0.5280, 0.4988],\n",
       "        [0.3883, 0.6399, 0.4468, 0.5280, 0.6234, 0.5876],\n",
       "        [0.2741, 0.6409, 0.6154, 0.4619, 0.6152, 0.5392],\n",
       "        [0.4567, 0.6180, 0.4269, 0.4648, 0.5228, 0.4615],\n",
       "        [0.4137, 0.5759, 0.4495, 0.3726, 0.5296, 0.4627],\n",
       "        [0.2513, 0.4046, 0.3606, 0.4324, 0.5681, 0.5163],\n",
       "        [0.3270, 0.6788, 0.5064, 0.4152, 0.6872, 0.3950],\n",
       "        [0.2901, 0.7751, 0.4961, 0.5467, 0.6582, 0.5673]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.8952, 0.5884, 0.4995, 0.6259, 0.6161, 0.7492],\n",
       "        [0.4918, 0.1162, 0.1747, 0.5083, 0.6343, 0.4377],\n",
       "        [0.6257, 0.3419, 0.5247, 0.4616, 0.3839, 0.6021],\n",
       "        [0.6750, 0.3975, 0.5153, 0.3185, 0.8448, 0.6388],\n",
       "        [0.5067, 0.2300, 0.3271, 0.5808, 0.5657, 0.6309],\n",
       "        [0.6646, 0.2740, 0.4474, 0.4992, 0.4469, 0.3190],\n",
       "        [0.3944, 0.4855, 0.7370, 0.4316, 0.1836, 0.7690],\n",
       "        [0.6145, 0.6219, 0.4470, 0.3743, 0.4192, 0.5896],\n",
       "        [0.6780, 0.5957, 0.4558, 0.3847, 0.4049, 0.8726],\n",
       "        [0.3751, 0.6781, 0.6187, 0.1628, 0.5775, 0.6727],\n",
       "        [0.3105, 0.5782, 0.5989, 0.2800, 0.2020, 0.6175],\n",
       "        [0.4685, 0.3101, 0.4049, 0.3312, 0.5632, 0.7146],\n",
       "        [0.2186, 0.2612, 0.6175, 0.3757, 0.4497, 0.5762],\n",
       "        [0.5488, 0.3625, 0.3316, 0.5707, 0.8581, 0.5722],\n",
       "        [0.5252, 0.0488, 0.1813, 0.6172, 0.9211, 0.5941],\n",
       "        [0.5348, 0.4405, 0.4777, 0.6367, 0.4389, 0.5871],\n",
       "        [0.4008, 0.2417, 0.5007, 0.6821, 0.7826, 0.5192],\n",
       "        [0.6801, 0.2341, 0.5158, 0.4028, 0.6089, 0.4093],\n",
       "        [0.4083, 0.1916, 0.5574, 0.4286, 0.4957, 0.2970],\n",
       "        [0.3772, 0.3757, 0.6304, 0.3540, 0.1529, 0.6347],\n",
       "        [0.6591, 0.3436, 0.5454, 0.5996, 0.4001, 0.3675],\n",
       "        [0.3200, 0.2143, 0.4472, 0.6064, 0.6331, 0.5967]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.6455, 0.6813, 0.6486, 0.3958, 0.5369, 0.3434],\n",
       "        [0.6754, 0.8107, 0.8391, 0.6816, 0.4623, 0.7474],\n",
       "        [0.7434, 0.4800, 0.5431, 0.4496, 0.6823, 0.6029],\n",
       "        [0.5295, 0.5241, 0.6957, 0.3875, 0.4577, 0.6167],\n",
       "        [0.6196, 0.6235, 0.5697, 0.4332, 0.4343, 0.3989],\n",
       "        [0.5642, 0.4616, 0.4658, 0.3663, 0.6337, 0.5233],\n",
       "        [0.5351, 0.6422, 0.7835, 0.2594, 0.5285, 0.8217],\n",
       "        [0.4930, 0.8063, 0.5951, 0.3561, 0.7660, 0.6303],\n",
       "        [0.7250, 0.7091, 0.7627, 0.5069, 0.4473, 0.3591],\n",
       "        [0.5076, 0.3465, 0.6348, 0.4169, 0.4617, 0.5645],\n",
       "        [0.6111, 0.6505, 0.7941, 0.4194, 0.5009, 0.6218],\n",
       "        [0.5008, 0.5955, 0.6343, 0.5233, 0.5510, 0.6491],\n",
       "        [0.6282, 0.6658, 0.5257, 0.4853, 0.5494, 0.5136],\n",
       "        [0.6384, 0.7625, 0.8356, 0.4520, 0.3525, 0.6238],\n",
       "        [0.7047, 0.6293, 0.7685, 0.4463, 0.5994, 0.3507],\n",
       "        [0.5380, 0.5672, 0.7489, 0.5222, 0.3506, 0.5481],\n",
       "        [0.6949, 0.5424, 0.6197, 0.4565, 0.5070, 0.5443],\n",
       "        [0.5051, 0.5758, 0.7895, 0.5118, 0.6871, 0.5682],\n",
       "        [0.5654, 0.4625, 0.5318, 0.3494, 0.4934, 0.4576],\n",
       "        [0.6234, 0.5736, 0.6184, 0.5274, 0.6528, 0.5364],\n",
       "        [0.6216, 0.3745, 0.6735, 0.1952, 0.7439, 0.5761],\n",
       "        [0.5820, 0.8017, 0.6982, 0.6365, 0.5812, 0.4383],\n",
       "        [0.6297, 0.6577, 0.7756, 0.5345, 0.5387, 0.6634],\n",
       "        [0.4385, 0.6194, 0.7509, 0.3316, 0.6184, 0.7758],\n",
       "        [0.2651, 0.3483, 0.6875, 0.4153, 0.7347, 0.4383],\n",
       "        [0.6937, 0.6087, 0.6834, 0.4526, 0.5897, 0.7277],\n",
       "        [0.5569, 0.3799, 0.6199, 0.4680, 0.6489, 0.6052],\n",
       "        [0.4739, 0.5062, 0.5414, 0.2899, 0.6351, 0.4303],\n",
       "        [0.4368, 0.5392, 0.7092, 0.3634, 0.6061, 0.4379],\n",
       "        [0.3447, 0.5773, 0.7103, 0.3225, 0.5192, 0.7572],\n",
       "        [0.7275, 0.6898, 0.5746, 0.3374, 0.5964, 0.4802],\n",
       "        [0.4988, 0.6402, 0.7647, 0.4280, 0.7101, 0.5177],\n",
       "        [0.5803, 0.5160, 0.7509, 0.4368, 0.5648, 0.5157]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.5129, 0.7279, 0.4625, 0.4621, 0.6829, 0.2517],\n",
       "        [0.2403, 0.3739, 0.1457, 0.6507, 0.8093, 0.1382],\n",
       "        [0.2102, 0.8319, 0.6384, 0.5414, 0.7816, 0.3392],\n",
       "        [0.5702, 0.6054, 0.5381, 0.5829, 0.2628, 0.7273],\n",
       "        [0.3691, 0.5544, 0.3024, 0.3677, 0.6567, 0.4021],\n",
       "        [0.1970, 0.2867, 0.4316, 0.6350, 0.6106, 0.4834],\n",
       "        [0.6956, 0.6225, 0.3575, 0.8107, 0.5667, 0.6455],\n",
       "        [0.3609, 0.8281, 0.1619, 0.7676, 0.7648, 0.3683],\n",
       "        [0.4419, 0.5868, 0.5309, 0.5175, 0.3910, 0.4703],\n",
       "        [0.4597, 0.6837, 0.3966, 0.5601, 0.6764, 0.6036],\n",
       "        [0.4511, 0.3065, 0.4023, 0.2478, 0.7303, 0.4561],\n",
       "        [0.3106, 0.5306, 0.5304, 0.4306, 0.6930, 0.5844],\n",
       "        [0.3553, 0.3629, 0.2307, 0.3749, 0.7583, 0.3676],\n",
       "        [0.3275, 0.6273, 0.2156, 0.6911, 0.6199, 0.4912],\n",
       "        [0.6482, 0.8097, 0.3403, 0.2813, 0.7663, 0.4150],\n",
       "        [0.3550, 0.6751, 0.2946, 0.5258, 0.5394, 0.2658],\n",
       "        [0.4088, 0.5127, 0.5448, 0.6851, 0.7809, 0.5340],\n",
       "        [0.5244, 0.7649, 0.5680, 0.2749, 0.7329, 0.4713],\n",
       "        [0.2940, 0.6178, 0.6354, 0.5773, 0.3383, 0.5371],\n",
       "        [0.4015, 0.5959, 0.5681, 0.6962, 0.7291, 0.7761],\n",
       "        [0.3831, 0.7382, 0.4803, 0.4737, 0.7852, 0.4484],\n",
       "        [0.4844, 0.7358, 0.4528, 0.4485, 0.8839, 0.4432],\n",
       "        [0.4202, 0.3491, 0.4425, 0.5824, 0.5917, 0.5380],\n",
       "        [0.5123, 0.5218, 0.3821, 0.5163, 0.6263, 0.5490],\n",
       "        [0.3317, 0.5953, 0.2498, 0.3972, 0.8640, 0.2836],\n",
       "        [0.3324, 0.5451, 0.2019, 0.4619, 0.7239, 0.4187],\n",
       "        [0.3746, 0.7665, 0.3002, 0.4571, 0.6841, 0.3804]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4419, 0.6658, 0.4549, 0.5813, 0.3978, 0.6625],\n",
       "        [0.5444, 0.7839, 0.3013, 0.1739, 0.4871, 0.4368],\n",
       "        [0.5505, 0.8073, 0.7634, 0.5216, 0.1374, 0.4870],\n",
       "        [0.7430, 0.5192, 0.3781, 0.5571, 0.7420, 0.4624],\n",
       "        [0.7271, 0.5928, 0.3331, 0.5182, 0.5627, 0.5153],\n",
       "        [0.3976, 0.5248, 0.6975, 0.1411, 0.7734, 0.5646],\n",
       "        [0.6990, 0.3929, 0.3339, 0.3772, 0.3957, 0.4660],\n",
       "        [0.4939, 0.7000, 0.3841, 0.2294, 0.5638, 0.3845],\n",
       "        [0.4677, 0.3675, 0.5752, 0.2700, 0.5999, 0.5814],\n",
       "        [0.5480, 0.6459, 0.4837, 0.4646, 0.4969, 0.4726],\n",
       "        [0.7243, 0.5510, 0.7970, 0.3866, 0.3264, 0.6564],\n",
       "        [0.8288, 0.4534, 0.6648, 0.3552, 0.5330, 0.4594],\n",
       "        [0.5267, 0.3787, 0.3828, 0.3561, 0.5348, 0.5705],\n",
       "        [0.7088, 0.4541, 0.4275, 0.1997, 0.4442, 0.3549],\n",
       "        [0.5391, 0.3152, 0.5180, 0.2049, 0.5736, 0.5533],\n",
       "        [0.6953, 0.6935, 0.6160, 0.3843, 0.2697, 0.5600],\n",
       "        [0.7113, 0.4031, 0.5568, 0.5330, 0.4058, 0.5327],\n",
       "        [0.5271, 0.3356, 0.5003, 0.3109, 0.5261, 0.3241],\n",
       "        [0.6150, 0.4102, 0.2722, 0.2232, 0.7074, 0.3593],\n",
       "        [0.6918, 0.6578, 0.4774, 0.4515, 0.5316, 0.6449],\n",
       "        [0.4470, 0.5596, 0.3460, 0.2782, 0.6961, 0.3807],\n",
       "        [0.7624, 0.6058, 0.5296, 0.3631, 0.6447, 0.4111],\n",
       "        [0.6547, 0.4544, 0.1377, 0.3718, 0.4688, 0.3704]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.6120, 0.6236, 0.5097, 0.4000, 0.4697, 0.2994],\n",
       "        [0.6925, 0.3928, 0.4523, 0.3064, 0.5986, 0.3778],\n",
       "        [0.3578, 0.6249, 0.6444, 0.5744, 0.4970, 0.7128],\n",
       "        [0.4763, 0.6311, 0.3724, 0.4814, 0.7857, 0.7014],\n",
       "        [0.4598, 0.5576, 0.5784, 0.4552, 0.6790, 0.3291],\n",
       "        [0.4255, 0.5707, 0.4816, 0.4432, 0.7232, 0.4672],\n",
       "        [0.6661, 0.6485, 0.5213, 0.5023, 0.4269, 0.5002],\n",
       "        [0.6535, 0.5669, 0.5325, 0.4054, 0.4740, 0.4038],\n",
       "        [0.6464, 0.7992, 0.6791, 0.6557, 0.6011, 0.4934],\n",
       "        [0.6983, 0.5244, 0.3636, 0.4176, 0.6433, 0.4347],\n",
       "        [0.4348, 0.7658, 0.3599, 0.3385, 0.6029, 0.5719],\n",
       "        [0.3673, 0.4274, 0.4295, 0.6503, 0.6106, 0.5414],\n",
       "        [0.7055, 0.4843, 0.4797, 0.5568, 0.6404, 0.5056],\n",
       "        [0.7579, 0.5111, 0.4706, 0.3917, 0.6484, 0.4511],\n",
       "        [0.5310, 0.5132, 0.4889, 0.4638, 0.5744, 0.4878],\n",
       "        [0.4241, 0.6499, 0.5252, 0.3631, 0.5614, 0.5893],\n",
       "        [0.4305, 0.5977, 0.3131, 0.5273, 0.8306, 0.4379],\n",
       "        [0.2330, 0.7097, 0.6031, 0.4891, 0.8841, 0.6136],\n",
       "        [0.3709, 0.6432, 0.4735, 0.4662, 0.6738, 0.4014],\n",
       "        [0.5827, 0.4796, 0.5017, 0.5828, 0.4146, 0.7184],\n",
       "        [0.4825, 0.5607, 0.4954, 0.4205, 0.5777, 0.6539]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4420, 0.7018, 0.1780, 0.2577, 0.4630, 0.4228],\n",
       "        [0.2194, 0.4465, 0.4312, 0.3890, 0.6887, 0.6923],\n",
       "        [0.5325, 0.9428, 0.1417, 0.2313, 0.6101, 0.8307],\n",
       "        [0.8235, 0.6889, 0.2725, 0.4069, 0.4814, 0.3161],\n",
       "        [0.3090, 0.4304, 0.1310, 0.3395, 0.3702, 0.6617],\n",
       "        [0.2495, 0.3545, 0.2427, 0.1815, 0.6922, 0.3787],\n",
       "        [0.4662, 0.6987, 0.7044, 0.5758, 0.5046, 0.6731],\n",
       "        [0.1651, 0.7679, 0.2659, 0.5137, 0.6419, 0.4976],\n",
       "        [0.6726, 0.4644, 0.1006, 0.2754, 0.7753, 0.3841],\n",
       "        [0.2136, 0.7223, 0.1598, 0.6045, 0.8620, 0.6453],\n",
       "        [0.7405, 0.3581, 0.3432, 0.2132, 0.5758, 0.7781],\n",
       "        [0.4725, 0.5582, 0.3606, 0.5482, 0.8213, 0.4480],\n",
       "        [0.4546, 0.7359, 0.6282, 0.3996, 0.6505, 0.7305],\n",
       "        [0.8264, 0.5104, 0.2988, 0.3126, 0.7199, 0.2773],\n",
       "        [0.4734, 0.2160, 0.1845, 0.2456, 0.7529, 0.6365],\n",
       "        [0.5416, 0.7998, 0.4963, 0.6069, 0.7679, 0.2255],\n",
       "        [0.2749, 0.4239, 0.3809, 0.3563, 0.5720, 0.2554],\n",
       "        [0.3393, 0.4962, 0.4173, 0.5963, 0.5859, 0.5455]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4360, 0.4475, 0.3105, 0.7823, 0.5520, 0.6171],\n",
       "        [0.1405, 0.2584, 0.3469, 0.8446, 0.3018, 0.4191],\n",
       "        [0.8360, 0.5503, 0.7757, 0.7503, 0.2712, 0.7029],\n",
       "        [0.1517, 0.2338, 0.2134, 0.5806, 0.3938, 0.1811],\n",
       "        [0.5795, 0.5643, 0.6987, 0.7275, 0.3369, 0.3660],\n",
       "        [0.5307, 0.2675, 0.7210, 0.5189, 0.5643, 0.3629],\n",
       "        [0.5520, 0.6391, 0.6753, 0.5190, 0.3350, 0.6384],\n",
       "        [0.6569, 0.3404, 0.6199, 0.4672, 0.4717, 0.7929],\n",
       "        [0.4478, 0.5315, 0.6193, 0.4066, 0.3390, 0.7042],\n",
       "        [0.6958, 0.4657, 0.6884, 0.7199, 0.3217, 0.8264],\n",
       "        [0.2290, 0.3993, 0.6603, 0.5158, 0.3743, 0.7238],\n",
       "        [0.1879, 0.3482, 0.2745, 0.6773, 0.7775, 0.7544],\n",
       "        [0.4714, 0.4469, 0.4637, 0.5401, 0.3953, 0.6127],\n",
       "        [0.5427, 0.5871, 0.3273, 0.7069, 0.6612, 0.5643],\n",
       "        [0.1521, 0.1643, 0.4555, 0.5506, 0.3228, 0.5098],\n",
       "        [0.2927, 0.1478, 0.5648, 0.5965, 0.4294, 0.3084],\n",
       "        [0.4726, 0.3543, 0.6953, 0.4899, 0.3443, 0.5112],\n",
       "        [0.3122, 0.2431, 0.4314, 0.4391, 0.4395, 0.2839],\n",
       "        [0.3007, 0.1489, 0.3254, 0.5126, 0.6062, 0.6139],\n",
       "        [0.4246, 0.4873, 0.5655, 0.5522, 0.3192, 0.5487],\n",
       "        [0.2538, 0.3605, 0.4692, 0.4691, 0.6479, 0.1962]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.7302, 0.6352, 0.6607, 0.7862, 0.2896, 0.3371],\n",
       "        [0.8704, 0.5999, 0.6387, 0.6853, 0.4905, 0.3392],\n",
       "        [0.8748, 0.7325, 0.6555, 0.7473, 0.3363, 0.3844],\n",
       "        [0.6096, 0.5331, 0.3747, 0.4364, 0.6808, 0.4331],\n",
       "        [0.7676, 0.7488, 0.4529, 0.5900, 0.3261, 0.7220],\n",
       "        [0.7239, 0.4871, 0.6700, 0.6453, 0.3782, 0.4689],\n",
       "        [0.7502, 0.6610, 0.5358, 0.6698, 0.3276, 0.2337],\n",
       "        [0.6003, 0.4059, 0.4731, 0.4140, 0.3648, 0.6494],\n",
       "        [0.4799, 0.3415, 0.3328, 0.3939, 0.5304, 0.3355],\n",
       "        [0.4678, 0.3825, 0.6408, 0.4215, 0.2062, 0.5142],\n",
       "        [0.5622, 0.4161, 0.7685, 0.3058, 0.3847, 0.6030],\n",
       "        [0.5985, 0.3414, 0.5737, 0.3615, 0.3879, 0.3638],\n",
       "        [0.6715, 0.2681, 0.6379, 0.3967, 0.4251, 0.5199],\n",
       "        [0.3146, 0.4255, 0.3618, 0.5230, 0.5834, 0.5511],\n",
       "        [0.5983, 0.6031, 0.5140, 0.5551, 0.2714, 0.5783],\n",
       "        [0.5641, 0.5427, 0.6678, 0.2520, 0.3636, 0.4702],\n",
       "        [0.7280, 0.5067, 0.5749, 0.3828, 0.2309, 0.4494],\n",
       "        [0.6552, 0.5432, 0.3322, 0.3013, 0.5033, 0.4325],\n",
       "        [0.5972, 0.3298, 0.5974, 0.4676, 0.3546, 0.6453],\n",
       "        [0.5703, 0.5687, 0.5706, 0.4587, 0.3948, 0.4669],\n",
       "        [0.5783, 0.5981, 0.4952, 0.1754, 0.3067, 0.6916],\n",
       "        [0.2624, 0.5417, 0.6877, 0.7612, 0.7220, 0.2573],\n",
       "        [0.5862, 0.6511, 0.5406, 0.3869, 0.4733, 0.5566],\n",
       "        [0.5379, 0.4381, 0.6827, 0.3549, 0.3691, 0.5294],\n",
       "        [0.3089, 0.4254, 0.4802, 0.2879, 0.4462, 0.4913],\n",
       "        [0.5669, 0.2115, 0.4838, 0.3805, 0.2856, 0.6109],\n",
       "        [0.4672, 0.2646, 0.4459, 0.2751, 0.3105, 0.2761],\n",
       "        [0.6765, 0.4057, 0.7094, 0.3873, 0.5560, 0.6587],\n",
       "        [0.5141, 0.1459, 0.3083, 0.2333, 0.7684, 0.6316],\n",
       "        [0.7534, 0.3223, 0.4940, 0.3905, 0.4746, 0.4147],\n",
       "        [0.7777, 0.4713, 0.6111, 0.3864, 0.4056, 0.4333],\n",
       "        [0.7146, 0.7710, 0.6689, 0.3649, 0.5567, 0.5854],\n",
       "        [0.8758, 0.6906, 0.3499, 0.5011, 0.5765, 0.7413],\n",
       "        [0.7155, 0.3734, 0.6358, 0.3866, 0.4227, 0.3613],\n",
       "        [0.5381, 0.6289, 0.6018, 0.4239, 0.3635, 0.5788],\n",
       "        [0.5225, 0.6928, 0.6287, 0.3158, 0.4597, 0.3759],\n",
       "        [0.7130, 0.6473, 0.6244, 0.6139, 0.3764, 0.3702],\n",
       "        [0.8232, 0.6611, 0.7242, 0.5726, 0.3983, 0.4307],\n",
       "        [0.6936, 0.5599, 0.6329, 0.4361, 0.4388, 0.2634]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.1528, 0.7793, 0.3807, 0.6988, 0.6149, 0.2533],\n",
       "        [0.6526, 0.5347, 0.6034, 0.4422, 0.4801, 0.3989],\n",
       "        [0.5057, 0.8158, 0.5274, 0.6580, 0.6972, 0.3523],\n",
       "        [0.4978, 0.4939, 0.4844, 0.3639, 0.4512, 0.3192],\n",
       "        [0.4780, 0.2283, 0.4218, 0.2487, 0.4377, 0.3113],\n",
       "        [0.4084, 0.3725, 0.5283, 0.4624, 0.6053, 0.4992],\n",
       "        [0.5689, 0.5373, 0.6369, 0.4979, 0.4428, 0.3975],\n",
       "        [0.4619, 0.3636, 0.5454, 0.4442, 0.5926, 0.5687],\n",
       "        [0.5432, 0.5630, 0.4376, 0.3254, 0.5143, 0.2367],\n",
       "        [0.1819, 0.2971, 0.4456, 0.3604, 0.5690, 0.2333],\n",
       "        [0.1951, 0.3483, 0.5971, 0.4097, 0.6323, 0.4121],\n",
       "        [0.2627, 0.4144, 0.5082, 0.6441, 0.5948, 0.4931],\n",
       "        [0.4154, 0.5311, 0.6124, 0.4300, 0.5860, 0.5686],\n",
       "        [0.4371, 0.6709, 0.6158, 0.5089, 0.4783, 0.5472],\n",
       "        [0.5423, 0.4977, 0.6388, 0.5501, 0.4685, 0.5497],\n",
       "        [0.7666, 0.3258, 0.5670, 0.4380, 0.6077, 0.2758],\n",
       "        [0.3949, 0.4701, 0.6607, 0.5980, 0.5545, 0.4808],\n",
       "        [0.3614, 0.3496, 0.5141, 0.5040, 0.6135, 0.4306],\n",
       "        [0.4915, 0.3498, 0.4236, 0.6264, 0.6655, 0.2818],\n",
       "        [0.3905, 0.2451, 0.6028, 0.5436, 0.4651, 0.3415],\n",
       "        [0.4974, 0.2243, 0.4927, 0.5827, 0.6595, 0.3296],\n",
       "        [0.4567, 0.4518, 0.4009, 0.6062, 0.6756, 0.4977]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.5370, 0.2980, 0.4416, 0.4858, 0.7241, 0.4460],\n",
       "        [0.4669, 0.4523, 0.4066, 0.5662, 0.5887, 0.4170],\n",
       "        [0.6444, 0.7369, 0.5782, 0.4881, 0.5497, 0.6970],\n",
       "        [0.6642, 0.4557, 0.6327, 0.3361, 0.3978, 0.4581],\n",
       "        [0.2521, 0.2151, 0.5463, 0.3781, 0.4229, 0.3878],\n",
       "        [0.4190, 0.2992, 0.5466, 0.4200, 0.5952, 0.4675],\n",
       "        [0.8140, 0.5402, 0.4740, 0.3843, 0.6506, 0.3839],\n",
       "        [0.4638, 0.6699, 0.7578, 0.2813, 0.4810, 0.2675],\n",
       "        [0.5514, 0.5813, 0.3890, 0.3479, 0.6380, 0.3843],\n",
       "        [0.3517, 0.5256, 0.4817, 0.3234, 0.4349, 0.8197],\n",
       "        [0.5401, 0.3237, 0.4136, 0.2871, 0.4834, 0.3850],\n",
       "        [0.6935, 0.5496, 0.6473, 0.3455, 0.5986, 0.4280],\n",
       "        [0.5484, 0.5635, 0.5602, 0.2281, 0.5795, 0.6056],\n",
       "        [0.5574, 0.5551, 0.3574, 0.3567, 0.6632, 0.6246],\n",
       "        [0.7390, 0.8637, 0.3849, 0.2856, 0.5857, 0.4163],\n",
       "        [0.5781, 0.6970, 0.6518, 0.2699, 0.5352, 0.2455],\n",
       "        [0.4771, 0.3905, 0.7064, 0.6096, 0.5378, 0.5405],\n",
       "        [0.4301, 0.3367, 0.3952, 0.3427, 0.5978, 0.6058],\n",
       "        [0.7326, 0.5410, 0.5081, 0.3700, 0.4373, 0.6939],\n",
       "        [0.7477, 0.5197, 0.7293, 0.5114, 0.4551, 0.5806],\n",
       "        [0.5166, 0.6459, 0.6461, 0.3660, 0.5795, 0.4758],\n",
       "        [0.6318, 0.4651, 0.5674, 0.3540, 0.3654, 0.4507],\n",
       "        [0.5339, 0.5752, 0.7342, 0.2311, 0.4536, 0.6754],\n",
       "        [0.7076, 0.6057, 0.5444, 0.5700, 0.6544, 0.4377],\n",
       "        [0.6350, 0.2940, 0.4091, 0.4338, 0.6076, 0.3599],\n",
       "        [0.4279, 0.2877, 0.6741, 0.4187, 0.7169, 0.4397],\n",
       "        [0.5508, 0.3417, 0.6599, 0.2907, 0.4274, 0.6227],\n",
       "        [0.4134, 0.2517, 0.5080, 0.2892, 0.5012, 0.5502],\n",
       "        [0.3514, 0.3613, 0.6372, 0.4573, 0.7298, 0.3764],\n",
       "        [0.5874, 0.5058, 0.4534, 0.4851, 0.5967, 0.6061],\n",
       "        [0.4121, 0.3257, 0.6588, 0.3001, 0.5424, 0.6526],\n",
       "        [0.5935, 0.3872, 0.3450, 0.4362, 0.6468, 0.4217],\n",
       "        [0.7533, 0.3476, 0.3336, 0.4398, 0.6728, 0.5636],\n",
       "        [0.6504, 0.3001, 0.3325, 0.3472, 0.6397, 0.5591],\n",
       "        [0.6698, 0.6049, 0.3520, 0.2894, 0.5151, 0.6198],\n",
       "        [0.4488, 0.3459, 0.6830, 0.3668, 0.5437, 0.3498]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.4937, 0.3421, 0.4051, 0.6007, 0.6384, 0.8595],\n",
       "        [0.2403, 0.5227, 0.5415, 0.5342, 0.6001, 0.4404],\n",
       "        [0.8206, 0.6202, 0.7025, 0.4014, 0.4017, 0.2760],\n",
       "        [0.5755, 0.6405, 0.6574, 0.3892, 0.3603, 0.2312],\n",
       "        [0.5315, 0.5318, 0.7386, 0.3319, 0.5236, 0.6607],\n",
       "        [0.6970, 0.6733, 0.6071, 0.5150, 0.6562, 0.6601],\n",
       "        [0.4545, 0.8437, 0.6119, 0.5252, 0.5715, 0.2447],\n",
       "        [0.2813, 0.4553, 0.4319, 0.3987, 0.4745, 0.4542],\n",
       "        [0.3568, 0.7281, 0.6934, 0.4450, 0.5313, 0.5539],\n",
       "        [0.6460, 0.8842, 0.7573, 0.1699, 0.5522, 0.4023],\n",
       "        [0.3818, 0.3889, 0.5510, 0.3264, 0.5161, 0.7023],\n",
       "        [0.4536, 0.6064, 0.6010, 0.4498, 0.5365, 0.5782],\n",
       "        [0.2735, 0.4364, 0.5745, 0.4136, 0.7041, 0.4669],\n",
       "        [0.3757, 0.6844, 0.7270, 0.5107, 0.2994, 0.7266],\n",
       "        [0.4881, 0.6791, 0.6609, 0.2831, 0.5646, 0.5181],\n",
       "        [0.5816, 0.6966, 0.6138, 0.5065, 0.4848, 0.7124],\n",
       "        [0.5223, 0.6316, 0.7280, 0.2806, 0.5336, 0.3741],\n",
       "        [0.5238, 0.7314, 0.6120, 0.3190, 0.6399, 0.4608],\n",
       "        [0.3986, 0.6188, 0.6801, 0.3560, 0.3783, 0.5110],\n",
       "        [0.4918, 0.6327, 0.5370, 0.3265, 0.6184, 0.5011],\n",
       "        [0.3503, 0.4379, 0.7033, 0.2691, 0.7353, 0.6961],\n",
       "        [0.7281, 0.6881, 0.5075, 0.5537, 0.6598, 0.5823],\n",
       "        [0.5447, 0.5351, 0.5941, 0.5423, 0.6854, 0.3536],\n",
       "        [0.5225, 0.6482, 0.4391, 0.4004, 0.6468, 0.6145],\n",
       "        [0.4794, 0.5179, 0.7339, 0.4256, 0.5518, 0.4756],\n",
       "        [0.3582, 0.5568, 0.5981, 0.4149, 0.6048, 0.6072],\n",
       "        [0.6879, 0.3526, 0.6268, 0.4758, 0.5653, 0.5570],\n",
       "        [0.5604, 0.5302, 0.5436, 0.3388, 0.6043, 0.5448]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.3898, 0.4988, 0.3190, 0.3526, 0.6651, 0.2140],\n",
       "        [0.1426, 0.2239, 0.0967, 0.0583, 0.7080, 0.2616],\n",
       "        [0.4261, 0.5680, 0.6726, 0.6948, 0.6259, 0.5571],\n",
       "        [0.6203, 0.5217, 0.5786, 0.3136, 0.5754, 0.5627],\n",
       "        [0.5354, 0.7854, 0.6607, 0.5848, 0.6299, 0.6707],\n",
       "        [0.4529, 0.4822, 0.5043, 0.6193, 0.4867, 0.5016],\n",
       "        [0.5079, 0.5662, 0.5482, 0.6249, 0.4895, 0.6034],\n",
       "        [0.5052, 0.6778, 0.6043, 0.7824, 0.5540, 0.4154],\n",
       "        [0.4860, 0.7106, 0.6171, 0.4936, 0.5815, 0.4713],\n",
       "        [0.4013, 0.5662, 0.6919, 0.4101, 0.4006, 0.3881],\n",
       "        [0.6382, 0.6143, 0.6726, 0.5003, 0.6226, 0.5120],\n",
       "        [0.0958, 0.3544, 0.4785, 0.5592, 0.7286, 0.4759],\n",
       "        [0.4401, 0.4919, 0.5843, 0.3924, 0.5247, 0.5355],\n",
       "        [0.4891, 0.2803, 0.4839, 0.4529, 0.5289, 0.3833],\n",
       "        [0.5809, 0.3899, 0.5167, 0.5302, 0.4649, 0.5143],\n",
       "        [0.4186, 0.6802, 0.7098, 0.6472, 0.5983, 0.5954],\n",
       "        [0.3883, 0.5406, 0.5478, 0.5092, 0.5006, 0.3987],\n",
       "        [0.4243, 0.5260, 0.6621, 0.6059, 0.6204, 0.6611]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>), tensor([[0.2823, 0.6182, 0.3101, 0.4142, 0.3269, 0.8876],\n",
       "        [0.3909, 0.9092, 0.6152, 0.3255, 0.1365, 0.8874],\n",
       "        [0.6888, 0.7720, 0.6480, 0.3972, 0.1884, 0.8909],\n",
       "        [0.2948, 0.5184, 0.4206, 0.2730, 0.5424, 0.7824],\n",
       "        [0.6715, 0.7624, 0.6511, 0.3347, 0.3234, 0.8625],\n",
       "        [0.4725, 0.6990, 0.3582, 0.4635, 0.6678, 0.5287],\n",
       "        [0.3319, 0.4662, 0.5010, 0.2467, 0.5418, 0.6819],\n",
       "        [0.8303, 0.6487, 0.7132, 0.3675, 0.4913, 0.9438],\n",
       "        [0.7919, 0.6161, 0.6952, 0.1253, 0.4435, 0.5854],\n",
       "        [0.6983, 0.7348, 0.7194, 0.4012, 0.3175, 0.6865],\n",
       "        [0.6070, 0.8601, 0.4735, 0.1806, 0.1766, 0.6492],\n",
       "        [0.3904, 0.4709, 0.7395, 0.2373, 0.6177, 0.4743],\n",
       "        [0.2243, 0.4486, 0.4868, 0.2047, 0.2147, 0.7882],\n",
       "        [0.3250, 0.8260, 0.4936, 0.1775, 0.2926, 0.4458],\n",
       "        [0.5223, 0.2856, 0.4103, 0.3600, 0.3642, 0.4935],\n",
       "        [0.7066, 0.7437, 0.6466, 0.2360, 0.4717, 0.7003],\n",
       "        [0.6334, 0.6443, 0.5921, 0.3359, 0.5178, 0.9026],\n",
       "        [0.5311, 0.6213, 0.7190, 0.0971, 0.1475, 0.9183],\n",
       "        [0.5463, 0.5461, 0.5657, 0.2811, 0.4125, 0.6312],\n",
       "        [0.5532, 0.4518, 0.6488, 0.2721, 0.6725, 0.6667],\n",
       "        [0.4639, 0.6612, 0.5216, 0.2046, 0.6180, 0.5865]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_learn.model(x.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAS 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>clas_acc_0</th>\n",
       "      <th>clas_acc_1</th>\n",
       "      <th>clas_acc_2</th>\n",
       "      <th>clas_acc_3</th>\n",
       "      <th>clas_acc_4</th>\n",
       "      <th>clas_mse_0</th>\n",
       "      <th>clas_mse_1</th>\n",
       "      <th>clas_mse_2</th>\n",
       "      <th>clas_mse_3</th>\n",
       "      <th>clas_mse_4</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.140300</td>\n",
       "      <td>4.588657</td>\n",
       "      <td>0.579490</td>\n",
       "      <td>0.569886</td>\n",
       "      <td>0.563459</td>\n",
       "      <td>0.601749</td>\n",
       "      <td>0.581221</td>\n",
       "      <td>0.581132</td>\n",
       "      <td>0.574616</td>\n",
       "      <td>0.508211</td>\n",
       "      <td>0.498929</td>\n",
       "      <td>0.482596</td>\n",
       "      <td>0.541949</td>\n",
       "      <td>02:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.072678</td>\n",
       "      <td>4.502055</td>\n",
       "      <td>0.583488</td>\n",
       "      <td>0.583274</td>\n",
       "      <td>0.552213</td>\n",
       "      <td>0.631739</td>\n",
       "      <td>0.576580</td>\n",
       "      <td>0.573634</td>\n",
       "      <td>0.560068</td>\n",
       "      <td>0.527401</td>\n",
       "      <td>0.457694</td>\n",
       "      <td>0.488843</td>\n",
       "      <td>0.501339</td>\n",
       "      <td>02:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.992404</td>\n",
       "      <td>4.474336</td>\n",
       "      <td>0.593717</td>\n",
       "      <td>0.595769</td>\n",
       "      <td>0.567565</td>\n",
       "      <td>0.631828</td>\n",
       "      <td>0.593092</td>\n",
       "      <td>0.580328</td>\n",
       "      <td>0.511692</td>\n",
       "      <td>0.479204</td>\n",
       "      <td>0.466887</td>\n",
       "      <td>0.481435</td>\n",
       "      <td>0.511335</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.969266</td>\n",
       "      <td>4.455817</td>\n",
       "      <td>0.596680</td>\n",
       "      <td>0.588629</td>\n",
       "      <td>0.574259</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.592378</td>\n",
       "      <td>0.591485</td>\n",
       "      <td>0.506605</td>\n",
       "      <td>0.511692</td>\n",
       "      <td>0.448679</td>\n",
       "      <td>0.477687</td>\n",
       "      <td>0.479472</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.976071</td>\n",
       "      <td>4.398461</td>\n",
       "      <td>0.602588</td>\n",
       "      <td>0.598893</td>\n",
       "      <td>0.580150</td>\n",
       "      <td>0.637897</td>\n",
       "      <td>0.601928</td>\n",
       "      <td>0.594074</td>\n",
       "      <td>0.512139</td>\n",
       "      <td>0.478044</td>\n",
       "      <td>0.456623</td>\n",
       "      <td>0.476883</td>\n",
       "      <td>0.499911</td>\n",
       "      <td>02:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.925586</td>\n",
       "      <td>4.411169</td>\n",
       "      <td>0.599625</td>\n",
       "      <td>0.594788</td>\n",
       "      <td>0.577919</td>\n",
       "      <td>0.636558</td>\n",
       "      <td>0.597733</td>\n",
       "      <td>0.591128</td>\n",
       "      <td>0.515352</td>\n",
       "      <td>0.472153</td>\n",
       "      <td>0.454570</td>\n",
       "      <td>0.466173</td>\n",
       "      <td>0.490539</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle( 6, max_lr=slice(1e-3,2e-2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>clas_acc_0</th>\n",
       "      <th>clas_acc_1</th>\n",
       "      <th>clas_acc_2</th>\n",
       "      <th>clas_acc_3</th>\n",
       "      <th>clas_acc_4</th>\n",
       "      <th>clas_mse_0</th>\n",
       "      <th>clas_mse_1</th>\n",
       "      <th>clas_mse_2</th>\n",
       "      <th>clas_mse_3</th>\n",
       "      <th>clas_mse_4</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.052145</td>\n",
       "      <td>4.481627</td>\n",
       "      <td>0.590414</td>\n",
       "      <td>0.576669</td>\n",
       "      <td>0.583899</td>\n",
       "      <td>0.625491</td>\n",
       "      <td>0.588718</td>\n",
       "      <td>0.577294</td>\n",
       "      <td>0.565334</td>\n",
       "      <td>0.477151</td>\n",
       "      <td>0.479829</td>\n",
       "      <td>0.494823</td>\n",
       "      <td>0.507944</td>\n",
       "      <td>02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.010029</td>\n",
       "      <td>4.533336</td>\n",
       "      <td>0.578383</td>\n",
       "      <td>0.558640</td>\n",
       "      <td>0.562656</td>\n",
       "      <td>0.591842</td>\n",
       "      <td>0.592378</td>\n",
       "      <td>0.586398</td>\n",
       "      <td>0.603088</td>\n",
       "      <td>0.522849</td>\n",
       "      <td>0.545162</td>\n",
       "      <td>0.484916</td>\n",
       "      <td>0.511157</td>\n",
       "      <td>02:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.934620</td>\n",
       "      <td>4.464402</td>\n",
       "      <td>0.590860</td>\n",
       "      <td>0.576758</td>\n",
       "      <td>0.570154</td>\n",
       "      <td>0.634416</td>\n",
       "      <td>0.594341</td>\n",
       "      <td>0.578633</td>\n",
       "      <td>0.594163</td>\n",
       "      <td>0.544181</td>\n",
       "      <td>0.470635</td>\n",
       "      <td>0.502588</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.957168</td>\n",
       "      <td>4.357931</td>\n",
       "      <td>0.599268</td>\n",
       "      <td>0.588451</td>\n",
       "      <td>0.580150</td>\n",
       "      <td>0.638522</td>\n",
       "      <td>0.596662</td>\n",
       "      <td>0.592556</td>\n",
       "      <td>0.515530</td>\n",
       "      <td>0.471707</td>\n",
       "      <td>0.441806</td>\n",
       "      <td>0.484202</td>\n",
       "      <td>0.490271</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.835490</td>\n",
       "      <td>4.345675</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>0.592735</td>\n",
       "      <td>0.584702</td>\n",
       "      <td>0.641021</td>\n",
       "      <td>0.598983</td>\n",
       "      <td>0.595769</td>\n",
       "      <td>0.522938</td>\n",
       "      <td>0.469743</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.476080</td>\n",
       "      <td>0.469297</td>\n",
       "      <td>02:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.816246</td>\n",
       "      <td>4.307751</td>\n",
       "      <td>0.603410</td>\n",
       "      <td>0.585237</td>\n",
       "      <td>0.591931</td>\n",
       "      <td>0.637986</td>\n",
       "      <td>0.605230</td>\n",
       "      <td>0.596662</td>\n",
       "      <td>0.569439</td>\n",
       "      <td>0.480810</td>\n",
       "      <td>0.468226</td>\n",
       "      <td>0.486612</td>\n",
       "      <td>0.491789</td>\n",
       "      <td>02:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.798492</td>\n",
       "      <td>4.310158</td>\n",
       "      <td>0.603088</td>\n",
       "      <td>0.586041</td>\n",
       "      <td>0.592913</td>\n",
       "      <td>0.637183</td>\n",
       "      <td>0.604159</td>\n",
       "      <td>0.595145</td>\n",
       "      <td>0.570689</td>\n",
       "      <td>0.472599</td>\n",
       "      <td>0.460818</td>\n",
       "      <td>0.484291</td>\n",
       "      <td>0.477954</td>\n",
       "      <td>02:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  With redist first\n",
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle( 7, max_lr=slice(1e-3,2e-2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = cls_learn.recorder.plot_losses()\n",
    "experiment.log_figure(figure_name=\"train loss 01\", figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.save('beer.clas.LIATLI.1.learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (63909 items)\n",
       "x: TextList\n",
       "xxbos a lot of foam xxperiod xxmaj but a lot xxperiod xxmaj in the smell some banana , and then lactic and tart xxperiod xxmaj not a good start xxperiod xxmaj quite dark orange in color , with a lively carbonation ( now visible , under the foam ) xxperiod xxmaj again tending to lactic sourness xxperiod xxmaj same for the taste xxperiod xxmaj with some yeast and banana xxperiod,xxbos xxmaj almost totally black xxperiod xxmaj beige foam , quite compact , not bad xxperiod xxmaj light smell , just a bit of roast , and some hop xxperiod a bit too light xxperiod xxmaj the taste is light oo , and drinkable , with some malt , roast , hints of coffee xxperiod xxmaj nothing exceptional , but after all drinkable and pleasant xxperiod xxmaj light to average body xxperiod xxmaj in the aftertaste some dust , xxunk roast , hint of caramel , and a bit of bitterness xxperiod xxmaj no defect , drinkable , not bad xxperiod,xxbos xxmaj golden yellow color xxperiod xxmaj white , compact foam , quite creamy xxperiod xxmaj good appearance xxperiod xxmaj fresh smell , with good hop xxperiod xxmaj quite dry , with a good grassy note xxperiod xxmaj hay xxperiod xxmaj fresh and pleasant xxperiod xxmaj more sweet in the mouth , with honey xxperiod xxmaj the hop comes back in the end , and in the aftertaste xxperiod xxmaj not bad , but a bit too sweet for a pils xxperiod xxmaj in the end some vanilla and camomile note xxperiod xxmaj in the aftertaste , too xxperiod xxmaj though the hop , a bit too sweet xxperiod xxmaj honest xxperiod,xxbos 22 oz bottle from \" xxmaj lifesource \" xxmaj salem xxperiod $ 3 xxperiod 95 xxmaj nice golden clear beer body with a nice sized frothy / creamy white head xxperiod xxmaj ok aromas xxperiod mainlly a bit of ginger xxunk and some bready malt xxperiod simple nice xxmaj taste very nice indeed xxperiod nice spicy ginger backed with slightly caramel maltiness xxperiod simple again but i like xxperiod xxmaj liked the mouthfeel of this one xxperiod very forward carbonation which helps the ginger effect and a lingering ginger in the after taste xxperiod xxmaj overall a simple ginger brew xxperiod i liked it xxperiod,xxbos xxmaj bottle says \" xxmaj malt beverage brewed with xxmaj ginger and ginger added \" xxmaj sounds redundant to me , but lets move on xxperiod xxmaj pours a bud light yellow with a tiny white head of small bubbles xxperiod xxmaj the beer is almost as clear as a glass of water with some food coloring in it xxperiod xxmaj aroma of light ginger , a very light malt aroma but primarily odorless on the malt side xxperiod i would n't be completely surprised if there were some adjuncts in here because of the lack of underlying malt flavors xxperiod xxmaj taste is of a light adjunct lager with a dosing of ginger xxperiod xxmaj not surprising there xxperiod xxmaj this is a light session beer , good for the warmer days of spring / summer xxperiod xxmaj mouthfeel is extremely light , high carbonation xxperiod xxmaj overall decent xxperiod xxmaj this would be great if you were drinking beers on draft at the bar with some friends just hanging out xxperiod i would n't necessarily seek it out though to drink out of a bottle xxperiod\n",
       "y: MultiCategoryList\n",
       "4,,,,3\n",
       "Path: data;\n",
       "\n",
       "Valid: LabelList (11204 items)\n",
       "x: TextList\n",
       "xxbos xxmaj dark red color , light beige foam , average xxperiod xxmaj in the smell malt and caramel , not really light xxperiod xxmaj again malt and caramel in the taste , not bad in the end xxperiod xxmaj maybe a note of honey in teh back , and a light fruitiness xxperiod xxmaj average body xxperiod xxmaj in the aftertaste a light bitterness , with the malt and red fruit xxperiod xxmaj nothing exceptional , but not bad , drinkable beer xxperiod,xxbos xxmaj poured from a 22 oz bomber into my xxmaj drie xxmaj fonteinen tumbler xxperiod xxmaj hazy xxunk yellow body ( which catches the shadows forming a beautiful mysterious gradient ) with an incredibly dense pillow of magnolia cream xxperiod xxmaj heavy persistent head and rich creamy lacing xxperiod xxmaj pale malt , asian pear , and a hint of citrus in the nose xxperiod a vaguely tropical lager xxperiod xxmaj tastes very much like a well done xxup apa , with a nice balance of pale malt and low hop bitterness xxperiod xxmaj the ginger adds to the refreshing character , but is n't readily detectable at first ( lacks any \" bite \" ) xxperiod xxmaj medium - dry finish - very clean and extremely quaffable xxperiod i can imagine hibiscus and beets working in small quantities , though i think they omitted those for this version xxperiod xxmaj light bodied , pillowy , smooth and moderately carbonated xxperiod xxmaj do n't go into this expecting a ginger beer ( despite its name ) as it has little in common with that spicy soft drink xxperiod xxmaj this is a wonderful session ale though , and worth seeking out if you are a fan of light yet flavorful lagers xxperiod xxmaj would obviously go perfectly with sushi xxperiod,xxbos xxmaj more of a ' dry ' than a lager , tasted at the 2002 xxmaj oregon xxmaj brewers xxmaj festival xxperiod xxmaj orange color , orange flavor in nose xxperiod xxmaj light malts and fairly aggressively hopped xxperiod yet it is not very bitter xxperiod xxmaj interesting taste , complex and subtle xxperiod xxmaj light yet flavorful xxperiod xxmaj mouthfeel is full and round xxperiod xxmaj finish is clean and smooth xxperiod xxmaj aftertaste is slightly bitter xxperiod xxmaj nice beer xxperiod xxmaj would be a great beer to sip during a hot summer day xxperiod,xxbos xxmaj pours a rich burnt caramel hue with some deep amber hues xxperiod dark for a rauchbier xxperiod a surprisingly dark tan head slowly fades and leaves ample lacing xxperiod aroma is pungently smokey xxperiod sweet malt , sweet smoke , and some salted caramel in the nose xxperiod the taste , as expected , is smokey xxperiod xxmaj not the smokiest or most pungent smoke profile i 've experienced , but damn up there xxperiod and i think it 's the intense rich malt and sweetness from the malt that keeps the smoke slightly tamed xxperiod a touch of lemon and citrus fruitiness in the finish which blends perfectly with the cherry smoke xxperiod xxmaj it 's not as crisp or dry as other xxmaj rachbiers , and it seems a bit more full - bodied , with almost a touch of syrupiness to it xxperiod which is kind of an interesting take on it xxperiod i 'm digging it xxperiod i 'm not usually one to say this , but it 's almost a touch malt - forward xxperiod xxmaj not that it 's a bad thing xxperiod i think this is probably my favorite domestically produced xxmaj rauchbier i 've had the pleasure of tasting xxperiod xxmaj it really hits on all cylinders xxperiod a touch sweet and malt forward , but all in all , it does what it 's going for and works beautifully xxperiod xxmaj definitely worth seeking out xxperiod,xxbos xxmaj got this in a 22 ounce bomber xxperiod xxmaj it poured a deep bright brown with a pretty small head xxperiod xxmaj aromas of smoked xxunk , and dark roasted malts xxperiod xxmaj this beer was xxup ok but for a style that i like so much it did'nt really please as much as i thought it would xxperiod xxmaj it had a nice amount of smokiness to it xxperiod xxmaj really obvious and pretty strong but not charred xxperiod xxmaj but the oily consistency of the beer along with the huge saltiness made the beer less than i was hoping for xxperiod xxmaj it was still a beer that i 'm glad i grabbed xxperiod xxmaj they do n't make many in the style anyways xxperiod\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: data;\n",
       "\n",
       "Test: LabelList (24884 items)\n",
       "x: TextList\n",
       "xxbos xxmaj according to the website , the style for the xxmaj caldera xxmaj cauldron changes every year xxunk xxperiod xxmaj the current release is a xxup dipa , which frankly is the only cauldron i 'm familiar with ( it was an xxup ipa / xxup dipa the last time i ordered a cauldron at the xxunk several years back ) xxunk xxperiod xxmaj in any event xxunk xxperiod at the xxmaj horse xxmaj brass yesterday xxunk xxperiod xxmaj the beer pours an orange copper color with good head retention and lacing xxunk xxperiod xxmaj the nose is all hoppy xxup ipa goodness , showcasing a huge aroma of dry citrus , pine and xxunk xxunk xxperiod xxmaj the flavor profile replicates the nose pretty closely in this xxmaj west xxmaj coast all the way xxup dipa xxunk xxperiod xxmaj this xxup dipa is not for the faint of heart and is a bit much even for a hophead like xxunk xxunk xxperiod xxmaj the finish is quite dry and hoppy , and there 's barely enough sweet malt to balance and hold up the avalanche of hoppy bitterness in this beer xxunk xxperiod xxmaj mouthfeel is actually fairly light , with a long , xxunk bitter finish xxunk xxperiod xxmaj drinkability is good , with the alcohol barely noticeable in this well crafted beer xxunk xxperiod xxmaj still , this beer is so hugely hoppy / bitter , it 's really hard for me to imagine ordering more than a single glass xxunk xxperiod xxmaj regardless , this is a very impressive beer from the folks at xxmaj caldera xxunk xxperiod,xxbos xxmaj poured from the bottle into a xxmaj chimay goblet xxunk xxperiod xxmaj appearance : xxmaj pours a slightly cloudy yellow / orange color with a half finger of fluffy white head xxunk xxperiod xxmaj the head fades to a small layer on top of the pour xxunk xxperiod xxmaj smell : xxmaj very light and crisp xxunk xxperiod i 'm definitely picking up the ginger , but it 's not overly powerful xxunk xxperiod xxmaj there is a slight sweetness from the malt as well xxunk xxperiod xxmaj taste : xxmaj very light and refreshing xxunk xxperiod xxmaj the ginger shows up right away and then fades towards the finish of the sip xxunk xxperiod xxmaj the finish is malty and bread like xxunk xxperiod xxmaj mouthfeel : xxmaj the body is on the thin side with smooth carbonation and a very dry finish xxunk xxperiod xxmaj overall : xxmaj this is a light and refreshing beer , but nothing spectacular xxunk xxperiod xxmaj the amount of ginger is nice , but i would have liked to have more going on xxunk xxperiod,xxbos xxmaj notes from 6 / 24 xxunk xxperiod a : xxmaj bright golden glowing beer in a moment of clarity with a lively white head of feathery fluff xxunk xxperiod s : xxmaj the ginger is definitely there , or am i smelling my xxmaj indian dinner ? xxmaj almost cake - like in its malt aroma , sharply bready with a slight edge of sweetness xxunk xxperiod t : xxmaj nice clear malty throat taste , reminds me of strands of complex sugars and grains , ginger is more subtle than i expected , more of an undertone than a backbone xxunk xxperiod m : a refreshing light beer feel , like a pilsner or summer ale xxunk xxperiod d : xxmaj if this was a sixer instead of a double - deuce , i could see this being a fine picnic pounder xxunk xxperiod xxmaj overall , pretty impressed for the particular style xxunk xxperiod,xxbos 22 oz xxunk xxperiod bomber , xxunk xxperiod a : xxmaj pours a clear yellow with a mild white head , good retention xxunk xxperiod s : xxmaj great nose of ginger , honey , perfume xxunk xxperiod t : xxmaj rather light upfront , it reminds me of xxmaj lawnmower , with a hint of ale fruitiness / xxmaj kolsch - like almost xxunk xxperiod xxmaj good ginger honey notes on the back end , not taking over the beer in any way xxunk xxperiod xxmaj the ginger flavour is clear , but i wanted it to come out a little more in the end xxunk xxperiod m : xxmaj very light - bodied , watery , light base beer for sure xxunk xxperiod d : xxmaj an easy drinking spiced beer , this will offend no one , but there 's not a complexity to this brew at all xxunk xxperiod,xxbos xxmaj brown in color , somewhere between a porter and a brown ale xxunk xxperiod xxmaj lacking in aroma , but no off stuff xxunk xxperiod xxmaj same with the taste , lacking flavor , complexity , just went with smoothness xxunk xxperiod xxmaj no off flavors though , so i ca n't say this is bad , just xxunk , especially for xxmaj caldera , whom i think is generally underrated xxunk xxperiod xxmaj you really have to search to pull anything out of this in terms of the usual chocolate / coffee flavors , really , the only thing i can tell is that the oats did their job , because this is smooth and unoffensive xxunk xxperiod xxmaj other than that , extremely pedestrian xxunk xxperiod\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: data, model=SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(31600, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(31600, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Cls02ATT400(\n",
       "    (aspect): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "      (7): Softmax(dim=1)\n",
       "    )\n",
       "    (s0): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s1): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s2): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s3): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s4): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s5): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=MultiLabelCEL(), metrics=[<function multi_acc at 0x7effd28298c0>, <function get_clas_acc.<locals>.asp_acc at 0x7effd2829950>, <function get_clas_acc.<locals>.asp_acc at 0x7effd2829b90>, <function get_clas_acc.<locals>.asp_acc at 0x7effd2829c20>, <function get_clas_acc.<locals>.asp_acc at 0x7effd2829cb0>, <function get_clas_acc.<locals>.asp_acc at 0x7effd2829d40>, <function get_clas_mse.<locals>.asp_mse at 0x7effd2829dd0>, <function get_clas_mse.<locals>.asp_mse at 0x7effd2829e60>, <function get_clas_mse.<locals>.asp_mse at 0x7effd2829ef0>, <function get_clas_mse.<locals>.asp_mse at 0x7effd2829f80>, <function get_clas_mse.<locals>.asp_mse at 0x7effac62d050>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(31600, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(31600, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Cls02ATT400(\n",
       "    (aspect): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "      (7): Softmax(dim=1)\n",
       "    )\n",
       "    (s0): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s1): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s2): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s3): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s4): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s5): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_learn.load('beer.clas.attfullind400.1.learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Embedding(31600, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(31600, 400, padding_idx=1)\n",
      "  )\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Cls02_LIATLI(\n",
      "    (aspect): Sequential(\n",
      "      (0): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=800, out_features=40, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (4): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.15, inplace=False)\n",
      "      (6): Linear(in_features=40, out_features=6, bias=True)\n",
      "      (7): Sigmoid()\n",
      "    )\n",
      "    (first_fn): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=1800, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "    (second_fn): Sequential(\n",
      "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.4, inplace=False)\n",
      "      (2): Linear(in_features=300, out_features=80, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (4): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.2, inplace=False)\n",
      "      (6): Linear(in_features=80, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cls_learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>clas_acc_0</th>\n",
       "      <th>clas_acc_1</th>\n",
       "      <th>clas_acc_2</th>\n",
       "      <th>clas_acc_3</th>\n",
       "      <th>clas_acc_4</th>\n",
       "      <th>clas_mse_0</th>\n",
       "      <th>clas_mse_1</th>\n",
       "      <th>clas_mse_2</th>\n",
       "      <th>clas_mse_3</th>\n",
       "      <th>clas_mse_4</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.788064</td>\n",
       "      <td>4.298103</td>\n",
       "      <td>0.611442</td>\n",
       "      <td>0.606748</td>\n",
       "      <td>0.593717</td>\n",
       "      <td>0.646912</td>\n",
       "      <td>0.607462</td>\n",
       "      <td>0.602374</td>\n",
       "      <td>0.496073</td>\n",
       "      <td>0.464655</td>\n",
       "      <td>0.434488</td>\n",
       "      <td>0.462335</td>\n",
       "      <td>0.479025</td>\n",
       "      <td>05:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.600244</td>\n",
       "      <td>4.137919</td>\n",
       "      <td>0.621992</td>\n",
       "      <td>0.615494</td>\n",
       "      <td>0.604963</td>\n",
       "      <td>0.654588</td>\n",
       "      <td>0.617637</td>\n",
       "      <td>0.617280</td>\n",
       "      <td>0.482953</td>\n",
       "      <td>0.447965</td>\n",
       "      <td>0.422974</td>\n",
       "      <td>0.451089</td>\n",
       "      <td>0.456176</td>\n",
       "      <td>05:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.412610</td>\n",
       "      <td>4.076941</td>\n",
       "      <td>0.629579</td>\n",
       "      <td>0.622813</td>\n",
       "      <td>0.608890</td>\n",
       "      <td>0.664227</td>\n",
       "      <td>0.624420</td>\n",
       "      <td>0.627544</td>\n",
       "      <td>0.467333</td>\n",
       "      <td>0.443770</td>\n",
       "      <td>0.404141</td>\n",
       "      <td>0.446091</td>\n",
       "      <td>0.449036</td>\n",
       "      <td>05:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.388107</td>\n",
       "      <td>4.029789</td>\n",
       "      <td>0.634505</td>\n",
       "      <td>0.631560</td>\n",
       "      <td>0.611567</td>\n",
       "      <td>0.672617</td>\n",
       "      <td>0.628526</td>\n",
       "      <td>0.628258</td>\n",
       "      <td>0.459122</td>\n",
       "      <td>0.443502</td>\n",
       "      <td>0.391289</td>\n",
       "      <td>0.435916</td>\n",
       "      <td>0.436987</td>\n",
       "      <td>05:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.260339</td>\n",
       "      <td>3.979668</td>\n",
       "      <td>0.639164</td>\n",
       "      <td>0.635577</td>\n",
       "      <td>0.615584</td>\n",
       "      <td>0.673956</td>\n",
       "      <td>0.634863</td>\n",
       "      <td>0.635844</td>\n",
       "      <td>0.450107</td>\n",
       "      <td>0.447251</td>\n",
       "      <td>0.391289</td>\n",
       "      <td>0.428418</td>\n",
       "      <td>0.432613</td>\n",
       "      <td>05:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.214543</td>\n",
       "      <td>3.988944</td>\n",
       "      <td>0.636612</td>\n",
       "      <td>0.631471</td>\n",
       "      <td>0.611478</td>\n",
       "      <td>0.670475</td>\n",
       "      <td>0.637362</td>\n",
       "      <td>0.632274</td>\n",
       "      <td>0.449661</td>\n",
       "      <td>0.452160</td>\n",
       "      <td>0.399589</td>\n",
       "      <td>0.423956</td>\n",
       "      <td>0.439932</td>\n",
       "      <td>04:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.218116</td>\n",
       "      <td>3.977113</td>\n",
       "      <td>0.638718</td>\n",
       "      <td>0.636112</td>\n",
       "      <td>0.611924</td>\n",
       "      <td>0.675830</td>\n",
       "      <td>0.637005</td>\n",
       "      <td>0.632720</td>\n",
       "      <td>0.448590</td>\n",
       "      <td>0.446358</td>\n",
       "      <td>0.388076</td>\n",
       "      <td>0.423956</td>\n",
       "      <td>0.437165</td>\n",
       "      <td>04:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.203924</td>\n",
       "      <td>3.941751</td>\n",
       "      <td>0.643127</td>\n",
       "      <td>0.639057</td>\n",
       "      <td>0.618261</td>\n",
       "      <td>0.679668</td>\n",
       "      <td>0.638433</td>\n",
       "      <td>0.640218</td>\n",
       "      <td>0.438683</td>\n",
       "      <td>0.434398</td>\n",
       "      <td>0.383702</td>\n",
       "      <td>0.422081</td>\n",
       "      <td>0.424581</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.125604</td>\n",
       "      <td>3.950826</td>\n",
       "      <td>0.640236</td>\n",
       "      <td>0.637362</td>\n",
       "      <td>0.616030</td>\n",
       "      <td>0.674402</td>\n",
       "      <td>0.638343</td>\n",
       "      <td>0.635041</td>\n",
       "      <td>0.444484</td>\n",
       "      <td>0.440111</td>\n",
       "      <td>0.390218</td>\n",
       "      <td>0.419225</td>\n",
       "      <td>0.432167</td>\n",
       "      <td>04:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.087418</td>\n",
       "      <td>3.939092</td>\n",
       "      <td>0.643163</td>\n",
       "      <td>0.638700</td>\n",
       "      <td>0.619065</td>\n",
       "      <td>0.677437</td>\n",
       "      <td>0.639950</td>\n",
       "      <td>0.640664</td>\n",
       "      <td>0.442610</td>\n",
       "      <td>0.433595</td>\n",
       "      <td>0.384595</td>\n",
       "      <td>0.416012</td>\n",
       "      <td>0.419582</td>\n",
       "      <td>05:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FULL INDIPENDENT\n",
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>clas_acc_0</th>\n",
       "      <th>clas_acc_1</th>\n",
       "      <th>clas_acc_2</th>\n",
       "      <th>clas_acc_3</th>\n",
       "      <th>clas_acc_4</th>\n",
       "      <th>clas_mse_0</th>\n",
       "      <th>clas_mse_1</th>\n",
       "      <th>clas_mse_2</th>\n",
       "      <th>clas_mse_3</th>\n",
       "      <th>clas_mse_4</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.764177</td>\n",
       "      <td>4.253979</td>\n",
       "      <td>0.609264</td>\n",
       "      <td>0.599875</td>\n",
       "      <td>0.597554</td>\n",
       "      <td>0.645573</td>\n",
       "      <td>0.607551</td>\n",
       "      <td>0.595769</td>\n",
       "      <td>0.537665</td>\n",
       "      <td>0.464477</td>\n",
       "      <td>0.443056</td>\n",
       "      <td>0.479829</td>\n",
       "      <td>0.479204</td>\n",
       "      <td>04:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.710799</td>\n",
       "      <td>4.201262</td>\n",
       "      <td>0.614245</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>0.598983</td>\n",
       "      <td>0.647983</td>\n",
       "      <td>0.612995</td>\n",
       "      <td>0.610764</td>\n",
       "      <td>0.524456</td>\n",
       "      <td>0.467601</td>\n",
       "      <td>0.441092</td>\n",
       "      <td>0.467869</td>\n",
       "      <td>0.456176</td>\n",
       "      <td>05:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.511265</td>\n",
       "      <td>4.220484</td>\n",
       "      <td>0.613692</td>\n",
       "      <td>0.597554</td>\n",
       "      <td>0.597911</td>\n",
       "      <td>0.647537</td>\n",
       "      <td>0.608533</td>\n",
       "      <td>0.616923</td>\n",
       "      <td>0.520350</td>\n",
       "      <td>0.481703</td>\n",
       "      <td>0.429936</td>\n",
       "      <td>0.469654</td>\n",
       "      <td>0.447429</td>\n",
       "      <td>04:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.468018</td>\n",
       "      <td>4.122464</td>\n",
       "      <td>0.624474</td>\n",
       "      <td>0.614245</td>\n",
       "      <td>0.606391</td>\n",
       "      <td>0.661371</td>\n",
       "      <td>0.623081</td>\n",
       "      <td>0.617280</td>\n",
       "      <td>0.483399</td>\n",
       "      <td>0.448233</td>\n",
       "      <td>0.405837</td>\n",
       "      <td>0.439218</td>\n",
       "      <td>0.440914</td>\n",
       "      <td>05:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.483001</td>\n",
       "      <td>4.084507</td>\n",
       "      <td>0.627365</td>\n",
       "      <td>0.614066</td>\n",
       "      <td>0.612103</td>\n",
       "      <td>0.662442</td>\n",
       "      <td>0.626651</td>\n",
       "      <td>0.621564</td>\n",
       "      <td>0.483131</td>\n",
       "      <td>0.449482</td>\n",
       "      <td>0.404588</td>\n",
       "      <td>0.433952</td>\n",
       "      <td>0.437612</td>\n",
       "      <td>05:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.336730</td>\n",
       "      <td>4.047126</td>\n",
       "      <td>0.633345</td>\n",
       "      <td>0.621921</td>\n",
       "      <td>0.612995</td>\n",
       "      <td>0.668422</td>\n",
       "      <td>0.632185</td>\n",
       "      <td>0.631203</td>\n",
       "      <td>0.475723</td>\n",
       "      <td>0.436719</td>\n",
       "      <td>0.389325</td>\n",
       "      <td>0.424313</td>\n",
       "      <td>0.425652</td>\n",
       "      <td>05:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.249311</td>\n",
       "      <td>4.057905</td>\n",
       "      <td>0.636487</td>\n",
       "      <td>0.629329</td>\n",
       "      <td>0.609336</td>\n",
       "      <td>0.671724</td>\n",
       "      <td>0.636380</td>\n",
       "      <td>0.635666</td>\n",
       "      <td>0.453767</td>\n",
       "      <td>0.447340</td>\n",
       "      <td>0.382006</td>\n",
       "      <td>0.420921</td>\n",
       "      <td>0.417708</td>\n",
       "      <td>05:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.244049</td>\n",
       "      <td>3.995905</td>\n",
       "      <td>0.642288</td>\n",
       "      <td>0.637362</td>\n",
       "      <td>0.617547</td>\n",
       "      <td>0.678954</td>\n",
       "      <td>0.638433</td>\n",
       "      <td>0.639147</td>\n",
       "      <td>0.440200</td>\n",
       "      <td>0.433060</td>\n",
       "      <td>0.375848</td>\n",
       "      <td>0.417886</td>\n",
       "      <td>0.418779</td>\n",
       "      <td>04:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.161983</td>\n",
       "      <td>3.981884</td>\n",
       "      <td>0.643163</td>\n",
       "      <td>0.638343</td>\n",
       "      <td>0.616208</td>\n",
       "      <td>0.681899</td>\n",
       "      <td>0.640396</td>\n",
       "      <td>0.638968</td>\n",
       "      <td>0.439486</td>\n",
       "      <td>0.435469</td>\n",
       "      <td>0.368618</td>\n",
       "      <td>0.415030</td>\n",
       "      <td>0.420029</td>\n",
       "      <td>05:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.127716</td>\n",
       "      <td>3.992565</td>\n",
       "      <td>0.643841</td>\n",
       "      <td>0.639950</td>\n",
       "      <td>0.618083</td>\n",
       "      <td>0.681007</td>\n",
       "      <td>0.639772</td>\n",
       "      <td>0.640396</td>\n",
       "      <td>0.438593</td>\n",
       "      <td>0.446448</td>\n",
       "      <td>0.372456</td>\n",
       "      <td>0.419672</td>\n",
       "      <td>0.415655</td>\n",
       "      <td>04:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.104802</td>\n",
       "      <td>3.999054</td>\n",
       "      <td>0.641557</td>\n",
       "      <td>0.633434</td>\n",
       "      <td>0.617101</td>\n",
       "      <td>0.679222</td>\n",
       "      <td>0.637897</td>\n",
       "      <td>0.640129</td>\n",
       "      <td>0.454481</td>\n",
       "      <td>0.446805</td>\n",
       "      <td>0.379061</td>\n",
       "      <td>0.424759</td>\n",
       "      <td>0.420207</td>\n",
       "      <td>05:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.091766</td>\n",
       "      <td>3.997255</td>\n",
       "      <td>0.644431</td>\n",
       "      <td>0.639236</td>\n",
       "      <td>0.618618</td>\n",
       "      <td>0.681810</td>\n",
       "      <td>0.640843</td>\n",
       "      <td>0.641646</td>\n",
       "      <td>0.437701</td>\n",
       "      <td>0.437522</td>\n",
       "      <td>0.368975</td>\n",
       "      <td>0.417172</td>\n",
       "      <td>0.415744</td>\n",
       "      <td>05:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LIATLI\n",
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web': 'https://www.comet.ml/api/image/download?imageId=0b3961087b6d4026a5778bac67999051&experimentKey=2b4c175e75bb4f35bcb54b814195ab54',\n",
       " 'api': 'https://www.comet.ml/api/rest/v1/image/get-image?imageId=0b3961087b6d4026a5778bac67999051&experimentKey=2b4c175e75bb4f35bcb54b814195ab54',\n",
       " 'imageId': '0b3961087b6d4026a5778bac67999051'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = cls_learn.recorder.plot_losses()\n",
    "experiment.log_figure(figure_name=\"train loss 02\", figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.save('beer.clas.LIATLI.2.learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (63909 items)\n",
       "x: TextList\n",
       "xxbos a lot of foam xxperiod xxmaj but a lot xxperiod xxmaj in the smell some banana , and then lactic and tart xxperiod xxmaj not a good start xxperiod xxmaj quite dark orange in color , with a lively carbonation ( now visible , under the foam ) xxperiod xxmaj again tending to lactic sourness xxperiod xxmaj same for the taste xxperiod xxmaj with some yeast and banana xxperiod,xxbos xxmaj almost totally black xxperiod xxmaj beige foam , quite compact , not bad xxperiod xxmaj light smell , just a bit of roast , and some hop xxperiod a bit too light xxperiod xxmaj the taste is light oo , and drinkable , with some malt , roast , hints of coffee xxperiod xxmaj nothing exceptional , but after all drinkable and pleasant xxperiod xxmaj light to average body xxperiod xxmaj in the aftertaste some dust , xxunk roast , hint of caramel , and a bit of bitterness xxperiod xxmaj no defect , drinkable , not bad xxperiod,xxbos xxmaj golden yellow color xxperiod xxmaj white , compact foam , quite creamy xxperiod xxmaj good appearance xxperiod xxmaj fresh smell , with good hop xxperiod xxmaj quite dry , with a good grassy note xxperiod xxmaj hay xxperiod xxmaj fresh and pleasant xxperiod xxmaj more sweet in the mouth , with honey xxperiod xxmaj the hop comes back in the end , and in the aftertaste xxperiod xxmaj not bad , but a bit too sweet for a pils xxperiod xxmaj in the end some vanilla and camomile note xxperiod xxmaj in the aftertaste , too xxperiod xxmaj though the hop , a bit too sweet xxperiod xxmaj honest xxperiod,xxbos 22 oz bottle from \" xxmaj lifesource \" xxmaj salem xxperiod $ 3 xxperiod 95 xxmaj nice golden clear beer body with a nice sized frothy / creamy white head xxperiod xxmaj ok aromas xxperiod mainlly a bit of ginger xxunk and some bready malt xxperiod simple nice xxmaj taste very nice indeed xxperiod nice spicy ginger backed with slightly caramel maltiness xxperiod simple again but i like xxperiod xxmaj liked the mouthfeel of this one xxperiod very forward carbonation which helps the ginger effect and a lingering ginger in the after taste xxperiod xxmaj overall a simple ginger brew xxperiod i liked it xxperiod,xxbos xxmaj bottle says \" xxmaj malt beverage brewed with xxmaj ginger and ginger added \" xxmaj sounds redundant to me , but lets move on xxperiod xxmaj pours a bud light yellow with a tiny white head of small bubbles xxperiod xxmaj the beer is almost as clear as a glass of water with some food coloring in it xxperiod xxmaj aroma of light ginger , a very light malt aroma but primarily odorless on the malt side xxperiod i would n't be completely surprised if there were some adjuncts in here because of the lack of underlying malt flavors xxperiod xxmaj taste is of a light adjunct lager with a dosing of ginger xxperiod xxmaj not surprising there xxperiod xxmaj this is a light session beer , good for the warmer days of spring / summer xxperiod xxmaj mouthfeel is extremely light , high carbonation xxperiod xxmaj overall decent xxperiod xxmaj this would be great if you were drinking beers on draft at the bar with some friends just hanging out xxperiod i would n't necessarily seek it out though to drink out of a bottle xxperiod\n",
       "y: MultiCategoryList\n",
       "4,,,,3\n",
       "Path: data;\n",
       "\n",
       "Valid: LabelList (11204 items)\n",
       "x: TextList\n",
       "xxbos xxmaj dark red color , light beige foam , average xxperiod xxmaj in the smell malt and caramel , not really light xxperiod xxmaj again malt and caramel in the taste , not bad in the end xxperiod xxmaj maybe a note of honey in teh back , and a light fruitiness xxperiod xxmaj average body xxperiod xxmaj in the aftertaste a light bitterness , with the malt and red fruit xxperiod xxmaj nothing exceptional , but not bad , drinkable beer xxperiod,xxbos xxmaj poured from a 22 oz bomber into my xxmaj drie xxmaj fonteinen tumbler xxperiod xxmaj hazy xxunk yellow body ( which catches the shadows forming a beautiful mysterious gradient ) with an incredibly dense pillow of magnolia cream xxperiod xxmaj heavy persistent head and rich creamy lacing xxperiod xxmaj pale malt , asian pear , and a hint of citrus in the nose xxperiod a vaguely tropical lager xxperiod xxmaj tastes very much like a well done xxup apa , with a nice balance of pale malt and low hop bitterness xxperiod xxmaj the ginger adds to the refreshing character , but is n't readily detectable at first ( lacks any \" bite \" ) xxperiod xxmaj medium - dry finish - very clean and extremely quaffable xxperiod i can imagine hibiscus and beets working in small quantities , though i think they omitted those for this version xxperiod xxmaj light bodied , pillowy , smooth and moderately carbonated xxperiod xxmaj do n't go into this expecting a ginger beer ( despite its name ) as it has little in common with that spicy soft drink xxperiod xxmaj this is a wonderful session ale though , and worth seeking out if you are a fan of light yet flavorful lagers xxperiod xxmaj would obviously go perfectly with sushi xxperiod,xxbos xxmaj more of a ' dry ' than a lager , tasted at the 2002 xxmaj oregon xxmaj brewers xxmaj festival xxperiod xxmaj orange color , orange flavor in nose xxperiod xxmaj light malts and fairly aggressively hopped xxperiod yet it is not very bitter xxperiod xxmaj interesting taste , complex and subtle xxperiod xxmaj light yet flavorful xxperiod xxmaj mouthfeel is full and round xxperiod xxmaj finish is clean and smooth xxperiod xxmaj aftertaste is slightly bitter xxperiod xxmaj nice beer xxperiod xxmaj would be a great beer to sip during a hot summer day xxperiod,xxbos xxmaj pours a rich burnt caramel hue with some deep amber hues xxperiod dark for a rauchbier xxperiod a surprisingly dark tan head slowly fades and leaves ample lacing xxperiod aroma is pungently smokey xxperiod sweet malt , sweet smoke , and some salted caramel in the nose xxperiod the taste , as expected , is smokey xxperiod xxmaj not the smokiest or most pungent smoke profile i 've experienced , but damn up there xxperiod and i think it 's the intense rich malt and sweetness from the malt that keeps the smoke slightly tamed xxperiod a touch of lemon and citrus fruitiness in the finish which blends perfectly with the cherry smoke xxperiod xxmaj it 's not as crisp or dry as other xxmaj rachbiers , and it seems a bit more full - bodied , with almost a touch of syrupiness to it xxperiod which is kind of an interesting take on it xxperiod i 'm digging it xxperiod i 'm not usually one to say this , but it 's almost a touch malt - forward xxperiod xxmaj not that it 's a bad thing xxperiod i think this is probably my favorite domestically produced xxmaj rauchbier i 've had the pleasure of tasting xxperiod xxmaj it really hits on all cylinders xxperiod a touch sweet and malt forward , but all in all , it does what it 's going for and works beautifully xxperiod xxmaj definitely worth seeking out xxperiod,xxbos xxmaj got this in a 22 ounce bomber xxperiod xxmaj it poured a deep bright brown with a pretty small head xxperiod xxmaj aromas of smoked xxunk , and dark roasted malts xxperiod xxmaj this beer was xxup ok but for a style that i like so much it did'nt really please as much as i thought it would xxperiod xxmaj it had a nice amount of smokiness to it xxperiod xxmaj really obvious and pretty strong but not charred xxperiod xxmaj but the oily consistency of the beer along with the huge saltiness made the beer less than i was hoping for xxperiod xxmaj it was still a beer that i 'm glad i grabbed xxperiod xxmaj they do n't make many in the style anyways xxperiod\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: data;\n",
       "\n",
       "Test: LabelList (24884 items)\n",
       "x: TextList\n",
       "xxbos xxmaj according to the website , the style for the xxmaj caldera xxmaj cauldron changes every year xxunk xxperiod xxmaj the current release is a xxup dipa , which frankly is the only cauldron i 'm familiar with ( it was an xxup ipa / xxup dipa the last time i ordered a cauldron at the xxunk several years back ) xxunk xxperiod xxmaj in any event xxunk xxperiod at the xxmaj horse xxmaj brass yesterday xxunk xxperiod xxmaj the beer pours an orange copper color with good head retention and lacing xxunk xxperiod xxmaj the nose is all hoppy xxup ipa goodness , showcasing a huge aroma of dry citrus , pine and xxunk xxunk xxperiod xxmaj the flavor profile replicates the nose pretty closely in this xxmaj west xxmaj coast all the way xxup dipa xxunk xxperiod xxmaj this xxup dipa is not for the faint of heart and is a bit much even for a hophead like xxunk xxunk xxperiod xxmaj the finish is quite dry and hoppy , and there 's barely enough sweet malt to balance and hold up the avalanche of hoppy bitterness in this beer xxunk xxperiod xxmaj mouthfeel is actually fairly light , with a long , xxunk bitter finish xxunk xxperiod xxmaj drinkability is good , with the alcohol barely noticeable in this well crafted beer xxunk xxperiod xxmaj still , this beer is so hugely hoppy / bitter , it 's really hard for me to imagine ordering more than a single glass xxunk xxperiod xxmaj regardless , this is a very impressive beer from the folks at xxmaj caldera xxunk xxperiod,xxbos xxmaj poured from the bottle into a xxmaj chimay goblet xxunk xxperiod xxmaj appearance : xxmaj pours a slightly cloudy yellow / orange color with a half finger of fluffy white head xxunk xxperiod xxmaj the head fades to a small layer on top of the pour xxunk xxperiod xxmaj smell : xxmaj very light and crisp xxunk xxperiod i 'm definitely picking up the ginger , but it 's not overly powerful xxunk xxperiod xxmaj there is a slight sweetness from the malt as well xxunk xxperiod xxmaj taste : xxmaj very light and refreshing xxunk xxperiod xxmaj the ginger shows up right away and then fades towards the finish of the sip xxunk xxperiod xxmaj the finish is malty and bread like xxunk xxperiod xxmaj mouthfeel : xxmaj the body is on the thin side with smooth carbonation and a very dry finish xxunk xxperiod xxmaj overall : xxmaj this is a light and refreshing beer , but nothing spectacular xxunk xxperiod xxmaj the amount of ginger is nice , but i would have liked to have more going on xxunk xxperiod,xxbos xxmaj notes from 6 / 24 xxunk xxperiod a : xxmaj bright golden glowing beer in a moment of clarity with a lively white head of feathery fluff xxunk xxperiod s : xxmaj the ginger is definitely there , or am i smelling my xxmaj indian dinner ? xxmaj almost cake - like in its malt aroma , sharply bready with a slight edge of sweetness xxunk xxperiod t : xxmaj nice clear malty throat taste , reminds me of strands of complex sugars and grains , ginger is more subtle than i expected , more of an undertone than a backbone xxunk xxperiod m : a refreshing light beer feel , like a pilsner or summer ale xxunk xxperiod d : xxmaj if this was a sixer instead of a double - deuce , i could see this being a fine picnic pounder xxunk xxperiod xxmaj overall , pretty impressed for the particular style xxunk xxperiod,xxbos 22 oz xxunk xxperiod bomber , xxunk xxperiod a : xxmaj pours a clear yellow with a mild white head , good retention xxunk xxperiod s : xxmaj great nose of ginger , honey , perfume xxunk xxperiod t : xxmaj rather light upfront , it reminds me of xxmaj lawnmower , with a hint of ale fruitiness / xxmaj kolsch - like almost xxunk xxperiod xxmaj good ginger honey notes on the back end , not taking over the beer in any way xxunk xxperiod xxmaj the ginger flavour is clear , but i wanted it to come out a little more in the end xxunk xxperiod m : xxmaj very light - bodied , watery , light base beer for sure xxunk xxperiod d : xxmaj an easy drinking spiced beer , this will offend no one , but there 's not a complexity to this brew at all xxunk xxperiod,xxbos xxmaj brown in color , somewhere between a porter and a brown ale xxunk xxperiod xxmaj lacking in aroma , but no off stuff xxunk xxperiod xxmaj same with the taste , lacking flavor , complexity , just went with smoothness xxunk xxperiod xxmaj no off flavors though , so i ca n't say this is bad , just xxunk , especially for xxmaj caldera , whom i think is generally underrated xxunk xxperiod xxmaj you really have to search to pull anything out of this in terms of the usual chocolate / coffee flavors , really , the only thing i can tell is that the oats did their job , because this is smooth and unoffensive xxunk xxperiod xxmaj other than that , extremely pedestrian xxunk xxperiod\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: data, model=SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(31600, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(31600, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Cls02ATT400(\n",
       "    (aspect): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "      (7): Softmax(dim=1)\n",
       "    )\n",
       "    (s0): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s1): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s2): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s3): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s4): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s5): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s6): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=MultiLabelCEL(), metrics=[<function multi_acc at 0x7f3e1e874d40>, <function get_clas_acc.<locals>.asp_acc at 0x7f3e1e8880e0>, <function get_clas_acc.<locals>.asp_acc at 0x7f3e1e888170>, <function get_clas_acc.<locals>.asp_acc at 0x7f3e1e888200>, <function get_clas_acc.<locals>.asp_acc at 0x7f3e1e888290>, <function get_clas_acc.<locals>.asp_acc at 0x7f3e1e888320>, <function get_clas_mse.<locals>.asp_mse at 0x7f3e1e8883b0>, <function get_clas_mse.<locals>.asp_mse at 0x7f3e1e888440>, <function get_clas_mse.<locals>.asp_mse at 0x7f3e1e8884d0>, <function get_clas_mse.<locals>.asp_mse at 0x7f3e1e888560>, <function get_clas_mse.<locals>.asp_mse at 0x7f3e1e8885f0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(31600, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(31600, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Cls02ATT400(\n",
       "    (aspect): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "      (7): Softmax(dim=1)\n",
       "    )\n",
       "    (s0): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s1): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s2): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s3): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s4): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s5): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s6): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_learn.load('beer.clas.attfullind400.2.learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINISH EXPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-86c369aed6fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_num_file = [\"train.count\", \"test.count\"]\n",
    "rating_file = [\"train.rating\", \"test.rating\"]\n",
    "content_file = [\"train.txt\", \"test.txt\"]\n",
    "\n",
    "dataset_dir = \"./data/beer_100k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_to_doc(sent_list, sent_count):\n",
    "    start_index = 0\n",
    "    docs = []\n",
    "    for s in sent_count:\n",
    "#         doc = \" xxPERIOD \".join(sent_list[start_index:start_index + s])\n",
    "#         doc = doc + \" xxPERIOD \"\n",
    "        docs.append(sent_list[start_index:start_index + s])\n",
    "        start_index = start_index + s\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATA = 0\n",
    "TEST_DATA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 12, 7, 9, 6]\n",
      "   0  1  2  3  4\n",
      "0  3  3  4  3  4\n",
      "1  2  2  2  2  2\n",
      "2  3  3  3  2  3\n",
      "3  3  2  2  2  4\n",
      "4  2  2  2  2  2\n",
      "['According to the website, the style for the Caldera Cauldron changes every year', \"The current release is a DIPA, which frankly is the only cauldron I'm familiar with (it was an IPA/DIPA the last time I ordered a cauldron at the horsebrass several years back)\", 'In any event', 'at the Horse Brass yesterday', 'The beer pours an orange copper color with good head retention and lacing']\n"
     ]
    }
   ],
   "source": [
    "# # Load Count\n",
    "sent_count_test = list(open(dataset_dir + sent_num_file[TEST_DATA], \"r\").readlines())\n",
    "sent_count_test = [int(s) for s in sent_count_test if (len(s) > 0 and s != \"\\n\")]\n",
    "print( sent_count_test[0:5] )\n",
    "\n",
    "# Load Ratings\n",
    "aspect_rating_test = list(open(dataset_dir + rating_file[TEST_DATA], \"r\").readlines())\n",
    "aspect_rating_test = [s for s in aspect_rating_test if (len(s) > 0 and s != \"\\n\")]\n",
    "\n",
    "aspect_rating_test = [s.split(\" \") for s in aspect_rating_test]\n",
    "aspect_rating_test = np.array(aspect_rating_test)[:, :]\n",
    "aspect_rating_test = aspect_rating_test.astype(np.float) - 1\n",
    "aspect_rating_test = np.rint(aspect_rating_test).astype(int)  # ROUND TO INTEGER =================\n",
    "aspect_rating_test = pd.DataFrame(aspect_rating_test)\n",
    "print( aspect_rating_test.head() )\n",
    "\n",
    "# Load Sents\n",
    "sents_test = list(open(dataset_dir + content_file[TEST_DATA], \"r\").readlines())\n",
    "sents_test = [s.strip() for s in sents_test]\n",
    "sents_test = [s[:-1] for s in sents_test if s.endswith(\".\")]\n",
    "print( sents_test[0:5] )\n",
    "\n",
    "# Sents to Doc\n",
    "docs_test = concat_to_doc(sents_test, sent_count_test)\n",
    "docs_test = pd.DataFrame({doc:docs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[According to the website, the style for the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[Poured from the bottle into a Chimay goblet, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[Notes from 6/24, A: Bright golden glowing bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[22 oz, bomber,, A: Pours a clear yellow with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[Brown in color, somewhere between a porter an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4                                                  5\n",
       "0  3  3  4  3  4  [According to the website, the style for the C...\n",
       "1  2  2  2  2  2  [Poured from the bottle into a Chimay goblet, ...\n",
       "2  3  3  3  2  3  [Notes from 6/24, A: Bright golden glowing bee...\n",
       "3  3  2  2  2  4  [22 oz, bomber,, A: Pours a clear yellow with ...\n",
       "4  2  2  2  2  2  [Brown in color, somewhere between a porter an..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.concat( [aspect_rating_test, docs_test], axis=1, ignore_index=True )\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clas_acc(asp_index):\n",
    "    def asp_acc(preds, targs):\n",
    "        preds = torch.max(preds, dim=2)[1]\n",
    "        targs = targs.contiguous().long()\n",
    "        return (preds[:,asp_index]==targs[:,asp_index]).float().mean()\n",
    "    return asp_acc\n",
    "def get_clas_mse(asp_index):\n",
    "    def asp_mse(preds, targs):\n",
    "        preds = torch.max(preds, dim=2)[1].float()[:,asp_index]\n",
    "        targs = targs.contiguous().float()[:,asp_index]\n",
    "        return torch.nn.functional.mse_loss(preds, targs)\n",
    "    return asp_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(self,\n",
    "              ds_type:DatasetType,\n",
    "              activ:nn.Module=None,\n",
    "              with_loss:bool=False,\n",
    "              n_batch:Optional[int]=None,\n",
    "              pbar:Optional[PBar]=None,\n",
    "              ordered:bool=False) -> List[Tensor]:\n",
    "    \"Return predictions and targets on the valid, train, or test set, depending on `ds_type`.\"\n",
    "    self.model.reset()\n",
    "    if ordered: np.random.seed(42)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outs = []\n",
    "        asps = []\n",
    "        for xb,yb in progress_bar(cls_learn.dl(ds_type)):\n",
    "            out,raw_enc,enc,asp = cls_learn.model(xb)\n",
    "            outs.append(out)\n",
    "            for doc in asp:\n",
    "                asps.append( to_float(doc.cpu()))\n",
    "\n",
    "    outs = to_float(torch.cat(outs).cpu())\n",
    "    \n",
    "    if ordered and hasattr(self.dl(ds_type), 'sampler'):\n",
    "        np.random.seed(42)\n",
    "        sampler = [i for i in self.dl(ds_type).sampler]\n",
    "        reverse_sampler = np.argsort(sampler)\n",
    "        \n",
    "        outs = outs[reverse_sampler]\n",
    "        asps = [asps[i] for i in reverse_sampler]\n",
    "    return (outs,asps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='778' class='' max='778', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [778/778 00:36<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outs,asps = get_preds(self=cls_learn, ds_type=DatasetType.Test, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24884, 6, 5])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 4, 3, 4],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 2, 3],\n",
       "        ...,\n",
       "        [4, 2, 4, 3, 4],\n",
       "        [2, 2, 1, 0, 3],\n",
       "        [3, 2, 4, 2, 4]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor( aspect_rating_test.values )\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9440)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mloss = MultiLabelCEL()\n",
    "mloss.forward(outs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASP0</th>\n",
       "      <th>ASP1</th>\n",
       "      <th>ASP2</th>\n",
       "      <th>ASP3</th>\n",
       "      <th>ASP4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.634785</td>\n",
       "      <td>0.62072</td>\n",
       "      <td>0.677544</td>\n",
       "      <td>0.636152</td>\n",
       "      <td>0.634785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ASP0     ASP1      ASP2      ASP3      ASP4\n",
       "0  0.634785  0.62072  0.677544  0.636152  0.634785"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict( {\"ASP\"+str(ai):[get_clas_acc(ai)(outs, target).item()] for ai in range(5)} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASP0</th>\n",
       "      <th>ASP1</th>\n",
       "      <th>ASP2</th>\n",
       "      <th>ASP3</th>\n",
       "      <th>ASP4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.443739</td>\n",
       "      <td>0.446873</td>\n",
       "      <td>0.376909</td>\n",
       "      <td>0.420712</td>\n",
       "      <td>0.432366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ASP0      ASP1      ASP2      ASP3      ASP4\n",
       "0  0.443739  0.446873  0.376909  0.420712  0.432366"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict( {\"ASP\"+str(ai):[get_clas_mse(ai)(outs, target).item()] for ai in range(5)} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth:\n",
      "[3, 3, 4, 3, 4]\n",
      "prediction:\n",
      "tensor([3, 3, 3, 3, 4])\n",
      "doc:\n",
      "According to the website, the style for the Caldera Cauldron changes every year\n",
      "          +++ Appearance +++ [0.02  0.042 0.021 0.014 0.032]\n",
      "The current release is a DIPA, which frankly is the only cauldron I'm familiar with (it was an IPA/DIPA the last time I ordered a cauldron at the horsebrass several years back)\n",
      "          +++ Appearance +++ [0.024 0.069 0.023 0.02  0.04 ]\n",
      "In any event\n",
      "          +++ Appearance +++ [0.025 0.072 0.026 0.021 0.04 ]\n",
      "at the Horse Brass yesterday\n",
      "          +++ Appearance +++ [0.025 0.079 0.027 0.022 0.04 ]\n",
      "The beer pours an orange copper color with good head retention and lacing\n",
      "          +++ Appearance +++ [9.225e-04 9.957e-01 5.844e-05 4.453e-03 2.528e-04]\n",
      "The nose is all hoppy IPA goodness, showcasing a huge aroma of dry citrus, pine and sandlewood\n",
      "          +++ Aroma +++ [7.361e-03 2.309e-02 2.116e-03 7.726e-04 9.884e-01]\n",
      "The flavor profile replicates the nose pretty closely in this West Coast all the way DIPA\n",
      "          +++ Taste +++ [0.141 0.081 0.543 0.091 0.165]\n",
      "This DIPA is not for the faint of heart and is a bit much even for a hophead like myslf\n",
      "          +++ Taste +++ [0.182 0.117 0.438 0.121 0.165]\n",
      "The finish is quite dry and hoppy, and there's barely enough sweet malt to balance and hold up the avalanche of hoppy bitterness in this beer\n",
      "          +++ Taste +++ [0.211 0.182 0.501 0.17  0.24 ]\n",
      "Mouthfeel is actually fairly light, with a long, persistentely bitter finish\n",
      "          +++ Palate +++ [0.078 0.143 0.051 0.772 0.027]\n",
      "Drinkability is good, with the alcohol barely noticeable in this well crafted beer\n",
      "          +++ Overall +++ [0.824 0.252 0.217 0.25  0.181]\n",
      "Still, this beer is so hugely hoppy/bitter, it's really hard for me to imagine ordering more than a single glass\n",
      "          +++ Overall +++ [0.876 0.253 0.286 0.278 0.185]\n",
      "Regardless, this is a very impressive beer from the folks at Caldera\n",
      "          +++ Taste +++ [0.467 0.366 0.716 0.357 0.437]\n",
      "===========\n",
      "truth:\n",
      "[2, 2, 2, 2, 2]\n",
      "prediction:\n",
      "tensor([2, 3, 2, 2, 2])\n",
      "doc:\n",
      "Poured from the bottle into a Chimay goblet\n",
      "          +++ Appearance +++ [0.062 0.163 0.075 0.068 0.099]\n",
      "Appearance: Pours a slightly cloudy yellow/orange color with a half finger of fluffy white head\n",
      "          +++ Appearance +++ [0.009 0.716 0.004 0.026 0.005]\n",
      "The head fades to a small layer on top of the pour\n",
      "          +++ Appearance +++ [0.011 0.652 0.005 0.02  0.009]\n",
      "Smell: Very light and crisp\n",
      "          +++ Aroma +++ [0.024 0.044 0.025 0.009 0.663]\n",
      "I'm definitely picking up the ginger, but it's not overly powerful\n",
      "          +++ Aroma +++ [0.031 0.058 0.036 0.014 0.645]\n",
      "There is a slight sweetness from the malt as well\n",
      "          +++ Aroma +++ [0.039 0.07  0.058 0.027 0.238]\n",
      "Taste: Very light and refreshing\n",
      "          +++ Taste +++ [0.136 0.109 0.35  0.1   0.157]\n",
      "The ginger shows up right away and then fades towards the finish of the sip\n",
      "          +++ Taste +++ [0.095 0.104 0.237 0.076 0.127]\n",
      "The finish is malty and bread like\n",
      "          +++ Taste +++ [0.069 0.082 0.17  0.055 0.098]\n",
      "Mouthfeel: The body is on the thin side with smooth carbonation and a very dry finish\n",
      "          +++ Palate +++ [0.055 0.158 0.058 0.549 0.042]\n",
      "Overall: This is a light and refreshing beer, but nothing spectacular\n",
      "          +++ Taste +++ [0.537 0.346 0.62  0.333 0.383]\n",
      "The amount of ginger is nice, but I would have liked to have more going on\n",
      "          +++ Taste +++ [0.428 0.312 0.514 0.283 0.35 ]\n",
      "===========\n",
      "truth:\n",
      "[3, 3, 3, 2, 3]\n",
      "prediction:\n",
      "tensor([3, 3, 2, 3, 2])\n",
      "doc:\n",
      "Notes from 6/24\n",
      "          +++ Appearance +++ [0.022 0.069 0.022 0.018 0.03 ]\n",
      "A: Bright golden glowing beer in a moment of clarity with a lively white head of feathery fluff\n",
      "          +++ Appearance +++ [0.006 0.883 0.001 0.015 0.004]\n",
      "S: The ginger is definitely there, or am I smelling my Indian dinner? Almost cake-like in its malt aroma, sharply bready with a slight edge of sweetness\n",
      "          +++ Aroma +++ [0.036 0.08  0.046 0.025 0.323]\n",
      "T: Nice clear malty throat taste, reminds me of strands of complex sugars and grains, ginger is more subtle than I expected, more of an undertone than a backbone\n",
      "          +++ Taste +++ [0.083 0.07  0.291 0.059 0.106]\n",
      "M: A refreshing light beer feel, like a pilsner or summer ale\n",
      "          +++ Palate +++ [0.052 0.148 0.047 0.769 0.028]\n",
      "D: If this was a sixer instead of a double-deuce, I could see this being a fine picnic pounder\n",
      "          +++ Overall +++ [0.605 0.3   0.283 0.275 0.234]\n",
      "Overall, pretty impressed for the particular style\n",
      "          +++ Taste +++ [0.383 0.309 0.47  0.271 0.341]\n",
      "===========\n",
      "truth:\n",
      "[3, 2, 2, 2, 4]\n",
      "prediction:\n",
      "tensor([2, 2, 2, 2, 4])\n",
      "doc:\n",
      "22 oz\n",
      "          +++ Appearance +++ [0.014 0.04  0.014 0.01  0.02 ]\n",
      "bomber,\n",
      "          +++ Appearance +++ [0.016 0.052 0.016 0.012 0.024]\n",
      "A: Pours a clear yellow with a mild white head, good retention\n",
      "          +++ Appearance +++ [1.126e-03 9.746e-01 1.166e-04 5.014e-03 5.162e-04]\n",
      "S: Great nose of ginger, honey, perfume\n",
      "          +++ Aroma +++ [0.01  0.026 0.004 0.001 0.97 ]\n",
      "T: Rather light upfront, it reminds me of Lawnmower, with a hint of ale fruitiness/Kolsch-like almost\n",
      "          +++ Taste +++ [0.087 0.075 0.279 0.064 0.118]\n",
      "Good ginger honey notes on the back end, not taking over the beer in any way\n",
      "          +++ Taste +++ [0.084 0.068 0.309 0.058 0.104]\n",
      "The ginger flavour is clear, but I wanted it to come out a little more in the end\n",
      "          +++ Taste +++ [0.143 0.084 0.485 0.092 0.141]\n",
      "M: Very light-bodied, watery, light base beer for sure\n",
      "          +++ Palate +++ [0.048 0.129 0.038 0.902 0.018]\n",
      "D: An easy drinking spiced beer, this will offend no one, but there's not a complexity to this brew at all\n",
      "          +++ Taste +++ [0.494 0.337 0.565 0.316 0.372]\n",
      "===========\n",
      "truth:\n",
      "[2, 2, 2, 2, 2]\n",
      "prediction:\n",
      "tensor([2, 2, 2, 2, 2])\n",
      "doc:\n",
      "Brown in color, somewhere between a porter and a brown ale\n",
      "          +++ Appearance +++ [0.022 0.33  0.018 0.033 0.035]\n",
      "Lacking in aroma, but no off stuff\n",
      "          +++ Aroma +++ [0.009 0.025 0.003 0.001 0.972]\n",
      "Same with the taste, lacking flavor, complexity, just went with smoothness\n",
      "          +++ Taste +++ [0.291 0.288 0.465 0.252 0.295]\n",
      "No off flavors though, so I can't say this is bad, just unadventurous, especially for Caldera, whom I think is generally underrated\n",
      "          +++ Taste +++ [0.387 0.342 0.619 0.316 0.388]\n",
      "You really have to search to pull anything out of this in terms of the usual chocolate/coffee flavors, really, the only thing I can tell is that the oats did their job, because this is smooth and unoffensive\n",
      "          +++ Taste +++ [0.393 0.324 0.438 0.291 0.307]\n",
      "Other than that, extremely pedestrian\n",
      "          +++ Taste +++ [0.347 0.333 0.504 0.289 0.369]\n",
      "===========\n",
      "truth:\n",
      "[4, 2, 4, 2, 4]\n",
      "prediction:\n",
      "tensor([3, 3, 4, 3, 4])\n",
      "doc:\n",
      "Pours a mahogany color, rich, with a tan head\n",
      "          +++ Appearance +++ [0.004 0.784 0.001 0.007 0.005]\n",
      "a finger, didn't stick around\n",
      "          +++ Appearance +++ [4.144e-03 8.914e-01 8.518e-04 1.076e-02 4.327e-03]\n",
      "Holy smokes! This is one of the most smokiest beer I've ever had! BUT\n",
      "          +++ Aroma +++ [0.021 0.042 0.017 0.007 0.496]\n",
      "It's balance and not one sided\n",
      "          +++ Aroma +++ [0.111 0.115 0.253 0.069 0.392]\n",
      "Smoke, cured bacon, rich caramel malts\n",
      "          +++ Aroma +++ [0.06  0.083 0.123 0.044 0.19 ]\n",
      "More smoke\n",
      "          +++ Taste +++ [0.051 0.056 0.136 0.038 0.101]\n",
      "Ok, maybe it one sided, but it's heavenly\n",
      "          +++ Aroma +++ [0.164 0.103 0.514 0.063 0.625]\n",
      "Rich caramel malts and salty cured bacon strips\n",
      "          +++ Taste +++ [0.065 0.052 0.217 0.042 0.097]\n",
      "Bacon lover right here, so, I don't feel the need to go on\n",
      "          +++ Taste +++ [0.186 0.135 0.468 0.128 0.208]\n",
      "Lighter, but medium in body\n",
      "          +++ Palate +++ [0.021 0.062 0.025 0.583 0.011]\n",
      "feels fine\n",
      "          +++ Palate +++ [0.017 0.067 0.012 0.942 0.004]\n",
      "Nothing tremendous\n",
      "          +++ Palate +++ [0.078 0.153 0.057 0.86  0.028]\n",
      "Aroma and flavor alone are mouthwatering\n",
      "          +++ Aroma +++ [0.148 0.146 0.357 0.068 0.696]\n",
      "Bravo\n",
      "          +++ Taste +++ [0.3   0.306 0.583 0.254 0.411]\n",
      "More please!!!\n",
      "          +++ Taste +++ [0.271 0.303 0.438 0.242 0.335]\n",
      "===========\n",
      "truth:\n",
      "[4, 3, 4, 3, 4]\n",
      "prediction:\n",
      "tensor([3, 3, 4, 3, 4])\n",
      "doc:\n",
      "Pours light caramel brown with reddish highlights\n",
      "          +++ Appearance +++ [0.018 0.187 0.015 0.027 0.023]\n",
      "The crisp white head doesn't hang around long, but quickly dissipates to a thin ring\n",
      "          +++ Appearance +++ [3.190e-03 9.727e-01 4.768e-04 1.366e-02 1.461e-03]\n",
      "Aroma is intense meaty smokyness\n",
      "          +++ Aroma +++ [0.015 0.028 0.014 0.005 0.418]\n",
      "Underneath is a mild malty sweetness\n",
      "          +++ Aroma +++ [0.033 0.063 0.043 0.022 0.215]\n",
      "Taste is really interesting\n",
      "          +++ Taste +++ [0.071 0.046 0.314 0.043 0.101]\n",
      "The smoked meat quality is retained, but there is also a generous earthy, woody character\n",
      "          +++ Taste +++ [0.072 0.072 0.235 0.053 0.107]\n",
      "Caramel and toffee really round things out\n",
      "          +++ Taste +++ [0.078 0.075 0.274 0.058 0.114]\n",
      "Finishes with a mildly dry cedar character\n",
      "          +++ Taste +++ [0.064 0.069 0.192 0.048 0.09 ]\n",
      "Mouthfeel is a chewy, medium body\n",
      "          +++ Palate +++ [0.017 0.053 0.023 0.411 0.011]\n",
      "A flavorful brew\n",
      "          +++ Overall +++ [0.394 0.315 0.387 0.273 0.294]\n",
      "Strong smokiness is tempered by the rich caramel malt qualities\n",
      "          +++ Taste +++ [0.249 0.254 0.378 0.204 0.28 ]\n",
      "A real winner that could bring the non-smoke lover around\n",
      "          +++ Taste +++ [0.431 0.342 0.66  0.322 0.424]\n",
      "===========\n",
      "truth:\n",
      "[4, 3, 3, 3, 4]\n",
      "prediction:\n",
      "tensor([3, 2, 4, 3, 4])\n",
      "doc:\n",
      "Poured a slightly cloudy deep amber/red color with a thin soap ring of head\n",
      "          +++ Appearance +++ [3.059e-03 9.714e-01 4.252e-04 1.523e-02 1.454e-03]\n",
      "The smell is amazing, big bacony smoked pork Rauch goodness\n",
      "          +++ Aroma +++ [4.131e-03 1.601e-02 7.204e-04 2.348e-04 9.975e-01]\n",
      "The taste follows the flavors nicely with a big smoked malt flavor with a nice bit of underlying sweetness, very pork like and very flavorful\n",
      "          +++ Taste +++ [0.224 0.128 0.718 0.147 0.238]\n",
      "Has a nice light bitterness in the finish that helps even it out very well\n",
      "          +++ Taste +++ [0.179 0.134 0.551 0.136 0.203]\n",
      "The mouthfeel is very slick and oily\n",
      "          +++ Palate +++ [0.031 0.096 0.032 0.679 0.016]\n",
      "Great Rauchbier!\n",
      "          +++ Taste +++ [0.461 0.378 0.639 0.361 0.393]\n",
      "===========\n",
      "truth:\n",
      "[4, 3, 3, 3, 4]\n",
      "prediction:\n",
      "tensor([4, 4, 4, 3, 4])\n",
      "doc:\n",
      "A- Semi aggressive pour produces a 1-1/2 finger very light brown head that rests atop a beautiful deep copper colored beer with nice clarity\n",
      "          +++ Appearance +++ [1.425e-03 9.956e-01 1.116e-04 6.354e-03 3.526e-04]\n",
      "Nicve lacing\n",
      "          +++ Appearance +++ [3.789e-03 9.793e-01 5.391e-04 1.274e-02 1.315e-03]\n",
      "S- Very similar to a smoked porter but more smoke\n",
      "          +++ Aroma +++ [0.037 0.061 0.064 0.026 0.205]\n",
      "The smoke smell is beautiful\n",
      "          +++ Aroma +++ [0.016 0.04  0.008 0.003 0.958]\n",
      "It smells like a campfire and the cherry wood is definitely present\n",
      "          +++ Aroma +++ [0.032 0.075 0.021 0.011 0.886]\n",
      "Excellent smelling beer\n",
      "          +++ Aroma +++ [1.636e-03 6.476e-03 2.269e-04 4.237e-05 9.996e-01]\n",
      "T- WOW! They were not kidding on the bottle when they said it has an intense smoke flavor\n",
      "          +++ Taste +++ [0.121 0.09  0.384 0.085 0.166]\n",
      "It is excellent and I could see some folks that the smoke is too in-your-face, but I love smoked beers and I love this one\n",
      "          +++ Taste +++ [0.305 0.139 0.827 0.179 0.28 ]\n",
      "This is as smoked wood forward as I've ever had\n",
      "          +++ Taste +++ [0.205 0.125 0.664 0.138 0.225]\n",
      "It's pretty much smoky wood, a slight malt sweetness, and finishes woody and smokey again\n",
      "          +++ Taste +++ [0.181 0.118 0.568 0.126 0.191]\n",
      "This is a perfect fall beer\n",
      "          +++ Taste +++ [0.401 0.252 0.786 0.275 0.377]\n",
      "M- Nice smooth mouthfeel with good carbonation\n",
      "          +++ Palate +++ [0.07  0.151 0.062 0.887 0.024]\n",
      "O- Not too complex because of the intense smoke taste but it works excellent in this beer\n",
      "          +++ Taste +++ [0.5   0.37  0.769 0.374 0.461]\n",
      "I will definitely be picking this up again\n",
      "          +++ Taste +++ [0.415 0.359 0.645 0.333 0.409]\n",
      "===========\n",
      "truth:\n",
      "[2, 2, 2, 1, 4]\n",
      "prediction:\n",
      "tensor([2, 3, 2, 2, 4])\n",
      "doc:\n",
      "Amber to yeloowcolored, was expecting darker, kind of even had a good degree of clarity to it\n",
      "          +++ Appearance +++ [0.006 0.78  0.001 0.016 0.005]\n",
      "Holy cherry wood campfire smoke! This smelled just like you were out in the woods\n",
      "          +++ Aroma +++ [0.016 0.028 0.012 0.005 0.582]\n",
      "Whoa, I could only take a small sip at first, massive liquid smoke feeling in the mouthfeel\n",
      "          +++ Aroma +++ [0.036 0.063 0.05  0.04  0.082]\n",
      "Ugh\n",
      "          +++ Taste +++ [0.107 0.13  0.207 0.094 0.18 ]\n",
      "Its not so much as 'bad' as opposed to 'challenging'\n",
      "          +++ Taste +++ [0.22  0.203 0.39  0.173 0.259]\n",
      "I must admit, I could not even finish a taster of it\n",
      "          +++ Taste +++ [0.447 0.332 0.481 0.318 0.36 ]\n",
      "This is the most intense smoked beer I've ever come across, by far\n",
      "          +++ Taste +++ [0.138 0.093 0.341 0.084 0.188]\n",
      "I mean, beer geek bacon was a walk in the park compared to this\n",
      "          +++ Aroma +++ [0.072 0.078 0.122 0.053 0.125]\n",
      "I could smell it all day\n",
      "          +++ Aroma +++ [0.03  0.071 0.018 0.006 0.955]\n",
      "As far as drinking it, I'm cool with 1oz at a time, its that intense\n",
      "          +++ Overall +++ [0.584 0.192 0.278 0.185 0.177]\n",
      "If you like smoked beers, then check this out\n",
      "          +++ Taste +++ [0.192 0.205 0.375 0.163 0.262]\n",
      "If not, stay away\n",
      "          +++ Taste +++ [0.166 0.182 0.329 0.139 0.245]\n",
      "I kind of like them, trying to get into them more, so I'd try it again next time, but for now, I'm not worthy\n",
      "          +++ Taste +++ [0.44  0.332 0.662 0.32  0.418]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "asp_inc_overall = True\n",
    "if not asp_inc_overall: \n",
    "    nasp_analysis = hyper_params[\"num_aspect\"] - 1\n",
    "else:\n",
    "    nasp_analysis = hyper_params[\"num_aspect\"]\n",
    "    \n",
    "np.set_printoptions(precision=3)\n",
    "asp_name = [\"Overall\", \"Appearance\", \"Taste\", \"Palate\", \"Aroma\"]\n",
    "for i in range(10):\n",
    "    print(\"truth:\")\n",
    "    print(df_test.iloc[i,0:5].values.flatten().tolist() )\n",
    "    print(\"prediction:\")\n",
    "    print( torch.argmax(outs[i][0:5],dim=1) )\n",
    "    print(\"doc:\")\n",
    "    dasp = torch.argmax(asps[i][:,0:nasp_analysis],dim=1).numpy()\n",
    "    if asp_inc_overall: dasp_noall = torch.argmax(asps[i][:,1:6],dim=1).numpy()\n",
    "#     dasp_dist = torch.nn.functional.softmax(asps[i][:,0:nasp_analysis], dim=1).numpy()\n",
    "    dasp_dist = asps[i][:,0:nasp_analysis].numpy()\n",
    "    for senti,s in enumerate(df_test.iloc[i,-1]):\n",
    "        print(s)\n",
    "        if asp_inc_overall:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]] + \" +++ \" + str(dasp_dist[senti]) )\n",
    "        else:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]+1] + \" +++ \" + str(dasp_dist[senti]) )\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hotel_asp(asp_pred, asp_true, asp_inc_overall):\n",
    "    asp_to_id = {\"appearance\":0, \"taste\":1, \"palate\":2, \"aroma\":3, \"none\":-1}\n",
    "    asp_true = np.array( [asp_to_id[l] for l in asp_true] )\n",
    "    print(\"total true: \" + str(len(asp_true)) )\n",
    "    print(\"total not none: \" + str(sum(asp_true>0)) )\n",
    "    \n",
    "    asp_pred_index = []\n",
    "    if asp_inc_overall:\n",
    "        for i in range(1000):\n",
    "            asp_pred_index.append( asp_pred[i][:,1:6].numpy().argsort() )\n",
    "    else:\n",
    "        for i in range(1000):\n",
    "            asp_pred_index.append( asp_pred[i][:,0:5].numpy().argsort() )\n",
    "    asp_pred_index = np.concatenate( asp_pred_index , axis=0)\n",
    "    \n",
    "    result_index = []\n",
    "    for i,lbl in enumerate(asp_true):\n",
    "        if(lbl==-1):\n",
    "            result_index.append(-1)\n",
    "        else:\n",
    "            at = np.where(asp_pred_index[i,] == lbl)\n",
    "            result_index.append(at[0][0])\n",
    "    result_index = np.array(result_index)\n",
    "    \n",
    "    print(\"Top 1 ACC:\")\n",
    "    print( sum(result_index>=4) / sum(result_index>=0) )\n",
    "    print(\"Top 2 ACC:\")\n",
    "    print( sum(result_index>=3) / sum(result_index>=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ee5f835c35b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_dir' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "yifan_label = open(dataset_dir + \"test_aspect_0.yifanmarjan.aspect\", \"r\").readlines()\n",
    "yifan_label = [s.split()[0] for s in yifan_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true: 1000\n",
      "total not none: 454\n",
      "Top 1 ACC:\n",
      "0.6763005780346821\n",
      "Top 2 ACC:\n",
      "0.9152215799614644\n"
     ]
    }
   ],
   "source": [
    "eval_hotel_asp(asps, yifan_label, asp_inc_overall=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_label = open(dataset_dir + \"test_aspect_0.fan.aspect\", \"r\").readlines()\n",
    "fan_label = [s.split()[0] for s in fan_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true: 621\n",
      "total not none: 288\n",
      "Top 1 ACC:\n",
      "0.6814159292035398\n",
      "Top 2 ACC:\n",
      "0.887905604719764\n"
     ]
    }
   ],
   "source": [
    "eval_hotel_asp(asps, fan_label, asp_inc_overall=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
