{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 9232, 27941, 45, 10, 195, 999, 5753, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"definitely not a 5 star resort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.modeling_longformer import LongformerModel, LongformerPreTrainedModel\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.init(project=\"my-project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df_path):\n",
    "        self.df = pd.read_pickle(df_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.df.iloc[idx,6], self.df.iloc[idx,0:6].to_numpy().astype(np.float) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongformerBaseline(LongformerPreTrainedModel):\n",
    "\n",
    "    authorized_unexpected_keys = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.longformer = LongformerModel(config, add_pooling_layer=False)\n",
    "        self.classifier = BaselineClasHead(config, num_aspect=6, num_rating=5)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        global_attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if global_attention_mask is None:\n",
    "            global_attention_mask = torch.zeros_like(input_ids)\n",
    "            # global attention on cls token\n",
    "            global_attention_mask[:, 0] = 1\n",
    "\n",
    "        outputs = self.longformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class BaselineClasHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config, num_aspect, num_rating):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.dp1 = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(config.hidden_size, 400)\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(400)\n",
    "        self.dp2 = nn.Dropout(0.4)\n",
    "        self.dense2 = nn.Linear(400, num_aspect * num_rating)\n",
    "\n",
    "    def forward(self, hidden_states, **kwargs):\n",
    "        hidden_states = hidden_states[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        \n",
    "        hidden_states = self.ln1(hidden_states)\n",
    "        hidden_states = self.dp1(hidden_states)\n",
    "        hidden_states = self.dense1(hidden_states)\n",
    "        \n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        \n",
    "        hidden_states = self.ln2(hidden_states)\n",
    "        hidden_states = self.dp2(hidden_states)\n",
    "        hidden_states = self.dense2(hidden_states)\n",
    "        \n",
    "        return hidden_states.view(-1, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerCollate:\n",
    "    def __init__(self):\n",
    "        self.tkz = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        batch_split = list(zip(*batch))\n",
    "        seqs, targs= batch_split[0], batch_split[1]\n",
    "        encode = self.tkz(seqs, padding=\"longest\")\n",
    "        return torch.tensor(encode[\"input_ids\"]), torch.tensor(encode[\"attention_mask\"]), torch.tensor(targs)\n",
    "    \n",
    "class MultiLabelCEL(nn.CrossEntropyLoss):\n",
    "    def forward(self, input, target, nasp=6):\n",
    "        target = target.long()\n",
    "        loss = 0\n",
    "        for i in range(nasp):\n",
    "            loss = loss + super(MultiLabelCEL, self).forward(input[:,i,:], target[:,i])\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class AspectACC(pl.metrics.metric.Metric):\n",
    "    def __init__(self, aspect: int,\n",
    "                compute_on_step: bool = True,\n",
    "                ddp_sync_on_step: bool = False,\n",
    "                process_group: Optional[Any] = None,):\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step,\n",
    "            ddp_sync_on_step=ddp_sync_on_step,\n",
    "            process_group=process_group,)\n",
    "        \n",
    "        self.aspect = aspect\n",
    "        self.add_state(\"correct\", default=torch.tensor(0).cuda(), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0).cuda(), dist_reduce_fx=\"sum\")\n",
    "    \n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        preds = torch.argmax(preds, dim=2)\n",
    "        assert preds.shape == target.shape\n",
    "        \n",
    "        target = target.contiguous().long()\n",
    "        \n",
    "        self.correct += torch.sum( preds[:, self.aspect]==target[:, self.aspect] )\n",
    "        self.total += target[:, self.aspect].numel()\n",
    "        \n",
    "    def compute(self):\n",
    "        return self.correct.float() / self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningLongformerBaseline(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.train_config = config\n",
    "        self.longformer = LongformerBaseline.from_pretrained('allenai/longformer-base-4096',\n",
    "                                                             cache_dir=self.train_config[\"cache_dir\"],\n",
    "                                                             gradient_checkpointing=True\n",
    "                                                               )\n",
    "        self.lossfunc = MultiLabelCEL()\n",
    "        self.metrics = [AspectACC(aspect=i) for i in range(6)]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.train_config[\"learning_rate\"])\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        self.dataset_train = ReviewDataset(\"../../data/hotel_balance_LengthFix1_3000per/df_train.pickle\")\n",
    "        self.loader_train = DataLoader(self.dataset_train,\n",
    "                                        batch_size=train_config[\"batch_size\"],\n",
    "                                        collate_fn=TokenizerCollate(),\n",
    "                                        num_workers=2,\n",
    "                                        pin_memory=True, drop_last=False, shuffle=False)\n",
    "        return self.loader_train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        self.dataset_val = ReviewDataset(\"../../data/hotel_balance_LengthFix1_3000per/df_test.pickle\")\n",
    "        self.loader_val = DataLoader(self.dataset_val,\n",
    "                                        batch_size=train_config[\"batch_size\"],\n",
    "                                        collate_fn=TokenizerCollate(),\n",
    "                                        num_workers=2,\n",
    "                                        pin_memory=True, drop_last=True, shuffle=False)\n",
    "        return self.loader_val\n",
    "    \n",
    "#     @autocast()\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        logits = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = self.lossfunc(logits, labels)\n",
    "\n",
    "        return (loss, logits)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, mask, label  = batch[0].type(torch.int64), batch[1].type(torch.int64), batch[2].type(torch.int64)\n",
    "        \n",
    "        loss, logits = self(input_ids=input_ids, attention_mask=mask, labels=label)\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return {\"loss\":loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, mask, label  = batch[0].type(torch.int64), batch[1].type(torch.int64), batch[2].type(torch.int64)\n",
    "        \n",
    "        loss, logits = self(input_ids=input_ids, attention_mask=mask, labels=label)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, reduce_fx=torch.mean, prog_bar=False)\n",
    "        accs = [m(logits, label) for m in self.metrics]  # update metric counters\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        for i,m in enumerate(self.metrics):\n",
    "            self.log('acc'+str(i), m.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {}\n",
    "train_config[\"cache_dir\"] = \"./cache/\"\n",
    "train_config[\"epochs\"] = 6\n",
    "train_config[\"batch_size\"] = 10\n",
    "train_config[\"accumulate_grad_batches\"] = 8\n",
    "train_config[\"gradient_clip_val\"] = 1.2\n",
    "train_config[\"learning_rate\"] = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"saam_hotel_longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeryen\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.5<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">baseline_accumu</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/aeryen/saam_hotel_longformer\" target=\"_blank\">https://wandb.ai/aeryen/saam_hotel_longformer</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/aeryen/saam_hotel_longformer/runs/25y7g0ed\" target=\"_blank\">https://wandb.ai/aeryen/saam_hotel_longformer/runs/25y7g0ed</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201010_155959-25y7g0ed</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(name='baseline_accumu',project='saam_hotel_longformer')\n",
    "wandb_logger.log_hyperparams(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerBaseline: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerBaseline from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing LongformerBaseline from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerBaseline were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.ln1.weight', 'classifier.ln1.bias', 'classifier.dense1.weight', 'classifier.dense1.bias', 'classifier.ln2.weight', 'classifier.ln2.bias', 'classifier.dense2.weight', 'classifier.dense2.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LightningLongformerBaseline(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=train_config[\"epochs\"],\n",
    "                     accumulate_grad_batches=train_config[\"accumulate_grad_batches\"],\n",
    "                     gradient_clip_val=train_config[\"gradient_clip_val\"],\n",
    "                     gpus=1, num_nodes=1,\n",
    "                     logger=wandb_logger,\n",
    "                     log_every_n_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | longformer | LongformerBaseline | 148 M \n",
      "1 | lossfunc   | MultiLabelCEL      | 0     \n",
      "/home/aeryen/anaconda3/envs/saam/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeryen/anaconda3/envs/saam/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/aeryen/anaconda3/envs/saam/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe7d838dfba4b309215449e0726de85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeryen/anaconda3/envs/saam/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeryen/anaconda3/envs/saam/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/aeryen/anaconda3/envs/saam/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/aeryen/anaconda3/envs/saam/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/aeryen/anaconda3/envs/saam/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/aeryen/anaconda3/envs/saam/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct  4 01:45:54 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.95.01    Driver Version: 440.95.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 41%   56C    P2    68W / 280W |   1733MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:02:00.0  On |                  N/A |\n",
      "|  0%   35C    P8    26W / 250W |    800MiB /  7959MiB |      2%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0    113723      C   ...e/aeryen/anaconda3/envs/saam/bin/python  1721MiB |\n",
      "|    1      1236      G   /usr/lib/xorg/Xorg                            53MiB |\n",
      "|    1      2117      G   /usr/lib/xorg/Xorg                           325MiB |\n",
      "|    1      2251      G   /usr/bin/gnome-shell                         190MiB |\n",
      "|    1      3482      G   /usr/lib/firefox/firefox                       2MiB |\n",
      "|    1      3778      G   /usr/lib/firefox/firefox                       2MiB |\n",
      "|    1      7435      G   /usr/lib/firefox/firefox                       2MiB |\n",
      "|    1     18739      G   /usr/lib/firefox/firefox                       2MiB |\n",
      "|    1     19308      G   /usr/lib/firefox/firefox                       2MiB |\n",
      "|    1     19362      G   /usr/lib/firefox/firefox                       2MiB |\n",
      "|    1     19540      G   /usr/lib/firefox/firefox                       2MiB |\n",
      "|    1    110098      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files   182MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
