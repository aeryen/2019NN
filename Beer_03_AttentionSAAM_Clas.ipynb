{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/aeryen/2019nn-beer/08e40a2cc6344488acf283269a92eb31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "experiment = comet_ml.Experiment(project_name=\"2019nn_beer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from fastai.text import *\n",
    "from data_helpers.Data import *\n",
    "from fastai.text.transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    \"max_sequence_length\": 20*70,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs1\": 12,\n",
    "    \"num_epochs2\": 15,\n",
    "    \"num_aspect\": 5,\n",
    "    \"num_rating\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LM Databunch and LM Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_db = load_data(\"./data/\", \"hotel_lm_databunch.1001\")\n",
    "# lm_learn = language_model_learner(lm_db, AWD_LSTM)\n",
    "# lm_learn = lm_learn.load(\"lang_model_hotel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_learn.save_encoder('lang_model_hotel_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_db = load_data(\"./data/\", \"beer_clas_databunch_rint.TraValTes\")\n",
    "cls_db.batch_size=hyper_params[\"batch_size\"]\n",
    "cls_db.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Feature Combo Pooling (1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_combo(output, start, end):\n",
    "    avg_pool = output[start:end, :].mean(dim=0)\n",
    "    max_pool = output[start:end, :].max(dim=0)[0]\n",
    "    x = torch.cat([output[-1,:], max_pool, avg_pool], 0)\n",
    "    return x\n",
    "\n",
    "def sentence_pool_1200(outputs, mask, p_index):\n",
    "    \"Pool MultiBatchEncoder outputs into one vector [last_hidden, max_pool, avg_pool].\"\n",
    "    output = outputs[-1]\n",
    "    seq_max = output.size(1)\n",
    "    doc_start = mask.int().sum(dim=1)\n",
    "    \n",
    "    batch = []\n",
    "    for doci in range(0,output.shape[0]):\n",
    "        pi = p_index[doci,:].nonzero(as_tuple=True)[0].int()\n",
    "        doc = []\n",
    "        for senti in range( len(pi) ):\n",
    "            if senti==0:\n",
    "                doc.append( pool_combo(output[doci,:,:], doc_start[doci], pi[senti]) )\n",
    "            else:\n",
    "                doc.append( pool_combo(output[doci,:,:], pi[senti-1]+1, pi[senti]) )\n",
    "            \n",
    "        batch.append( torch.stack(doc, 0) )\n",
    "\n",
    "    return batch\n",
    "\n",
    "def sentence_pool_400(outputs, mask, p_index):\n",
    "    output = outputs[-1]\n",
    "    \n",
    "    batch = []\n",
    "    for doci in range(0,output.shape[0]):\n",
    "        doc = output[doci,p_index[doci,:],:]\n",
    "        batch.append( doc )\n",
    "\n",
    "    return batch\n",
    "\n",
    "def masked_concat_pool(outputs, mask):\n",
    "    \"Pool MultiBatchEncoder outputs into one vector [last_hidden, max_pool, avg_pool].\"\n",
    "    output = outputs[-1]\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).mean(dim=1)\n",
    "    avg_pool *= output.size(1) / (output.size(1)-mask.type(avg_pool.dtype).sum(dim=1))[:,None]\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    x = torch.cat([output[:,-1], max_pool, avg_pool], 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(Module):\n",
    "    \"Create an encoder over `module` that can process a full sentence.\"\n",
    "    def __init__(self, bptt:int, max_len:int, module:nn.Module, vocab, pad_idx:int=1):\n",
    "        print(\"Encoder init\")\n",
    "        self.max_len,self.bptt,self.module,self.pad_idx = max_len,bptt,module,pad_idx\n",
    "        self.vocab = vocab\n",
    "        self.period_index = self.vocab.stoi[\"xxperiod\"]\n",
    "\n",
    "    def concat(self, arrs:Collection[Tensor])->Tensor:\n",
    "        \"Concatenate the `arrs` along the batch dimension.\"\n",
    "        return [torch.cat([l[si] for l in arrs], dim=1) for si in range_of(arrs[0])]\n",
    "\n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()\n",
    "\n",
    "    def forward(self, input:LongTensor)->Tuple[Tensor,Tensor]:\n",
    "        bs,sl = input.size()\n",
    "        self.reset()\n",
    "        raw_outputs,outputs,masks = [],[],[]\n",
    "        p_index = []\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            r, o = self.module(input[:,i: min(i+self.bptt, sl)])\n",
    "            if i>(sl-self.max_len):\n",
    "                masks.append(input[:,i: min(i+self.bptt, sl)] == self.pad_idx)\n",
    "                raw_outputs.append(r)\n",
    "                outputs.append(o)\n",
    "                p_index.append( input[:,i: min(i+self.bptt, sl)] == self.period_index )\n",
    "\n",
    "                \n",
    "        # print(\"number of sentences in docs:\")\n",
    "#         n_sent = torch.sum( x==self.vocab.stoi[\"xxperiod\"] , dim=1)\n",
    "        # print(n_sent)\n",
    "        \n",
    "        # print(\"locating period marks\")\n",
    "        period_index = torch.cat(p_index,dim=1)\n",
    "        \n",
    "        return self.concat(raw_outputs),self.concat(outputs), \\\n",
    "               torch.cat(masks,dim=1),period_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLS 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.add_tag(\"CLAS02\")\n",
    "# experiment.add_tag(\"ATTIND400\")\n",
    "\n",
    "# ATTENTIONAL AVERAGING, INDEPENDENT SENTI OUT\n",
    "\n",
    "class Cls02ATT400(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int, layers:Collection[int], drops:Collection[float]):\n",
    "        print(\"CLS init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp + 1\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_asp, p=0.1, actn=torch.nn.Softmax(dim=1) )\n",
    "#         mod_layers += bn_drop_lin( 50, self.n_asp, p=0, actn=None )\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        self.senti_base = nn.Sequential(*bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) ) )\n",
    "        \n",
    "        self.s0 = nn.Sequential(* bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) )\n",
    "        self.s1 = nn.Sequential(* bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) )\n",
    "        self.s2 = nn.Sequential(* bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) )\n",
    "        self.s3 = nn.Sequential(* bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) )\n",
    "        self.s4 = nn.Sequential(* bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) )\n",
    "        self.s5 = nn.Sequential(* bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) )\n",
    "        self.s6 = nn.Sequential(* bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) )\n",
    "        self.sentiments = []\n",
    "        self.sentiments.append( self.s0 )\n",
    "        self.sentiments.append( self.s1 )\n",
    "        self.sentiments.append( self.s2 )\n",
    "        self.sentiments.append( self.s3 )\n",
    "        self.sentiments.append( self.s4 )\n",
    "        self.sentiments.append( self.s5 )\n",
    "        self.sentiments.append( self.s6 )\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "        \n",
    "        batch = sentence_pool_400(outputs, mask, p_index)\n",
    "        \n",
    "        allsent_emb = torch.cat(batch, dim=0)          # [n_sentence, emb400]\n",
    "        aspect_dist = self.aspect(allsent_emb)         # [n_sentence, aspect6]\n",
    "\n",
    "        sent_bmm = torch.bmm(aspect_dist.unsqueeze(2), allsent_emb.unsqueeze(1))  # [319, 7, 400]\n",
    "        \n",
    "        all_doc_emb = []\n",
    "        aspect_doc = []\n",
    "        sentim_doc = []\n",
    "        cur = 0\n",
    "        for doci in range(0, len(batch)):\n",
    "            sn = batch[doci].shape[0]\n",
    "            doc_emb_avg = torch.sum(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True) # [1, 7, 400]\n",
    "            asp_w_sum = torch.sum(aspect_dist[cur:(cur+sn),:], dim=0, keepdim=True) # [1, 7]\n",
    "            doc_emb_avg = doc_emb_avg / asp_w_sum[:,:,None]                                 # [1, 7, 400]\n",
    "#             doc_emb_max = torch.max(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True)[0] # [1, 7, 400]\n",
    "#             all_doc_emb.append( torch.cat( [doc_emb_avg, doc_emb_max], dim=2 ) )\n",
    "            all_doc_emb.append( doc_emb_avg )\n",
    "            aspect_doc.append( aspect_dist[cur:(cur+sn), :] )\n",
    "            \n",
    "            cur = cur + sn\n",
    "\n",
    "        all_doc_emb = torch.cat( all_doc_emb, dim=0 )          # [batch, asp, 1200]\n",
    "        \n",
    "        result_senti_base = self.senti_base( all_doc_emb.view(-1, 400) ) # [batch*asp, 50]\n",
    "        result_senti_base = result_senti_base.view(-1, self.n_asp, 50)    # [batch, asp, 50]\n",
    "        \n",
    "        result_senti = [ self.sentiments[aspi]( result_senti_base[:,aspi,:] ) for aspi in range(0,self.n_asp)] # [batch, ra]\n",
    "        \n",
    "        result = torch.stack(result_senti, dim=1)  # [batch, asp, sentiment5]\n",
    "        \n",
    "        return result,raw_outputs,outputs,aspect_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_tag(\"CLAS02\")\n",
    "experiment.add_tag(\"FULLIND400\")\n",
    "\n",
    "# ATTENTIONAL AVERAGING, COMPLETELY INDEPENDENT SENTI OUT\n",
    "\n",
    "class Cls02ATT400(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int, layers:Collection[int], drops:Collection[float]):\n",
    "        print(\"CLS init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp + 1\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_asp, p=0.1, actn=torch.nn.Softmax(dim=1) )\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "#         self.senti_base = nn.Sequential(*bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) ) )\n",
    "        self.s0 = nn.Sequential(* (bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s1 = nn.Sequential(* (bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s2 = nn.Sequential(* (bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s3 = nn.Sequential(* (bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s4 = nn.Sequential(* (bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.s5 = nn.Sequential(* (bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "                                   bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) ) )\n",
    "#         self.s6 = nn.Sequential(* (bn_drop_lin( 400, 50, p=0.5, actn=nn.ReLU(inplace=True) ) + \n",
    "#                                    bn_drop_lin( 50, self.n_rat, p=0.1, actn=None ) ) )\n",
    "        self.sentiments = []\n",
    "        self.sentiments.append( self.s0 )\n",
    "        self.sentiments.append( self.s1 )\n",
    "        self.sentiments.append( self.s2 )\n",
    "        self.sentiments.append( self.s3 )\n",
    "        self.sentiments.append( self.s4 )\n",
    "        self.sentiments.append( self.s5 )\n",
    "#         self.sentiments.append( self.s6 )\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "        \n",
    "        batch = sentence_pool_400(outputs, mask, p_index)\n",
    "        \n",
    "        allsent_emb = torch.cat(batch, dim=0)          # [n_sentence, emb400]\n",
    "        aspect_dist = self.aspect(allsent_emb)         # [n_sentence, aspect6]\n",
    "\n",
    "        sent_bmm = torch.bmm(aspect_dist.unsqueeze(2), allsent_emb.unsqueeze(1))  # [319, 7, 400]\n",
    "        \n",
    "        all_doc_emb = []\n",
    "        aspect_doc = []\n",
    "        sentim_doc = []\n",
    "        cur = 0\n",
    "        for doci in range(0, len(batch)):\n",
    "            sn = batch[doci].shape[0]\n",
    "            doc_emb_avg = torch.sum(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True) # [1, 7, 400]\n",
    "            asp_w_sum = torch.sum(aspect_dist[cur:(cur+sn),:], dim=0, keepdim=True) # [1, 7]\n",
    "            doc_emb_avg = doc_emb_avg / asp_w_sum[:,:,None]                                 # [1, 7, 400]\n",
    "#             doc_emb_max = torch.max(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True)[0] # [1, 7, 400]\n",
    "#             all_doc_emb.append( torch.cat( [doc_emb_avg, doc_emb_max], dim=2 ) )\n",
    "            all_doc_emb.append( doc_emb_avg )\n",
    "            aspect_doc.append( aspect_dist[cur:(cur+sn), :] )\n",
    "            \n",
    "            cur = cur + sn\n",
    "\n",
    "        all_doc_emb = torch.cat( all_doc_emb, dim=0 )          # [batch, asp, 400]\n",
    "        \n",
    "#         result_senti_base = self.senti_base( all_doc_emb.view(-1, 400) ) # [batch*asp, 50]\n",
    "#         result_senti_base = result_senti_base.view(-1, self.n_asp, 50)    # [batch, asp, 50]\n",
    "        \n",
    "        result_senti = [ self.sentiments[aspi]( all_doc_emb[:,aspi,:] ) for aspi in range(0,self.n_asp)] # [batch, ra]\n",
    "        \n",
    "        result = torch.stack(result_senti, dim=1)  # [batch, asp, sentiment5]\n",
    "        \n",
    "        return result,raw_outputs,outputs,aspect_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_classifier(arch:Callable, vocab_sz:int, vocab, n_class:int, bptt:int=70, max_len:int=20*70, config:dict=None,\n",
    "                        drop_mult:float=1., lin_ftrs:Collection[int]=None, ps:Collection[float]=None,\n",
    "                        pad_idx:int=1) -> nn.Module:\n",
    "    \"Create a text classifier from `arch` and its `config`, maybe `pretrained`.\"\n",
    "    print(\"CUSTOM DEFINED CLASSIFIER\")\n",
    "    meta = text.learner._model_meta[arch]\n",
    "    config = ifnone(config, meta['config_clas']).copy()\n",
    "    for k in config.keys():\n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "    if lin_ftrs is None: lin_ftrs = [50]\n",
    "    if ps is None:  ps = [0.1]*len(lin_ftrs)\n",
    "    layers = [config[meta['hid_name']] * 3] + lin_ftrs + [n_class]\n",
    "    ps = [config.pop('output_p')] + ps\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    encoder = SentenceEncoder(bptt, max_len, arch(vocab_sz, **config), vocab, pad_idx=pad_idx)\n",
    "    cls_layer = Cls02ATT400(n_asp=hyper_params[\"num_aspect\"], n_rat=hyper_params[\"num_rating\"], layers=layers, drops=ps)\n",
    "    model = SequentialRNN(encoder, cls_layer)\n",
    "    return model if init is None else model.apply(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classifier_learner(data:DataBunch, arch:Callable, bptt:int=70, max_len:int=20*70, config:dict=None,\n",
    "                            pretrained:bool=True, drop_mult:float=1., lin_ftrs:Collection[int]=None,\n",
    "                            ps:Collection[float]=None, **learn_kwargs) -> 'TextClassifierLearner':\n",
    "    \"Create a `Learner` with a text classifier from `data` and `arch`.\"\n",
    "    model = get_text_classifier(arch, len(data.vocab.itos), data.vocab, data.c, bptt=bptt, max_len=max_len,\n",
    "                                config=config, drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps)\n",
    "    meta = text.learner._model_meta[arch]\n",
    "    learn = RNNLearner(data, model, split_func=meta['split_clas'], **learn_kwargs)\n",
    "    if pretrained:\n",
    "        if 'url' not in meta:\n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], data=False)\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn = learn.load_pretrained(*fnames, strict=False)\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelCEL(nn.CrossEntropyLoss):\n",
    "    def forward(self, input, target, nasp=5):\n",
    "        target = target.long()\n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(nasp):\n",
    "            loss = loss + super(MultiLabelCEL, self).forward(input[:,i,:], target[:,i])\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(preds, targs, nasp=hyper_params[\"num_aspect\"], nrat=5):\n",
    "    preds = preds[:,0:nasp,:]\n",
    "    preds = preds.contiguous().view(-1, nrat)\n",
    "    preds = torch.max(preds, dim=1)[1]\n",
    "    targs = targs.contiguous().view(-1).long()\n",
    "    return (preds==targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clas_acc(asp_index):\n",
    "    def asp_acc(preds, targs):\n",
    "        preds = torch.max(preds, dim=2)[1]\n",
    "        targs = targs.contiguous().long()\n",
    "        return (preds[:,asp_index]==targs[:,asp_index]).float().mean()\n",
    "    return asp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clas_mse(asp_index):\n",
    "    def asp_mse(preds, targs):\n",
    "        preds = torch.max(preds, dim=2)[1].float()[:,asp_index]\n",
    "        targs = targs.contiguous().float()[:,asp_index]\n",
    "        return torch.nn.functional.mse_loss(preds, targs)\n",
    "    return asp_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "macc=[get_clas_acc(ai) for ai in range(hyper_params[\"num_aspect\"])]\n",
    "for ai in range(hyper_params[\"num_aspect\"]): macc[ai].__name__ = \"clas_acc_\"+str(ai)\n",
    "mmse=[get_clas_mse(ai) for ai in range(hyper_params[\"num_aspect\"])]\n",
    "for ai in range(hyper_params[\"num_aspect\"]): mmse[ai].__name__ = \"clas_mse_\"+str(ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM DEFINED CLASSIFIER\n",
      "Encoder init\n",
      "CLS init\n",
      "Num Aspect: 5\n",
      "Num Rating: 5\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): Embedding(31600, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(31600, 400, padding_idx=1)\n",
      "  )\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Cls02ATT400(\n",
      "    (aspect): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
      "      (7): Softmax(dim=1)\n",
      "    )\n",
      "    (s0): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s1): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s3): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s4): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s5): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mloss = MultiLabelCEL()\n",
    "cls_learn = text_classifier_learner(cls_db, AWD_LSTM,\n",
    "                                    drop_mult=1.2,\n",
    "                                    loss_func=mloss,\n",
    "                                    metrics=[multi_acc]+macc+mmse,\n",
    "                                    bptt=70,\n",
    "                                    max_len=hyper_params[\"max_sequence_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks.tracker import TrackerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_validate(model:nn.Module, dl:DataLoader, loss_func:OptLossFunc=None, n_batch:Optional[int]=None)->Iterator[Tuple[Union[Tensor,int],...]]:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_losses,nums = [],[]\n",
    "\n",
    "        for xb,yb in dl:            \n",
    "            out = model(xb)[0]\n",
    "            val_loss = loss_func(out, yb)\n",
    "            val_loss = val_loss.detach().cpu()\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            if not is_listy(yb): yb = [yb]\n",
    "            nums.append(first_el(yb).shape[0])\n",
    "            if n_batch and (len(nums)>=n_batch): break\n",
    "            \n",
    "        nums = np.array(nums, dtype=np.float32)\n",
    "        return (to_np(torch.stack(val_losses)) * nums).sum() / nums.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestStepModel(TrackerCallback):\n",
    "    \"A `TrackerCallback` that saves the model when monitored quantity is best.\"\n",
    "    def __init__(self, learn:Learner, monitor:str='valid_loss', mode:str='auto', every:int=50, name:str='bestmodel'):\n",
    "        super().__init__(learn, monitor=monitor, mode=mode)\n",
    "        self.every, self.name = every, name\n",
    "        self.step = 0\n",
    "        self.records = []\n",
    "    \n",
    "    def on_train_begin(self, **kwargs:Any)->None:\n",
    "        super().on_train_begin(**kwargs)\n",
    "        self.step = 0\n",
    "        \n",
    "    def on_batch_end(self, **kwargs:Any)->None:\n",
    "        self.step += 1\n",
    "\n",
    "        if self.step % self.every == 0:\n",
    "            self.learn.model.eval()\n",
    "            current = fast_validate(self.learn.model, self.learn.data.valid_dl, self.learn.loss_func, n_batch=50)\n",
    "            self.learn.model.train()\n",
    "            \n",
    "            if isinstance(current, Tensor): current = current.cpu()\n",
    "            self.records.append(current)\n",
    "            \n",
    "            if current is not None and self.operator(current, self.best):\n",
    "                print(f'Better model found at step {self.step} with {self.monitor} value: {current}.')\n",
    "                self.best = current\n",
    "                self.learn.save(f'{self.name}')\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        pass\n",
    "#         \"Load the best model.\"\n",
    "#         if self.every==\"improvement\" and os.path.isfile(self.path/self.model_dir/f'{self.name}.pth'):\n",
    "#             self.learn.load(f'{self.name}', purge=False)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.callback_fns = [cls_learn.callback_fns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_learn.callback_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.callback_fns += [ partial(SaveBestStepModel, monitor=\"valid_loss\", mode=\"min\", every=100, name='beer.clas.attfullind400.best.learner') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREEZING\n",
      "Sequential(\n",
      "  (0): Embedding(31600, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(31600, 400, padding_idx=1)\n",
      "  )\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Cls02ATT400(\n",
      "    (aspect): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
      "      (7): Softmax(dim=1)\n",
      "    )\n",
      "    (s0): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s1): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s3): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s4): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s5): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "_ = cls_learn.load_encoder('lm_enc_beer.1115')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAS 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='6', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/6 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>clas_acc_0</th>\n",
       "      <th>clas_acc_1</th>\n",
       "      <th>clas_acc_2</th>\n",
       "      <th>clas_acc_3</th>\n",
       "      <th>clas_acc_4</th>\n",
       "      <th>clas_mse_0</th>\n",
       "      <th>clas_mse_1</th>\n",
       "      <th>clas_mse_2</th>\n",
       "      <th>clas_mse_3</th>\n",
       "      <th>clas_mse_4</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='880' class='' max='1997', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      44.07% [880/1997 01:39<02:06 5.1134]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at step 100 with valid_loss value: 5.984330654144287.\n",
      "Better model found at step 200 with valid_loss value: 5.288189888000488.\n",
      "Better model found at step 300 with valid_loss value: 4.8232245445251465.\n",
      "Better model found at step 400 with valid_loss value: 4.622148036956787.\n",
      "Better model found at step 500 with valid_loss value: 4.59514045715332.\n",
      "Better model found at step 800 with valid_loss value: 4.553602695465088.\n"
     ]
    }
   ],
   "source": [
    "# good loss\n",
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle( 6 , max_lr=slice(1e-3,2e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = cls_learn.recorder.plot_losses()\n",
    "experiment.log_figure(figure_name=\"train loss 01\", figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.save('beer.clas.attfullind400.1.learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SequentialRNN:\n\tUnexpected key(s) in state_dict: \"1.s6.0.weight\", \"1.s6.0.bias\", \"1.s6.0.running_mean\", \"1.s6.0.running_var\", \"1.s6.0.num_batches_tracked\", \"1.s6.2.weight\", \"1.s6.2.bias\", \"1.s6.4.weight\", \"1.s6.4.bias\", \"1.s6.4.running_mean\", \"1.s6.4.running_var\", \"1.s6.4.num_batches_tracked\", \"1.s6.6.weight\", \"1.s6.6.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3f8a214a4fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcls_learn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beer.clas.attfullind400.1.learner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Code/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, file, device, strict, with_opt, purge, remove_module)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremove_module\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_module_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SequentialRNN:\n\tUnexpected key(s) in state_dict: \"1.s6.0.weight\", \"1.s6.0.bias\", \"1.s6.0.running_mean\", \"1.s6.0.running_var\", \"1.s6.0.num_batches_tracked\", \"1.s6.2.weight\", \"1.s6.2.bias\", \"1.s6.4.weight\", \"1.s6.4.bias\", \"1.s6.4.running_mean\", \"1.s6.4.running_var\", \"1.s6.4.num_batches_tracked\", \"1.s6.6.weight\", \"1.s6.6.bias\". "
     ]
    }
   ],
   "source": [
    "cls_learn.load('beer.clas.attfullind400.1.learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Embedding(31600, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(31600, 400, padding_idx=1)\n",
      "  )\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Cls02ATT400(\n",
      "    (aspect): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
      "      (7): Softmax(dim=1)\n",
      "    )\n",
      "    (s0): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s1): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s3): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s4): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s5): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "    (s6): Sequential(\n",
      "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cls_learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>clas_acc_0</th>\n",
       "      <th>clas_acc_1</th>\n",
       "      <th>clas_acc_2</th>\n",
       "      <th>clas_acc_3</th>\n",
       "      <th>clas_acc_4</th>\n",
       "      <th>clas_mse_0</th>\n",
       "      <th>clas_mse_1</th>\n",
       "      <th>clas_mse_2</th>\n",
       "      <th>clas_mse_3</th>\n",
       "      <th>clas_mse_4</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.738288</td>\n",
       "      <td>4.251144</td>\n",
       "      <td>0.611960</td>\n",
       "      <td>0.608086</td>\n",
       "      <td>0.594163</td>\n",
       "      <td>0.648875</td>\n",
       "      <td>0.603624</td>\n",
       "      <td>0.605052</td>\n",
       "      <td>0.493752</td>\n",
       "      <td>0.456801</td>\n",
       "      <td>0.437701</td>\n",
       "      <td>0.465816</td>\n",
       "      <td>0.478133</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.562331</td>\n",
       "      <td>4.173386</td>\n",
       "      <td>0.619475</td>\n",
       "      <td>0.611746</td>\n",
       "      <td>0.603088</td>\n",
       "      <td>0.652624</td>\n",
       "      <td>0.613263</td>\n",
       "      <td>0.616655</td>\n",
       "      <td>0.497412</td>\n",
       "      <td>0.447965</td>\n",
       "      <td>0.422171</td>\n",
       "      <td>0.461264</td>\n",
       "      <td>0.459032</td>\n",
       "      <td>04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.515416</td>\n",
       "      <td>4.101168</td>\n",
       "      <td>0.624813</td>\n",
       "      <td>0.620582</td>\n",
       "      <td>0.597911</td>\n",
       "      <td>0.659407</td>\n",
       "      <td>0.625402</td>\n",
       "      <td>0.620760</td>\n",
       "      <td>0.478311</td>\n",
       "      <td>0.476794</td>\n",
       "      <td>0.407890</td>\n",
       "      <td>0.445823</td>\n",
       "      <td>0.458943</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.446592</td>\n",
       "      <td>4.060063</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.625312</td>\n",
       "      <td>0.609693</td>\n",
       "      <td>0.664941</td>\n",
       "      <td>0.626473</td>\n",
       "      <td>0.625580</td>\n",
       "      <td>0.475634</td>\n",
       "      <td>0.441449</td>\n",
       "      <td>0.417351</td>\n",
       "      <td>0.441449</td>\n",
       "      <td>0.440378</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.402438</td>\n",
       "      <td>4.020736</td>\n",
       "      <td>0.632988</td>\n",
       "      <td>0.624420</td>\n",
       "      <td>0.611924</td>\n",
       "      <td>0.668690</td>\n",
       "      <td>0.628972</td>\n",
       "      <td>0.630935</td>\n",
       "      <td>0.475991</td>\n",
       "      <td>0.456712</td>\n",
       "      <td>0.406462</td>\n",
       "      <td>0.434577</td>\n",
       "      <td>0.444930</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.302449</td>\n",
       "      <td>3.982461</td>\n",
       "      <td>0.637790</td>\n",
       "      <td>0.634416</td>\n",
       "      <td>0.615494</td>\n",
       "      <td>0.673152</td>\n",
       "      <td>0.635398</td>\n",
       "      <td>0.630489</td>\n",
       "      <td>0.447162</td>\n",
       "      <td>0.433417</td>\n",
       "      <td>0.384059</td>\n",
       "      <td>0.418154</td>\n",
       "      <td>0.448768</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.176979</td>\n",
       "      <td>3.955843</td>\n",
       "      <td>0.640682</td>\n",
       "      <td>0.636201</td>\n",
       "      <td>0.614780</td>\n",
       "      <td>0.676990</td>\n",
       "      <td>0.635844</td>\n",
       "      <td>0.639593</td>\n",
       "      <td>0.442342</td>\n",
       "      <td>0.443502</td>\n",
       "      <td>0.378615</td>\n",
       "      <td>0.424134</td>\n",
       "      <td>0.416905</td>\n",
       "      <td>04:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.076322</td>\n",
       "      <td>3.947469</td>\n",
       "      <td>0.642681</td>\n",
       "      <td>0.635577</td>\n",
       "      <td>0.617280</td>\n",
       "      <td>0.677347</td>\n",
       "      <td>0.642806</td>\n",
       "      <td>0.640396</td>\n",
       "      <td>0.460371</td>\n",
       "      <td>0.438772</td>\n",
       "      <td>0.384595</td>\n",
       "      <td>0.412710</td>\n",
       "      <td>0.425473</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.077200</td>\n",
       "      <td>3.937829</td>\n",
       "      <td>0.642199</td>\n",
       "      <td>0.634149</td>\n",
       "      <td>0.618261</td>\n",
       "      <td>0.678240</td>\n",
       "      <td>0.639057</td>\n",
       "      <td>0.641289</td>\n",
       "      <td>0.453409</td>\n",
       "      <td>0.444573</td>\n",
       "      <td>0.373438</td>\n",
       "      <td>0.414227</td>\n",
       "      <td>0.417619</td>\n",
       "      <td>04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.976473</td>\n",
       "      <td>3.949420</td>\n",
       "      <td>0.642502</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.619868</td>\n",
       "      <td>0.673688</td>\n",
       "      <td>0.641824</td>\n",
       "      <td>0.640486</td>\n",
       "      <td>0.456623</td>\n",
       "      <td>0.438950</td>\n",
       "      <td>0.389772</td>\n",
       "      <td>0.428418</td>\n",
       "      <td>0.417886</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.045985</td>\n",
       "      <td>3.928784</td>\n",
       "      <td>0.645038</td>\n",
       "      <td>0.644145</td>\n",
       "      <td>0.615048</td>\n",
       "      <td>0.681096</td>\n",
       "      <td>0.642360</td>\n",
       "      <td>0.642538</td>\n",
       "      <td>0.435112</td>\n",
       "      <td>0.443413</td>\n",
       "      <td>0.372278</td>\n",
       "      <td>0.407979</td>\n",
       "      <td>0.418958</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.996596</td>\n",
       "      <td>3.918359</td>\n",
       "      <td>0.646002</td>\n",
       "      <td>0.642985</td>\n",
       "      <td>0.619779</td>\n",
       "      <td>0.678954</td>\n",
       "      <td>0.643966</td>\n",
       "      <td>0.644323</td>\n",
       "      <td>0.437522</td>\n",
       "      <td>0.428775</td>\n",
       "      <td>0.381382</td>\n",
       "      <td>0.408693</td>\n",
       "      <td>0.424670</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.010465</td>\n",
       "      <td>3.928847</td>\n",
       "      <td>0.647340</td>\n",
       "      <td>0.642895</td>\n",
       "      <td>0.620850</td>\n",
       "      <td>0.685291</td>\n",
       "      <td>0.642806</td>\n",
       "      <td>0.644859</td>\n",
       "      <td>0.433327</td>\n",
       "      <td>0.430382</td>\n",
       "      <td>0.369154</td>\n",
       "      <td>0.411996</td>\n",
       "      <td>0.421010</td>\n",
       "      <td>04:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.929832</td>\n",
       "      <td>3.917945</td>\n",
       "      <td>0.647161</td>\n",
       "      <td>0.643877</td>\n",
       "      <td>0.621742</td>\n",
       "      <td>0.681989</td>\n",
       "      <td>0.643966</td>\n",
       "      <td>0.644234</td>\n",
       "      <td>0.435559</td>\n",
       "      <td>0.429489</td>\n",
       "      <td>0.374063</td>\n",
       "      <td>0.411460</td>\n",
       "      <td>0.427169</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.917912</td>\n",
       "      <td>3.934230</td>\n",
       "      <td>0.644198</td>\n",
       "      <td>0.641646</td>\n",
       "      <td>0.619154</td>\n",
       "      <td>0.676276</td>\n",
       "      <td>0.642806</td>\n",
       "      <td>0.641110</td>\n",
       "      <td>0.445734</td>\n",
       "      <td>0.439843</td>\n",
       "      <td>0.387808</td>\n",
       "      <td>0.416905</td>\n",
       "      <td>0.427080</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FULL INDIPENDENT\n",
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4' class='' max='18', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      22.22% [4/18 27:19<1:35:38]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>clas_acc_0</th>\n",
       "      <th>clas_acc_1</th>\n",
       "      <th>clas_acc_2</th>\n",
       "      <th>clas_acc_3</th>\n",
       "      <th>clas_acc_4</th>\n",
       "      <th>clas_mse_0</th>\n",
       "      <th>clas_mse_1</th>\n",
       "      <th>clas_mse_2</th>\n",
       "      <th>clas_mse_3</th>\n",
       "      <th>clas_mse_4</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.906426</td>\n",
       "      <td>4.866243</td>\n",
       "      <td>0.545073</td>\n",
       "      <td>0.537040</td>\n",
       "      <td>0.533113</td>\n",
       "      <td>0.572206</td>\n",
       "      <td>0.540878</td>\n",
       "      <td>0.542128</td>\n",
       "      <td>0.659407</td>\n",
       "      <td>0.559711</td>\n",
       "      <td>0.605141</td>\n",
       "      <td>0.578722</td>\n",
       "      <td>0.604070</td>\n",
       "      <td>06:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.844094</td>\n",
       "      <td>4.724708</td>\n",
       "      <td>0.562370</td>\n",
       "      <td>0.559622</td>\n",
       "      <td>0.546323</td>\n",
       "      <td>0.591128</td>\n",
       "      <td>0.560603</td>\n",
       "      <td>0.554177</td>\n",
       "      <td>0.610586</td>\n",
       "      <td>0.548019</td>\n",
       "      <td>0.570154</td>\n",
       "      <td>0.563013</td>\n",
       "      <td>0.594520</td>\n",
       "      <td>06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.703526</td>\n",
       "      <td>4.702122</td>\n",
       "      <td>0.563245</td>\n",
       "      <td>0.555694</td>\n",
       "      <td>0.547751</td>\n",
       "      <td>0.591128</td>\n",
       "      <td>0.567030</td>\n",
       "      <td>0.554623</td>\n",
       "      <td>0.647447</td>\n",
       "      <td>0.558372</td>\n",
       "      <td>0.573277</td>\n",
       "      <td>0.554088</td>\n",
       "      <td>0.605230</td>\n",
       "      <td>06:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.599808</td>\n",
       "      <td>4.521493</td>\n",
       "      <td>0.583077</td>\n",
       "      <td>0.576312</td>\n",
       "      <td>0.565423</td>\n",
       "      <td>0.612281</td>\n",
       "      <td>0.587558</td>\n",
       "      <td>0.573813</td>\n",
       "      <td>0.591307</td>\n",
       "      <td>0.538558</td>\n",
       "      <td>0.527401</td>\n",
       "      <td>0.514548</td>\n",
       "      <td>0.546323</td>\n",
       "      <td>07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='393' class='' max='1997', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      19.68% [393/1997 01:06<04:33 4.6424]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at step 100 with valid_loss value: 4.400759696960449.\n",
      "Better model found at step 300 with valid_loss value: 4.392011642456055.\n",
      "Better model found at step 600 with valid_loss value: 4.3827714920043945.\n",
      "Better model found at step 700 with valid_loss value: 4.344567775726318.\n",
      "Better model found at step 1500 with valid_loss value: 4.328266143798828.\n",
      "Better model found at step 2100 with valid_loss value: 4.293142795562744.\n",
      "Better model found at step 3100 with valid_loss value: 4.2860188484191895.\n",
      "Better model found at step 3300 with valid_loss value: 4.259410381317139.\n",
      "Better model found at step 3500 with valid_loss value: 4.2545857429504395.\n",
      "Better model found at step 4800 with valid_loss value: 4.232550144195557.\n",
      "Better model found at step 4900 with valid_loss value: 4.215993404388428.\n",
      "Better model found at step 5400 with valid_loss value: 4.214898109436035.\n",
      "Better model found at step 5500 with valid_loss value: 4.202560901641846.\n",
      "Better model found at step 5700 with valid_loss value: 4.199269771575928.\n",
      "Better model found at step 5800 with valid_loss value: 4.188429355621338.\n",
      "Better model found at step 6300 with valid_loss value: 4.177000045776367.\n",
      "Better model found at step 6500 with valid_loss value: 4.1564836502075195.\n",
      "Better model found at step 6800 with valid_loss value: 4.1370038986206055.\n",
      "Better model found at step 7100 with valid_loss value: 4.136020660400391.\n",
      "Better model found at step 7600 with valid_loss value: 4.127263069152832.\n",
      "Better model found at step 7700 with valid_loss value: 4.125432968139648.\n",
      "Better model found at step 7900 with valid_loss value: 4.117752552032471.\n",
      "Better model found at step 8000 with valid_loss value: 4.117354869842529.\n",
      "Better model found at step 8100 with valid_loss value: 4.1130547523498535.\n",
      "Better model found at step 8400 with valid_loss value: 4.101819038391113.\n",
      "Better model found at step 8500 with valid_loss value: 4.077582836151123.\n",
      "Better model found at step 9100 with valid_loss value: 4.073221206665039.\n",
      "Better model found at step 9200 with valid_loss value: 4.066157341003418.\n",
      "Better model found at step 9300 with valid_loss value: 4.056008815765381.\n"
     ]
    }
   ],
   "source": [
    "# FULL INDIPENDENT, 1.2 DROPOUT\n",
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web': 'https://www.comet.ml/api/image/download?imageId=566a53d3bfd947e8947da80251ae9f78&experimentKey=3cbb025fbcf1445990d612bcae8d241f',\n",
       " 'api': 'https://www.comet.ml/api/rest/v1/image/get-image?imageId=566a53d3bfd947e8947da80251ae9f78&experimentKey=3cbb025fbcf1445990d612bcae8d241f',\n",
       " 'imageId': '566a53d3bfd947e8947da80251ae9f78'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = cls_learn.recorder.plot_losses()\n",
    "experiment.log_figure(figure_name=\"train loss 02\", figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.save('beer.clas.attfullind400.2.learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (63909 items)\n",
       "x: TextList\n",
       "xxbos a lot of foam xxperiod xxmaj but a lot xxperiod xxmaj in the smell some banana , and then lactic and tart xxperiod xxmaj not a good start xxperiod xxmaj quite dark orange in color , with a lively carbonation ( now visible , under the foam ) xxperiod xxmaj again tending to lactic sourness xxperiod xxmaj same for the taste xxperiod xxmaj with some yeast and banana xxperiod,xxbos xxmaj almost totally black xxperiod xxmaj beige foam , quite compact , not bad xxperiod xxmaj light smell , just a bit of roast , and some hop xxperiod a bit too light xxperiod xxmaj the taste is light oo , and drinkable , with some malt , roast , hints of coffee xxperiod xxmaj nothing exceptional , but after all drinkable and pleasant xxperiod xxmaj light to average body xxperiod xxmaj in the aftertaste some dust , xxunk roast , hint of caramel , and a bit of bitterness xxperiod xxmaj no defect , drinkable , not bad xxperiod,xxbos xxmaj golden yellow color xxperiod xxmaj white , compact foam , quite creamy xxperiod xxmaj good appearance xxperiod xxmaj fresh smell , with good hop xxperiod xxmaj quite dry , with a good grassy note xxperiod xxmaj hay xxperiod xxmaj fresh and pleasant xxperiod xxmaj more sweet in the mouth , with honey xxperiod xxmaj the hop comes back in the end , and in the aftertaste xxperiod xxmaj not bad , but a bit too sweet for a pils xxperiod xxmaj in the end some vanilla and camomile note xxperiod xxmaj in the aftertaste , too xxperiod xxmaj though the hop , a bit too sweet xxperiod xxmaj honest xxperiod,xxbos 22 oz bottle from \" xxmaj lifesource \" xxmaj salem xxperiod $ 3 xxperiod 95 xxmaj nice golden clear beer body with a nice sized frothy / creamy white head xxperiod xxmaj ok aromas xxperiod mainlly a bit of ginger xxunk and some bready malt xxperiod simple nice xxmaj taste very nice indeed xxperiod nice spicy ginger backed with slightly caramel maltiness xxperiod simple again but i like xxperiod xxmaj liked the mouthfeel of this one xxperiod very forward carbonation which helps the ginger effect and a lingering ginger in the after taste xxperiod xxmaj overall a simple ginger brew xxperiod i liked it xxperiod,xxbos xxmaj bottle says \" xxmaj malt beverage brewed with xxmaj ginger and ginger added \" xxmaj sounds redundant to me , but lets move on xxperiod xxmaj pours a bud light yellow with a tiny white head of small bubbles xxperiod xxmaj the beer is almost as clear as a glass of water with some food coloring in it xxperiod xxmaj aroma of light ginger , a very light malt aroma but primarily odorless on the malt side xxperiod i would n't be completely surprised if there were some adjuncts in here because of the lack of underlying malt flavors xxperiod xxmaj taste is of a light adjunct lager with a dosing of ginger xxperiod xxmaj not surprising there xxperiod xxmaj this is a light session beer , good for the warmer days of spring / summer xxperiod xxmaj mouthfeel is extremely light , high carbonation xxperiod xxmaj overall decent xxperiod xxmaj this would be great if you were drinking beers on draft at the bar with some friends just hanging out xxperiod i would n't necessarily seek it out though to drink out of a bottle xxperiod\n",
       "y: MultiCategoryList\n",
       "4,,,,3\n",
       "Path: data;\n",
       "\n",
       "Valid: LabelList (11204 items)\n",
       "x: TextList\n",
       "xxbos xxmaj dark red color , light beige foam , average xxperiod xxmaj in the smell malt and caramel , not really light xxperiod xxmaj again malt and caramel in the taste , not bad in the end xxperiod xxmaj maybe a note of honey in teh back , and a light fruitiness xxperiod xxmaj average body xxperiod xxmaj in the aftertaste a light bitterness , with the malt and red fruit xxperiod xxmaj nothing exceptional , but not bad , drinkable beer xxperiod,xxbos xxmaj poured from a 22 oz bomber into my xxmaj drie xxmaj fonteinen tumbler xxperiod xxmaj hazy xxunk yellow body ( which catches the shadows forming a beautiful mysterious gradient ) with an incredibly dense pillow of magnolia cream xxperiod xxmaj heavy persistent head and rich creamy lacing xxperiod xxmaj pale malt , asian pear , and a hint of citrus in the nose xxperiod a vaguely tropical lager xxperiod xxmaj tastes very much like a well done xxup apa , with a nice balance of pale malt and low hop bitterness xxperiod xxmaj the ginger adds to the refreshing character , but is n't readily detectable at first ( lacks any \" bite \" ) xxperiod xxmaj medium - dry finish - very clean and extremely quaffable xxperiod i can imagine hibiscus and beets working in small quantities , though i think they omitted those for this version xxperiod xxmaj light bodied , pillowy , smooth and moderately carbonated xxperiod xxmaj do n't go into this expecting a ginger beer ( despite its name ) as it has little in common with that spicy soft drink xxperiod xxmaj this is a wonderful session ale though , and worth seeking out if you are a fan of light yet flavorful lagers xxperiod xxmaj would obviously go perfectly with sushi xxperiod,xxbos xxmaj more of a ' dry ' than a lager , tasted at the 2002 xxmaj oregon xxmaj brewers xxmaj festival xxperiod xxmaj orange color , orange flavor in nose xxperiod xxmaj light malts and fairly aggressively hopped xxperiod yet it is not very bitter xxperiod xxmaj interesting taste , complex and subtle xxperiod xxmaj light yet flavorful xxperiod xxmaj mouthfeel is full and round xxperiod xxmaj finish is clean and smooth xxperiod xxmaj aftertaste is slightly bitter xxperiod xxmaj nice beer xxperiod xxmaj would be a great beer to sip during a hot summer day xxperiod,xxbos xxmaj pours a rich burnt caramel hue with some deep amber hues xxperiod dark for a rauchbier xxperiod a surprisingly dark tan head slowly fades and leaves ample lacing xxperiod aroma is pungently smokey xxperiod sweet malt , sweet smoke , and some salted caramel in the nose xxperiod the taste , as expected , is smokey xxperiod xxmaj not the smokiest or most pungent smoke profile i 've experienced , but damn up there xxperiod and i think it 's the intense rich malt and sweetness from the malt that keeps the smoke slightly tamed xxperiod a touch of lemon and citrus fruitiness in the finish which blends perfectly with the cherry smoke xxperiod xxmaj it 's not as crisp or dry as other xxmaj rachbiers , and it seems a bit more full - bodied , with almost a touch of syrupiness to it xxperiod which is kind of an interesting take on it xxperiod i 'm digging it xxperiod i 'm not usually one to say this , but it 's almost a touch malt - forward xxperiod xxmaj not that it 's a bad thing xxperiod i think this is probably my favorite domestically produced xxmaj rauchbier i 've had the pleasure of tasting xxperiod xxmaj it really hits on all cylinders xxperiod a touch sweet and malt forward , but all in all , it does what it 's going for and works beautifully xxperiod xxmaj definitely worth seeking out xxperiod,xxbos xxmaj got this in a 22 ounce bomber xxperiod xxmaj it poured a deep bright brown with a pretty small head xxperiod xxmaj aromas of smoked xxunk , and dark roasted malts xxperiod xxmaj this beer was xxup ok but for a style that i like so much it did'nt really please as much as i thought it would xxperiod xxmaj it had a nice amount of smokiness to it xxperiod xxmaj really obvious and pretty strong but not charred xxperiod xxmaj but the oily consistency of the beer along with the huge saltiness made the beer less than i was hoping for xxperiod xxmaj it was still a beer that i 'm glad i grabbed xxperiod xxmaj they do n't make many in the style anyways xxperiod\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: data;\n",
       "\n",
       "Test: LabelList (24884 items)\n",
       "x: TextList\n",
       "xxbos xxmaj according to the website , the style for the xxmaj caldera xxmaj cauldron changes every year xxunk xxperiod xxmaj the current release is a xxup dipa , which frankly is the only cauldron i 'm familiar with ( it was an xxup ipa / xxup dipa the last time i ordered a cauldron at the xxunk several years back ) xxunk xxperiod xxmaj in any event xxunk xxperiod at the xxmaj horse xxmaj brass yesterday xxunk xxperiod xxmaj the beer pours an orange copper color with good head retention and lacing xxunk xxperiod xxmaj the nose is all hoppy xxup ipa goodness , showcasing a huge aroma of dry citrus , pine and xxunk xxunk xxperiod xxmaj the flavor profile replicates the nose pretty closely in this xxmaj west xxmaj coast all the way xxup dipa xxunk xxperiod xxmaj this xxup dipa is not for the faint of heart and is a bit much even for a hophead like xxunk xxunk xxperiod xxmaj the finish is quite dry and hoppy , and there 's barely enough sweet malt to balance and hold up the avalanche of hoppy bitterness in this beer xxunk xxperiod xxmaj mouthfeel is actually fairly light , with a long , xxunk bitter finish xxunk xxperiod xxmaj drinkability is good , with the alcohol barely noticeable in this well crafted beer xxunk xxperiod xxmaj still , this beer is so hugely hoppy / bitter , it 's really hard for me to imagine ordering more than a single glass xxunk xxperiod xxmaj regardless , this is a very impressive beer from the folks at xxmaj caldera xxunk xxperiod,xxbos xxmaj poured from the bottle into a xxmaj chimay goblet xxunk xxperiod xxmaj appearance : xxmaj pours a slightly cloudy yellow / orange color with a half finger of fluffy white head xxunk xxperiod xxmaj the head fades to a small layer on top of the pour xxunk xxperiod xxmaj smell : xxmaj very light and crisp xxunk xxperiod i 'm definitely picking up the ginger , but it 's not overly powerful xxunk xxperiod xxmaj there is a slight sweetness from the malt as well xxunk xxperiod xxmaj taste : xxmaj very light and refreshing xxunk xxperiod xxmaj the ginger shows up right away and then fades towards the finish of the sip xxunk xxperiod xxmaj the finish is malty and bread like xxunk xxperiod xxmaj mouthfeel : xxmaj the body is on the thin side with smooth carbonation and a very dry finish xxunk xxperiod xxmaj overall : xxmaj this is a light and refreshing beer , but nothing spectacular xxunk xxperiod xxmaj the amount of ginger is nice , but i would have liked to have more going on xxunk xxperiod,xxbos xxmaj notes from 6 / 24 xxunk xxperiod a : xxmaj bright golden glowing beer in a moment of clarity with a lively white head of feathery fluff xxunk xxperiod s : xxmaj the ginger is definitely there , or am i smelling my xxmaj indian dinner ? xxmaj almost cake - like in its malt aroma , sharply bready with a slight edge of sweetness xxunk xxperiod t : xxmaj nice clear malty throat taste , reminds me of strands of complex sugars and grains , ginger is more subtle than i expected , more of an undertone than a backbone xxunk xxperiod m : a refreshing light beer feel , like a pilsner or summer ale xxunk xxperiod d : xxmaj if this was a sixer instead of a double - deuce , i could see this being a fine picnic pounder xxunk xxperiod xxmaj overall , pretty impressed for the particular style xxunk xxperiod,xxbos 22 oz xxunk xxperiod bomber , xxunk xxperiod a : xxmaj pours a clear yellow with a mild white head , good retention xxunk xxperiod s : xxmaj great nose of ginger , honey , perfume xxunk xxperiod t : xxmaj rather light upfront , it reminds me of xxmaj lawnmower , with a hint of ale fruitiness / xxmaj kolsch - like almost xxunk xxperiod xxmaj good ginger honey notes on the back end , not taking over the beer in any way xxunk xxperiod xxmaj the ginger flavour is clear , but i wanted it to come out a little more in the end xxunk xxperiod m : xxmaj very light - bodied , watery , light base beer for sure xxunk xxperiod d : xxmaj an easy drinking spiced beer , this will offend no one , but there 's not a complexity to this brew at all xxunk xxperiod,xxbos xxmaj brown in color , somewhere between a porter and a brown ale xxunk xxperiod xxmaj lacking in aroma , but no off stuff xxunk xxperiod xxmaj same with the taste , lacking flavor , complexity , just went with smoothness xxunk xxperiod xxmaj no off flavors though , so i ca n't say this is bad , just xxunk , especially for xxmaj caldera , whom i think is generally underrated xxunk xxperiod xxmaj you really have to search to pull anything out of this in terms of the usual chocolate / coffee flavors , really , the only thing i can tell is that the oats did their job , because this is smooth and unoffensive xxunk xxperiod xxmaj other than that , extremely pedestrian xxunk xxperiod\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: data, model=SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(31600, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(31600, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Cls02ATT400(\n",
       "    (aspect): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "      (7): Softmax(dim=1)\n",
       "    )\n",
       "    (s0): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s1): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s2): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s3): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s4): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s5): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s6): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=MultiLabelCEL(), metrics=[<function multi_acc at 0x7f3e1e874d40>, <function get_clas_acc.<locals>.asp_acc at 0x7f3e1e8880e0>, <function get_clas_acc.<locals>.asp_acc at 0x7f3e1e888170>, <function get_clas_acc.<locals>.asp_acc at 0x7f3e1e888200>, <function get_clas_acc.<locals>.asp_acc at 0x7f3e1e888290>, <function get_clas_acc.<locals>.asp_acc at 0x7f3e1e888320>, <function get_clas_mse.<locals>.asp_mse at 0x7f3e1e8883b0>, <function get_clas_mse.<locals>.asp_mse at 0x7f3e1e888440>, <function get_clas_mse.<locals>.asp_mse at 0x7f3e1e8884d0>, <function get_clas_mse.<locals>.asp_mse at 0x7f3e1e888560>, <function get_clas_mse.<locals>.asp_mse at 0x7f3e1e8885f0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(31600, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(31600, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Cls02ATT400(\n",
       "    (aspect): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "      (7): Softmax(dim=1)\n",
       "    )\n",
       "    (s0): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s1): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s2): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s3): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s4): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s5): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "    (s6): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_learn.load('beer.clas.attfullind400.2.learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINISH EXPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/aeryen/2019nn-beer/ba8df291ce4b4b7987213dd7d905cc85\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     sys.cpu.percent.01 [1315]       : (0.0, 15.4)\n",
      "COMET INFO:     sys.cpu.percent.02 [1315]       : (0.0, 15.2)\n",
      "COMET INFO:     sys.cpu.percent.03 [1315]       : (0.0, 16.1)\n",
      "COMET INFO:     sys.cpu.percent.04 [1315]       : (0.0, 17.0)\n",
      "COMET INFO:     sys.cpu.percent.05 [1315]       : (0.0, 16.0)\n",
      "COMET INFO:     sys.cpu.percent.06 [1315]       : (0.0, 15.8)\n",
      "COMET INFO:     sys.cpu.percent.07 [1315]       : (0.0, 16.3)\n",
      "COMET INFO:     sys.cpu.percent.08 [1315]       : (0.0, 16.0)\n",
      "COMET INFO:     sys.cpu.percent.09 [1315]       : (0.0, 15.5)\n",
      "COMET INFO:     sys.cpu.percent.10 [1315]       : (0.0, 18.6)\n",
      "COMET INFO:     sys.cpu.percent.11 [1315]       : (0.1, 21.0)\n",
      "COMET INFO:     sys.cpu.percent.12 [1315]       : (0.0, 15.1)\n",
      "COMET INFO:     sys.cpu.percent.avg [1315]      : (0.09999999999999999, 16.183333333333334)\n",
      "COMET INFO:     sys.gpu.0.free_memory [1491]    : (21342912512.0, 24967774208.0)\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization [1491]: (0.0, 6.0)\n",
      "COMET INFO:     sys.gpu.0.total_memory          : (25373310976.0, 25373310976.0)\n",
      "COMET INFO:     sys.gpu.0.used_memory [1491]    : (405536768.0, 4030398464.0)\n",
      "COMET INFO:     sys.load.avg [1315]             : (0.0, 2.14)\n",
      "COMET INFO:     sys.ram.total [1315]            : (16703754240.0, 16703754240.0)\n",
      "COMET INFO:     sys.ram.used [1315]             : (1842450432.0, 4560240640.0)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_num_file = [\"train.count\", \"test.count\"]\n",
    "rating_file = [\"train.rating\", \"test.rating\"]\n",
    "content_file = [\"train.txt\", \"test.txt\"]\n",
    "\n",
    "dataset_dir = \"./data/beer_100k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_to_doc(sent_list, sent_count):\n",
    "    start_index = 0\n",
    "    docs = []\n",
    "    for s in sent_count:\n",
    "#         doc = \" xxPERIOD \".join(sent_list[start_index:start_index + s])\n",
    "#         doc = doc + \" xxPERIOD \"\n",
    "        docs.append(sent_list[start_index:start_index + s])\n",
    "        start_index = start_index + s\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATA = 0\n",
    "TEST_DATA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 12, 7, 9, 6]\n",
      "   0  1  2  3  4\n",
      "0  3  3  4  3  4\n",
      "1  2  2  2  2  2\n",
      "2  3  3  3  2  3\n",
      "3  3  2  2  2  4\n",
      "4  2  2  2  2  2\n",
      "['According to the website, the style for the Caldera Cauldron changes every year', \"The current release is a DIPA, which frankly is the only cauldron I'm familiar with (it was an IPA/DIPA the last time I ordered a cauldron at the horsebrass several years back)\", 'In any event', 'at the Horse Brass yesterday', 'The beer pours an orange copper color with good head retention and lacing']\n"
     ]
    }
   ],
   "source": [
    "# # Load Count\n",
    "sent_count_test = list(open(dataset_dir + sent_num_file[TEST_DATA], \"r\").readlines())\n",
    "sent_count_test = [int(s) for s in sent_count_test if (len(s) > 0 and s != \"\\n\")]\n",
    "print( sent_count_test[0:5] )\n",
    "\n",
    "# Load Ratings\n",
    "aspect_rating_test = list(open(dataset_dir + rating_file[TEST_DATA], \"r\").readlines())\n",
    "aspect_rating_test = [s for s in aspect_rating_test if (len(s) > 0 and s != \"\\n\")]\n",
    "\n",
    "aspect_rating_test = [s.split(\" \") for s in aspect_rating_test]\n",
    "aspect_rating_test = np.array(aspect_rating_test)[:, :]\n",
    "aspect_rating_test = aspect_rating_test.astype(np.float) - 1\n",
    "aspect_rating_test = np.rint(aspect_rating_test).astype(int)  # ROUND TO INTEGER =================\n",
    "aspect_rating_test = pd.DataFrame(aspect_rating_test)\n",
    "print( aspect_rating_test.head() )\n",
    "\n",
    "# Load Sents\n",
    "sents_test = list(open(dataset_dir + content_file[TEST_DATA], \"r\").readlines())\n",
    "sents_test = [s.strip() for s in sents_test]\n",
    "sents_test = [s[:-1] for s in sents_test if s.endswith(\".\")]\n",
    "print( sents_test[0:5] )\n",
    "\n",
    "# Sents to Doc\n",
    "docs_test = concat_to_doc(sents_test, sent_count_test)\n",
    "docs_test = pd.DataFrame({doc:docs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[According to the website, the style for the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[Poured from the bottle into a Chimay goblet, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[Notes from 6/24, A: Bright golden glowing bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[22 oz, bomber,, A: Pours a clear yellow with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[Brown in color, somewhere between a porter an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4                                                  5\n",
       "0  3  3  4  3  4  [According to the website, the style for the C...\n",
       "1  2  2  2  2  2  [Poured from the bottle into a Chimay goblet, ...\n",
       "2  3  3  3  2  3  [Notes from 6/24, A: Bright golden glowing bee...\n",
       "3  3  2  2  2  4  [22 oz, bomber,, A: Pours a clear yellow with ...\n",
       "4  2  2  2  2  2  [Brown in color, somewhere between a porter an..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.concat( [aspect_rating_test, docs_test], axis=1, ignore_index=True )\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clas_acc(asp_index):\n",
    "    def asp_acc(preds, targs):\n",
    "        preds = torch.max(preds, dim=2)[1]\n",
    "        targs = targs.contiguous().long()\n",
    "        return (preds[:,asp_index]==targs[:,asp_index]).float().mean()\n",
    "    return asp_acc\n",
    "def get_clas_mse(asp_index):\n",
    "    def asp_mse(preds, targs):\n",
    "        preds = torch.max(preds, dim=2)[1].float()[:,asp_index]\n",
    "        targs = targs.contiguous().float()[:,asp_index]\n",
    "        return torch.nn.functional.mse_loss(preds, targs)\n",
    "    return asp_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(self,\n",
    "              ds_type:DatasetType,\n",
    "              activ:nn.Module=None,\n",
    "              with_loss:bool=False,\n",
    "              n_batch:Optional[int]=None,\n",
    "              pbar:Optional[PBar]=None,\n",
    "              ordered:bool=False) -> List[Tensor]:\n",
    "    \"Return predictions and targets on the valid, train, or test set, depending on `ds_type`.\"\n",
    "    self.model.reset()\n",
    "    if ordered: np.random.seed(42)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outs = []\n",
    "        asps = []\n",
    "        for xb,yb in progress_bar(cls_learn.dl(ds_type)):\n",
    "            out,raw_enc,enc,asp = cls_learn.model(xb)\n",
    "            outs.append(out)\n",
    "            for doc in asp:\n",
    "                asps.append( to_float(doc.cpu()))\n",
    "\n",
    "    outs = to_float(torch.cat(outs).cpu())\n",
    "    \n",
    "    if ordered and hasattr(self.dl(ds_type), 'sampler'):\n",
    "        np.random.seed(42)\n",
    "        sampler = [i for i in self.dl(ds_type).sampler]\n",
    "        reverse_sampler = np.argsort(sampler)\n",
    "        \n",
    "        outs = outs[reverse_sampler]\n",
    "        asps = [asps[i] for i in reverse_sampler]\n",
    "    return (outs,asps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='778' class='' max='778', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [778/778 00:33<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outs,asps = get_preds(self=cls_learn, ds_type=DatasetType.Test, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24884, 6, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 4, 3, 4],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 2, 3],\n",
       "        ...,\n",
       "        [4, 2, 4, 3, 4],\n",
       "        [2, 2, 1, 0, 3],\n",
       "        [3, 2, 4, 2, 4]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor( aspect_rating_test.values )\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3240)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mloss = MultiLabelCEL()\n",
    "mloss.forward(outs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASP0</th>\n",
       "      <th>ASP1</th>\n",
       "      <th>ASP2</th>\n",
       "      <th>ASP3</th>\n",
       "      <th>ASP4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.598738</td>\n",
       "      <td>0.591786</td>\n",
       "      <td>0.641416</td>\n",
       "      <td>0.610272</td>\n",
       "      <td>0.606052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ASP0      ASP1      ASP2      ASP3      ASP4\n",
       "0  0.598738  0.591786  0.641416  0.610272  0.606052"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict( {\"ASP\"+str(ai):[get_clas_acc(ai)(outs, target).item()] for ai in range(5)} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASP0</th>\n",
       "      <th>ASP1</th>\n",
       "      <th>ASP2</th>\n",
       "      <th>ASP3</th>\n",
       "      <th>ASP4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.453062</td>\n",
       "      <td>0.447597</td>\n",
       "      <td>0.390452</td>\n",
       "      <td>0.42879</td>\n",
       "      <td>0.443056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ASP0      ASP1      ASP2     ASP3      ASP4\n",
       "0  0.453062  0.447597  0.390452  0.42879  0.443056"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict( {\"ASP\"+str(ai):[get_clas_mse(ai)(outs, target).item()] for ai in range(5)} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth:\n",
      "[3, 3, 4, 3, 4]\n",
      "prediction:\n",
      "tensor([3, 3, 3, 3, 4])\n",
      "doc:\n",
      "According to the website, the style for the Caldera Cauldron changes every year\n",
      "          +++ Appearance +++ [0.003 0.039 0.003 0.012 0.014]\n",
      "The current release is a DIPA, which frankly is the only cauldron I'm familiar with (it was an IPA/DIPA the last time I ordered a cauldron at the horsebrass several years back)\n",
      "          +++ Appearance +++ [0.006 0.072 0.011 0.033 0.046]\n",
      "In any event\n",
      "          +++ Appearance +++ [0.006 0.074 0.01  0.017 0.027]\n",
      "at the Horse Brass yesterday\n",
      "          +++ Appearance +++ [0.005 0.067 0.011 0.022 0.037]\n",
      "The beer pours an orange copper color with good head retention and lacing\n",
      "          +++ Appearance +++ [7.520e-05 9.309e-01 5.485e-05 1.836e-03 3.334e-04]\n",
      "The nose is all hoppy IPA goodness, showcasing a huge aroma of dry citrus, pine and sandlewood\n",
      "          +++ Aroma +++ [0.002 0.037 0.003 0.002 0.478]\n",
      "The flavor profile replicates the nose pretty closely in this West Coast all the way DIPA\n",
      "          +++ Aroma +++ [0.01  0.032 0.035 0.018 0.104]\n",
      "This DIPA is not for the faint of heart and is a bit much even for a hophead like myslf\n",
      "          +++ Aroma +++ [0.016 0.059 0.049 0.047 0.068]\n",
      "The finish is quite dry and hoppy, and there's barely enough sweet malt to balance and hold up the avalanche of hoppy bitterness in this beer\n",
      "          +++ Aroma +++ [0.028 0.085 0.11  0.065 0.127]\n",
      "Mouthfeel is actually fairly light, with a long, persistentely bitter finish\n",
      "          +++ Palate +++ [0.011 0.08  0.016 0.443 0.022]\n",
      "Drinkability is good, with the alcohol barely noticeable in this well crafted beer\n",
      "          +++ Appearance +++ [0.155 0.211 0.093 0.187 0.166]\n",
      "Still, this beer is so hugely hoppy/bitter, it's really hard for me to imagine ordering more than a single glass\n",
      "          +++ Appearance +++ [0.079 0.151 0.093 0.092 0.125]\n",
      "Regardless, this is a very impressive beer from the folks at Caldera\n",
      "          +++ Aroma +++ [0.065 0.21  0.141 0.162 0.228]\n",
      "===========\n",
      "truth:\n",
      "[2, 2, 2, 2, 2]\n",
      "prediction:\n",
      "tensor([2, 3, 2, 2, 2])\n",
      "doc:\n",
      "Poured from the bottle into a Chimay goblet\n",
      "          +++ Appearance +++ [0.007 0.137 0.015 0.025 0.057]\n",
      "Appearance: Pours a slightly cloudy yellow/orange color with a half finger of fluffy white head\n",
      "          +++ Appearance +++ [0.003 0.418 0.002 0.033 0.009]\n",
      "The head fades to a small layer on top of the pour\n",
      "          +++ Appearance +++ [0.002 0.322 0.001 0.014 0.012]\n",
      "Smell: Very light and crisp\n",
      "          +++ Aroma +++ [0.003 0.041 0.005 0.003 0.421]\n",
      "I'm definitely picking up the ginger, but it's not overly powerful\n",
      "          +++ Aroma +++ [0.006 0.065 0.008 0.006 0.603]\n",
      "There is a slight sweetness from the malt as well\n",
      "          +++ Aroma +++ [0.004 0.05  0.011 0.008 0.263]\n",
      "Taste: Very light and refreshing\n",
      "          +++ Aroma +++ [0.03  0.076 0.09  0.054 0.132]\n",
      "The ginger shows up right away and then fades towards the finish of the sip\n",
      "          +++ Aroma +++ [0.029 0.083 0.086 0.081 0.087]\n",
      "The finish is malty and bread like\n",
      "          +++ Palate +++ [0.01  0.054 0.039 0.065 0.039]\n",
      "Mouthfeel: The body is on the thin side with smooth carbonation and a very dry finish\n",
      "          +++ Palate +++ [0.007 0.079 0.011 0.239 0.02 ]\n",
      "Overall: This is a light and refreshing beer, but nothing spectacular\n",
      "          +++ Palate +++ [0.066 0.168 0.186 0.244 0.232]\n",
      "The amount of ginger is nice, but I would have liked to have more going on\n",
      "          +++ Aroma +++ [0.081 0.165 0.154 0.169 0.247]\n",
      "===========\n",
      "truth:\n",
      "[3, 3, 3, 2, 3]\n",
      "prediction:\n",
      "tensor([3, 3, 3, 3, 2])\n",
      "doc:\n",
      "Notes from 6/24\n",
      "          +++ Appearance +++ [0.002 0.025 0.002 0.006 0.014]\n",
      "A: Bright golden glowing beer in a moment of clarity with a lively white head of feathery fluff\n",
      "          +++ Appearance +++ [0.001 0.443 0.001 0.009 0.009]\n",
      "S: The ginger is definitely there, or am I smelling my Indian dinner? Almost cake-like in its malt aroma, sharply bready with a slight edge of sweetness\n",
      "          +++ Aroma +++ [0.006 0.041 0.015 0.009 0.274]\n",
      "T: Nice clear malty throat taste, reminds me of strands of complex sugars and grains, ginger is more subtle than I expected, more of an undertone than a backbone\n",
      "          +++ Taste +++ [0.016 0.052 0.08  0.027 0.055]\n",
      "M: A refreshing light beer feel, like a pilsner or summer ale\n",
      "          +++ Palate +++ [0.007 0.077 0.008 0.476 0.012]\n",
      "D: If this was a sixer instead of a double-deuce, I could see this being a fine picnic pounder\n",
      "          +++ Appearance +++ [0.16  0.204 0.06  0.178 0.131]\n",
      "Overall, pretty impressed for the particular style\n",
      "          +++ Appearance +++ [0.074 0.231 0.118 0.167 0.223]\n",
      "===========\n",
      "truth:\n",
      "[3, 2, 2, 2, 4]\n",
      "prediction:\n",
      "tensor([2, 2, 2, 2, 3])\n",
      "doc:\n",
      "22 oz\n",
      "          +++ Appearance +++ [0.001 0.026 0.001 0.008 0.004]\n",
      "bomber,\n",
      "          +++ Appearance +++ [0.001 0.024 0.001 0.005 0.002]\n",
      "A: Pours a clear yellow with a mild white head, good retention\n",
      "          +++ Appearance +++ [0.003 0.691 0.001 0.026 0.011]\n",
      "S: Great nose of ginger, honey, perfume\n",
      "          +++ Aroma +++ [4.693e-04 6.857e-03 5.549e-04 1.787e-04 6.361e-01]\n",
      "T: Rather light upfront, it reminds me of Lawnmower, with a hint of ale fruitiness/Kolsch-like almost\n",
      "          +++ Appearance +++ [0.018 0.065 0.062 0.052 0.061]\n",
      "Good ginger honey notes on the back end, not taking over the beer in any way\n",
      "          +++ Taste +++ [0.012 0.038 0.06  0.027 0.059]\n",
      "The ginger flavour is clear, but I wanted it to come out a little more in the end\n",
      "          +++ Taste +++ [0.011 0.026 0.103 0.026 0.05 ]\n",
      "M: Very light-bodied, watery, light base beer for sure\n",
      "          +++ Palate +++ [0.006 0.146 0.009 0.502 0.014]\n",
      "D: An easy drinking spiced beer, this will offend no one, but there's not a complexity to this brew at all\n",
      "          +++ Palate +++ [0.06  0.137 0.085 0.14  0.137]\n",
      "===========\n",
      "truth:\n",
      "[2, 2, 2, 2, 2]\n",
      "prediction:\n",
      "tensor([2, 2, 2, 2, 2])\n",
      "doc:\n",
      "Brown in color, somewhere between a porter and a brown ale\n",
      "          +++ Appearance +++ [0.003 0.421 0.004 0.024 0.04 ]\n",
      "Lacking in aroma, but no off stuff\n",
      "          +++ Aroma +++ [0.004 0.037 0.004 0.004 0.705]\n",
      "Same with the taste, lacking flavor, complexity, just went with smoothness\n",
      "          +++ Appearance +++ [0.055 0.225 0.09  0.155 0.178]\n",
      "No off flavors though, so I can't say this is bad, just unadventurous, especially for Caldera, whom I think is generally underrated\n",
      "          +++ Aroma +++ [0.063 0.181 0.169 0.148 0.223]\n",
      "You really have to search to pull anything out of this in terms of the usual chocolate/coffee flavors, really, the only thing I can tell is that the oats did their job, because this is smooth and unoffensive\n",
      "          +++ Palate +++ [0.065 0.195 0.078 0.278 0.142]\n",
      "Other than that, extremely pedestrian\n",
      "          +++ Aroma +++ [0.074 0.207 0.108 0.128 0.266]\n",
      "===========\n",
      "truth:\n",
      "[4, 2, 4, 2, 4]\n",
      "prediction:\n",
      "tensor([2, 3, 4, 3, 4])\n",
      "doc:\n",
      "Pours a mahogany color, rich, with a tan head\n",
      "          +++ Appearance +++ [2.105e-04 5.720e-01 9.089e-05 1.703e-03 2.206e-03]\n",
      "a finger, didn't stick around\n",
      "          +++ Appearance +++ [0.001 0.401 0.001 0.007 0.006]\n",
      "Holy smokes! This is one of the most smokiest beer I've ever had! BUT\n",
      "          +++ Aroma +++ [0.006 0.029 0.016 0.009 0.216]\n",
      "It's balance and not one sided\n",
      "          +++ Aroma +++ [0.036 0.123 0.067 0.068 0.153]\n",
      "Smoke, cured bacon, rich caramel malts\n",
      "          +++ Aroma +++ [0.014 0.046 0.039 0.021 0.129]\n",
      "More smoke\n",
      "          +++ Aroma +++ [0.009 0.036 0.034 0.019 0.116]\n",
      "Ok, maybe it one sided, but it's heavenly\n",
      "          +++ Aroma +++ [0.041 0.097 0.126 0.069 0.412]\n",
      "Rich caramel malts and salty cured bacon strips\n",
      "          +++ Aroma +++ [0.017 0.071 0.064 0.036 0.131]\n",
      "Bacon lover right here, so, I don't feel the need to go on\n",
      "          +++ Aroma +++ [0.016 0.073 0.055 0.048 0.154]\n",
      "Lighter, but medium in body\n",
      "          +++ Palate +++ [0.002 0.021 0.005 0.083 0.008]\n",
      "feels fine\n",
      "          +++ Palate +++ [0.002 0.036 0.004 0.193 0.004]\n",
      "Nothing tremendous\n",
      "          +++ Palate +++ [0.018 0.098 0.055 0.286 0.096]\n",
      "Aroma and flavor alone are mouthwatering\n",
      "          +++ Aroma +++ [0.023 0.078 0.079 0.042 0.314]\n",
      "Bravo\n",
      "          +++ Aroma +++ [0.018 0.088 0.067 0.044 0.221]\n",
      "More please!!!\n",
      "          +++ Aroma +++ [0.009 0.047 0.019 0.026 0.084]\n",
      "===========\n",
      "truth:\n",
      "[4, 3, 4, 3, 4]\n",
      "prediction:\n",
      "tensor([3, 3, 4, 3, 4])\n",
      "doc:\n",
      "Pours light caramel brown with reddish highlights\n",
      "          +++ Appearance +++ [0.003 0.097 0.003 0.026 0.012]\n",
      "The crisp white head doesn't hang around long, but quickly dissipates to a thin ring\n",
      "          +++ Appearance +++ [1.347e-04 8.162e-01 7.516e-05 6.170e-03 5.231e-04]\n",
      "Aroma is intense meaty smokyness\n",
      "          +++ Aroma +++ [0.003 0.031 0.005 0.003 0.307]\n",
      "Underneath is a mild malty sweetness\n",
      "          +++ Aroma +++ [0.012 0.072 0.024 0.03  0.124]\n",
      "Taste is really interesting\n",
      "          +++ Aroma +++ [0.016 0.055 0.081 0.032 0.101]\n",
      "The smoked meat quality is retained, but there is also a generous earthy, woody character\n",
      "          +++ Aroma +++ [0.015 0.067 0.082 0.037 0.108]\n",
      "Caramel and toffee really round things out\n",
      "          +++ Taste +++ [0.017 0.057 0.077 0.035 0.076]\n",
      "Finishes with a mildly dry cedar character\n",
      "          +++ Aroma +++ [0.019 0.076 0.082 0.046 0.089]\n",
      "Mouthfeel is a chewy, medium body\n",
      "          +++ Palate +++ [0.005 0.025 0.007 0.186 0.008]\n",
      "A flavorful brew\n",
      "          +++ Appearance +++ [0.073 0.169 0.1   0.164 0.162]\n",
      "Strong smokiness is tempered by the rich caramel malt qualities\n",
      "          +++ Aroma +++ [0.027 0.092 0.055 0.062 0.152]\n",
      "A real winner that could bring the non-smoke lover around\n",
      "          +++ Aroma +++ [0.06  0.149 0.158 0.11  0.26 ]\n",
      "===========\n",
      "truth:\n",
      "[4, 3, 3, 3, 4]\n",
      "prediction:\n",
      "tensor([3, 3, 4, 4, 4])\n",
      "doc:\n",
      "Poured a slightly cloudy deep amber/red color with a thin soap ring of head\n",
      "          +++ Appearance +++ [0.001 0.552 0.001 0.018 0.009]\n",
      "The smell is amazing, big bacony smoked pork Rauch goodness\n",
      "          +++ Aroma +++ [0.002 0.053 0.004 0.002 0.855]\n",
      "The taste follows the flavors nicely with a big smoked malt flavor with a nice bit of underlying sweetness, very pork like and very flavorful\n",
      "          +++ Taste +++ [0.041 0.096 0.183 0.082 0.172]\n",
      "Has a nice light bitterness in the finish that helps even it out very well\n",
      "          +++ Taste +++ [0.016 0.028 0.204 0.039 0.058]\n",
      "The mouthfeel is very slick and oily\n",
      "          +++ Palate +++ [0.002 0.037 0.004 0.667 0.002]\n",
      "Great Rauchbier!\n",
      "          +++ Palate +++ [0.071 0.166 0.189 0.271 0.193]\n",
      "===========\n",
      "truth:\n",
      "[4, 3, 3, 3, 4]\n",
      "prediction:\n",
      "tensor([4, 4, 4, 3, 4])\n",
      "doc:\n",
      "A- Semi aggressive pour produces a 1-1/2 finger very light brown head that rests atop a beautiful deep copper colored beer with nice clarity\n",
      "          +++ Appearance +++ [9.676e-05 9.440e-01 7.380e-05 1.271e-03 7.759e-04]\n",
      "Nicve lacing\n",
      "          +++ Appearance +++ [3.168e-04 7.476e-01 2.372e-04 5.623e-03 1.605e-03]\n",
      "S- Very similar to a smoked porter but more smoke\n",
      "          +++ Aroma +++ [0.007 0.08  0.014 0.019 0.124]\n",
      "The smoke smell is beautiful\n",
      "          +++ Aroma +++ [0.002 0.069 0.003 0.003 0.46 ]\n",
      "It smells like a campfire and the cherry wood is definitely present\n",
      "          +++ Aroma +++ [0.004 0.075 0.009 0.008 0.609]\n",
      "Excellent smelling beer\n",
      "          +++ Aroma +++ [7.765e-04 1.065e-02 3.980e-04 1.971e-04 9.392e-01]\n",
      "T- WOW! They were not kidding on the bottle when they said it has an intense smoke flavor\n",
      "          +++ Aroma +++ [0.021 0.083 0.077 0.057 0.143]\n",
      "It is excellent and I could see some folks that the smoke is too in-your-face, but I love smoked beers and I love this one\n",
      "          +++ Aroma +++ [0.038 0.142 0.2   0.093 0.254]\n",
      "This is as smoked wood forward as I've ever had\n",
      "          +++ Aroma +++ [0.021 0.084 0.077 0.066 0.122]\n",
      "It's pretty much smoky wood, a slight malt sweetness, and finishes woody and smokey again\n",
      "          +++ Taste +++ [0.011 0.041 0.063 0.03  0.054]\n",
      "This is a perfect fall beer\n",
      "          +++ Aroma +++ [0.049 0.145 0.173 0.117 0.251]\n",
      "M- Nice smooth mouthfeel with good carbonation\n",
      "          +++ Palate +++ [0.005 0.083 0.009 0.327 0.012]\n",
      "O- Not too complex because of the intense smoke taste but it works excellent in this beer\n",
      "          +++ Aroma +++ [0.061 0.183 0.134 0.139 0.206]\n",
      "I will definitely be picking this up again\n",
      "          +++ Aroma +++ [0.058 0.225 0.13  0.154 0.229]\n",
      "===========\n",
      "truth:\n",
      "[2, 2, 2, 1, 4]\n",
      "prediction:\n",
      "tensor([0, 2, 1, 2, 2])\n",
      "doc:\n",
      "Amber to yeloowcolored, was expecting darker, kind of even had a good degree of clarity to it\n",
      "          +++ Appearance +++ [0.007 0.249 0.012 0.047 0.044]\n",
      "Holy cherry wood campfire smoke! This smelled just like you were out in the woods\n",
      "          +++ Aroma +++ [0.006 0.037 0.014 0.016 0.113]\n",
      "Whoa, I could only take a small sip at first, massive liquid smoke feeling in the mouthfeel\n",
      "          +++ Palate +++ [0.008 0.102 0.013 0.118 0.028]\n",
      "Ugh\n",
      "          +++ Palate +++ [0.014 0.074 0.026 0.099 0.053]\n",
      "Its not so much as 'bad' as opposed to 'challenging'\n",
      "          +++ Aroma +++ [0.035 0.108 0.086 0.103 0.127]\n",
      "I must admit, I could not even finish a taster of it\n",
      "          +++ Appearance +++ [0.082 0.203 0.098 0.14  0.191]\n",
      "This is the most intense smoked beer I've ever come across, by far\n",
      "          +++ Aroma +++ [0.011 0.051 0.056 0.054 0.079]\n",
      "I mean, beer geek bacon was a walk in the park compared to this\n",
      "          +++ Aroma +++ [0.008 0.052 0.018 0.024 0.084]\n",
      "I could smell it all day\n",
      "          +++ Aroma +++ [0.003 0.045 0.003 0.003 0.392]\n",
      "As far as drinking it, I'm cool with 1oz at a time, its that intense\n",
      "          +++ Aroma +++ [0.026 0.098 0.073 0.075 0.123]\n",
      "If you like smoked beers, then check this out\n",
      "          +++ Aroma +++ [0.016 0.07  0.046 0.046 0.124]\n",
      "If not, stay away\n",
      "          +++ Appearance +++ [0.025 0.131 0.057 0.078 0.131]\n",
      "I kind of like them, trying to get into them more, so I'd try it again next time, but for now, I'm not worthy\n",
      "          +++ Aroma +++ [0.047 0.115 0.126 0.104 0.165]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "asp_inc_overall = True\n",
    "if not asp_inc_overall: \n",
    "    nasp_analysis = hyper_params[\"num_aspect\"] - 1\n",
    "else:\n",
    "    nasp_analysis = hyper_params[\"num_aspect\"]\n",
    "    \n",
    "np.set_printoptions(precision=3)\n",
    "asp_name = [\"Overall\", \"Appearance\", \"Taste\", \"Palate\", \"Aroma\"]\n",
    "for i in range(10):\n",
    "    print(\"truth:\")\n",
    "    print(df_test.iloc[i,0:5].values.flatten().tolist() )\n",
    "    print(\"prediction:\")\n",
    "    print( torch.argmax(outs[i][0:5],dim=1) )\n",
    "    print(\"doc:\")\n",
    "    dasp = torch.argmax(asps[i][:,0:nasp_analysis],dim=1).numpy()\n",
    "    if asp_inc_overall: dasp_noall = torch.argmax(asps[i][:,1:6],dim=1).numpy()\n",
    "#     dasp_dist = torch.nn.functional.softmax(asps[i][:,0:nasp_analysis], dim=1).numpy()\n",
    "    dasp_dist = asps[i][:,0:nasp_analysis].numpy()\n",
    "    for senti,s in enumerate(df_test.iloc[i,-1]):\n",
    "        print(s)\n",
    "        if asp_inc_overall:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]] + \" +++ \" + str(dasp_dist[senti]) )\n",
    "        else:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]+1] + \" +++ \" + str(dasp_dist[senti]) )\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hotel_asp(asp_pred, asp_true, asp_inc_overall):\n",
    "    asp_to_id = {\"appearance\":0, \"taste\":1, \"palate\":2, \"aroma\":3, \"none\":-1}\n",
    "    asp_true = np.array( [asp_to_id[l] for l in asp_true] )\n",
    "    print(\"total true: \" + str(len(asp_true)) )\n",
    "    print(\"total not none: \" + str(sum(asp_true>0)) )\n",
    "    \n",
    "    asp_pred_index = []\n",
    "    if asp_inc_overall:\n",
    "        for i in range(1000):\n",
    "            asp_pred_index.append( asp_pred[i][:,1:6].numpy().argsort() )\n",
    "    else:\n",
    "        for i in range(1000):\n",
    "            asp_pred_index.append( asp_pred[i][:,0:5].numpy().argsort() )\n",
    "    asp_pred_index = np.concatenate( asp_pred_index , axis=0)\n",
    "    \n",
    "    result_index = []\n",
    "    for i,lbl in enumerate(asp_true):\n",
    "        if(lbl==-1):\n",
    "            result_index.append(-1)\n",
    "        else:\n",
    "            at = np.where(asp_pred_index[i,] == lbl)\n",
    "            result_index.append(at[0][0])\n",
    "    result_index = np.array(result_index)\n",
    "    \n",
    "    print(\"Top 1 ACC:\")\n",
    "    print( sum(result_index>=4) / sum(result_index>=0) )\n",
    "    print(\"Top 2 ACC:\")\n",
    "    print( sum(result_index>=3) / sum(result_index>=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ee5f835c35b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_dir' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "yifan_label = open(dataset_dir + \"test_aspect_0.yifanmarjan.aspect\", \"r\").readlines()\n",
    "yifan_label = [s.split()[0] for s in yifan_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true: 1000\n",
      "total not none: 454\n",
      "Top 1 ACC:\n",
      "0.6763005780346821\n",
      "Top 2 ACC:\n",
      "0.9152215799614644\n"
     ]
    }
   ],
   "source": [
    "eval_hotel_asp(asps, yifan_label, asp_inc_overall=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_label = open(dataset_dir + \"test_aspect_0.fan.aspect\", \"r\").readlines()\n",
    "fan_label = [s.split()[0] for s in fan_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true: 621\n",
      "total not none: 288\n",
      "Top 1 ACC:\n",
      "0.6814159292035398\n",
      "Top 2 ACC:\n",
      "0.887905604719764\n"
     ]
    }
   ],
   "source": [
    "eval_hotel_asp(asps, fan_label, asp_inc_overall=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
