{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/aeryen/2019nn/d2f7af223dfb4099a5ac55312900fa34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "experiment = comet_ml.Experiment(project_name=\"2019nn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from fastai.text import *\n",
    "from data_helpers.Data import *\n",
    "from fastai.text.transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    \"max_sequence_length\": 40*70,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs1\": 8,\n",
    "    \"num_epochs2\": 12,\n",
    "    \"num_aspect\": 6,\n",
    "    \"num_rating\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LM Databunch and LM Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_db = load_data(\"./data/\", \"hotel_lm_databunch.1001\")\n",
    "# lm_learn = language_model_learner(lm_db, AWD_LSTM)\n",
    "# lm_learn = lm_learn.load(\"lang_model_hotel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_learn.save_encoder('lang_model_hotel_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_db = load_data(\"./data/\", \"cls_databunch_hotel.allaspect.1115\")\n",
    "cls_db.batch_size=hyper_params[\"batch_size\"]\n",
    "cls_db.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Feature Combo Pooling (1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_combo(output, start, end):\n",
    "    avg_pool = output[start:end, :].mean(dim=0)\n",
    "    max_pool = output[start:end, :].max(dim=0)[0]\n",
    "    x = torch.cat([output[-1,:], max_pool, avg_pool], 0)\n",
    "    return x\n",
    "\n",
    "def sentence_extract_pool(outputs, mask, p_index):\n",
    "    \"Pool MultiBatchEncoder outputs into one vector [last_hidden, max_pool, avg_pool].\"\n",
    "    output = outputs[-1]\n",
    "    seq_max = output.size(1)\n",
    "    doc_start = mask.int().sum(dim=1)\n",
    "    \n",
    "    batch = []\n",
    "    for doci in range(0,output.shape[0]):\n",
    "        pi = p_index[doci,:].nonzero(as_tuple=True)[0].int()\n",
    "        doc = []\n",
    "        for senti in range( len(pi) ):\n",
    "            if senti==0:\n",
    "                doc.append( pool_combo(output[doci,:,:], doc_start[doci], pi[senti]) )\n",
    "            else:\n",
    "                doc.append( pool_combo(output[doci,:,:], pi[senti-1]+1, pi[senti]) )\n",
    "            \n",
    "        batch.append( torch.stack(doc, 0) )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(Module):\n",
    "    \"Create an encoder over `module` that can process a full sentence.\"\n",
    "    def __init__(self, bptt:int, max_len:int, module:nn.Module, vocab, pad_idx:int=1):\n",
    "        print(\"Encoder init\")\n",
    "        self.max_len,self.bptt,self.module,self.pad_idx = max_len,bptt,module,pad_idx\n",
    "        self.vocab = vocab\n",
    "        self.period_index = self.vocab.stoi[\"xxperiod\"]\n",
    "\n",
    "    def concat(self, arrs:Collection[Tensor])->Tensor:\n",
    "        \"Concatenate the `arrs` along the batch dimension.\"\n",
    "        return [torch.cat([l[si] for l in arrs], dim=1) for si in range_of(arrs[0])]\n",
    "\n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()\n",
    "\n",
    "    def forward(self, input:LongTensor)->Tuple[Tensor,Tensor]:\n",
    "        bs,sl = input.size()\n",
    "        self.reset()\n",
    "        raw_outputs,outputs,masks = [],[],[]\n",
    "        p_index = []\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            r, o = self.module(input[:,i: min(i+self.bptt, sl)])\n",
    "            if i>(sl-self.max_len):\n",
    "                masks.append(input[:,i: min(i+self.bptt, sl)] == self.pad_idx)\n",
    "                raw_outputs.append(r)\n",
    "                outputs.append(o)\n",
    "                p_index.append( input[:,i: min(i+self.bptt, sl)] == self.period_index )\n",
    "\n",
    "                \n",
    "        # print(\"number of sentences in docs:\")\n",
    "#         n_sent = torch.sum( x==self.vocab.stoi[\"xxperiod\"] , dim=1)\n",
    "        # print(n_sent)\n",
    "        \n",
    "        # print(\"locating period marks\")\n",
    "        period_index = torch.cat(p_index,dim=1)\n",
    "        \n",
    "        return self.concat(raw_outputs),self.concat(outputs), \\\n",
    "               torch.cat(masks,dim=1),period_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 01: Fastai Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_tag(\"base01\")\n",
    "def masked_concat_pool(outputs, mask):\n",
    "    \"Pool MultiBatchEncoder outputs into one vector [last_hidden, max_pool, avg_pool].\"\n",
    "    output = outputs[-1]\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).mean(dim=1)\n",
    "    avg_pool *= output.size(1) / (output.size(1)-mask.type(avg_pool.dtype).sum(dim=1))[:,None]\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    x = torch.cat([output[:,-1], max_pool, avg_pool], 1)\n",
    "    return x\n",
    "class SimpleDocModule(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int, layers:Collection[int], drops:Collection[float]):\n",
    "        print(\"SimpleDocModule init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin(1200, 50, p=0.5, actn=nn.ReLU(inplace=True))\n",
    "        mod_layers += bn_drop_lin(50, n_asp*n_rat, p=0, actn=None)\n",
    "        self.layers = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "        \n",
    "        x = masked_concat_pool(outputs, mask)\n",
    "\n",
    "        sentiment_dist = self.layers(x)\n",
    "        sentiment_dist = sentiment_dist.view(-1, self.n_asp, self.n_rat)\n",
    "        \n",
    "        return sentiment_dist,raw_outputs,outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 02: Sentence feature (400) extract then estimate distribution, sent. dist. sumed to doc output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_tag(\"base02\")\n",
    "class SimpleSentModule(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int, layers:Collection[int], drops:Collection[float]):\n",
    "        print(\"SimpleSentModule init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        mod_layers = []\n",
    "#         mod_layers += bn_drop_lin(400, 60, p=0.4, actn=nn.ReLU(inplace=True))\n",
    "        mod_layers += bn_drop_lin(400, n_asp*n_rat, p=0, actn=None)\n",
    "        self.layers = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "\n",
    "        # takes only last layer output\n",
    "        output = outputs[-1] # [batch, seq_len, emb_size]\n",
    "\n",
    "        result = []\n",
    "        for bati in range(0,output.shape[0]):\n",
    "            sent_output = output[bati, p_index[bati,:], :]\n",
    "            sentiment_dist = self.layers(sent_output)\n",
    "            sentiment_dist = torch.sum(sentiment_dist, dim=0, keepdim=True)\n",
    "            sentiment_dist = sentiment_dist.view(-1, self.n_asp, self.n_rat)\n",
    "            result.append(sentiment_dist)\n",
    "        \n",
    "        result = torch.cat( result, dim=0 )\n",
    "        \n",
    "        return result,raw_outputs,outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 03: average sentence combo pool feature then do document BMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_tag(\"base03\")\n",
    "class ClsModule1200avg(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int, layers:Collection[int], drops:Collection[float]):\n",
    "        print(\"CLS init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_asp+1, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "#         mod_layers += bn_drop_lin( 1200, self.n_asp+1, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_rat, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "#         mod_layers += bn_drop_lin( 1200, self.n_rat, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "        self.sentiment = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "\n",
    "        output = outputs[-1] # [batch, seq_len, emb_size]\n",
    "\n",
    "        # print(\"number of sentences in docs:\")\n",
    "        n_sent = torch.sum( p_index , dim=1)\n",
    "\n",
    "        batch = sentence_extract_pool(outputs, mask, p_index)\n",
    "        doc_list = []\n",
    "        result = []\n",
    "        for doci in range(0,output.shape[0]):\n",
    "            sent_output = batch[doci]\n",
    "            doc_output = sent_output.mean(dim=0, keepdim=True)\n",
    "            doc_list.append(doc_output)\n",
    "\n",
    "        doc_list = torch.cat( doc_list, dim=0 )\n",
    "        aspect_dist = self.aspect(doc_list)         # [aspect]\n",
    "        sentiment_dist = self.sentiment(doc_list)   # [sentiment]\n",
    "        result = torch.bmm(aspect_dist.unsqueeze(2), sentiment_dist.unsqueeze(1))\n",
    "        \n",
    "        return result,raw_outputs,outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLS 01: overall averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_tag(\"CLS01\")\n",
    "def masked_concat_pool(outputs, mask):\n",
    "    \"Pool MultiBatchEncoder outputs into one vector [last_hidden, max_pool, avg_pool].\"\n",
    "    output = outputs[-1]\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).mean(dim=1)\n",
    "    avg_pool *= output.size(1) / (output.size(1)-mask.type(avg_pool.dtype).sum(dim=1))[:,None]\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    x = torch.cat([output[:,-1], max_pool, avg_pool], 1)\n",
    "    return x\n",
    "class Cls1Module1200(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int, layers:Collection[int], drops:Collection[float]):\n",
    "        print(\"CLS init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_rat, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "        self.overall = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_asp, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_rat, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "        self.sentiment = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "        \n",
    "        doc_batch = masked_concat_pool(outputs, mask)\n",
    "        overall_dist = self.overall(doc_batch)\n",
    "        \n",
    "        batch = sentence_extract_pool(outputs, mask, p_index)\n",
    "        \n",
    "        allsent_emb = torch.cat(batch, dim=0)\n",
    "        aspect_dist = self.aspect(allsent_emb)         # [n_sentence, aspect]\n",
    "        sentiment_dist = self.sentiment(allsent_emb)   # [n_sentence, sentiment]\n",
    "        sent_bmm = torch.bmm(aspect_dist.unsqueeze(2), sentiment_dist.unsqueeze(1))\n",
    "        \n",
    "        result = []\n",
    "        aspect_doc = []\n",
    "        sentim_doc = []\n",
    "        cur = 0\n",
    "        for doci in range(0, len(batch)):\n",
    "            sn = batch[doci].shape[0]\n",
    "            doc = torch.sum(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True) # [1, 7, 5]\n",
    "            result.append(doc)\n",
    "            \n",
    "            aspect_doc.append( aspect_dist[cur:(cur+sn), :] )\n",
    "            sentim_doc.append( sentiment_dist[cur:(cur+sn), :] )\n",
    "            \n",
    "            cur = cur + sn\n",
    "        \n",
    "        result = torch.cat( result, dim=0 )\n",
    "        \n",
    "        result = torch.cat( [overall_dist[:,None,:], result], dim=1 )\n",
    "        \n",
    "        return result,raw_outputs,outputs,aspect_doc,aspect_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLS 02: all aspects attributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_tag(\"CLS02\")\n",
    "class Cls2Module1200(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int, layers:Collection[int], drops:Collection[float]):\n",
    "        print(\"CLS init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_asp+1, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_rat, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "        self.sentiment = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "\n",
    "        batch = sentence_extract_pool(outputs, mask, p_index)\n",
    "        \n",
    "        allsent_emb = torch.cat(batch, dim=0)\n",
    "        aspect_dist = self.aspect(allsent_emb)         # [n_sentence, aspect]\n",
    "        sentiment_dist = self.sentiment(allsent_emb)   # [n_sentence, sentiment]\n",
    "        sent_bmm = torch.bmm(aspect_dist.unsqueeze(2), sentiment_dist.unsqueeze(1))\n",
    "        \n",
    "        result = []\n",
    "        aspect_doc = []\n",
    "        sentim_doc = []\n",
    "        cur = 0\n",
    "        for doci in range(0, len(batch)):\n",
    "            sn = batch[doci].shape[0]\n",
    "            doc = torch.sum(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True) # [1, 7, 5]\n",
    "            result.append(doc)\n",
    "            \n",
    "            aspect_doc.append( aspect_dist[cur:(cur+sn), :] )\n",
    "            sentim_doc.append( sentiment_dist[cur:(cur+sn), :] )\n",
    "            \n",
    "            cur = cur + sn\n",
    "        \n",
    "        result = torch.cat( result, dim=0 )\n",
    "        \n",
    "        return result,raw_outputs,outputs,aspect_doc,sentim_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGR 01: all aspects attributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_tag(\"REGR01\")\n",
    "experiment.add_tag(\"DOCPOLL4ALL\")\n",
    "def masked_concat_pool(outputs, mask):\n",
    "    \"Pool MultiBatchEncoder outputs into one vector [last_hidden, max_pool, avg_pool].\"\n",
    "    output = outputs[-1]\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).mean(dim=1)\n",
    "    avg_pool *= output.size(1) / (output.size(1)-mask.type(avg_pool.dtype).sum(dim=1))[:,None]\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    x = torch.cat([output[:,-1], max_pool, avg_pool], 1)\n",
    "    return x\n",
    "class Regr1Module1200(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int, layers:Collection[int], drops:Collection[float]):\n",
    "        print(\"REGR 1 init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, 1, p=0, actn=None )\n",
    "        self.overall = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_asp, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "#         mod_layers += bn_drop_lin( 50, self.n_asp, p=0, actn=torch.nn.Sigmoid() )\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, 1, p=0, actn=None )\n",
    "        self.sentiment = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "        \n",
    "        doc_batch = masked_concat_pool(outputs, mask)\n",
    "        overall_dist = (self.overall(doc_batch) + 0.5) * (self.n_rat-1)  # [4, 1]\n",
    "        \n",
    "        batch = sentence_extract_pool(outputs, mask, p_index) # list of [80?, 1200]\n",
    "        \n",
    "#         temp_all = []\n",
    "#         for doci in range(0, len(batch)):\n",
    "#             temp_all.append( batch[doci].mean(dim=0, keepdim=True) )\n",
    "#         temp_all = torch.cat( temp_all , dim=0 )\n",
    "#         overall_dist = (self.overall(temp_all) + 0.5) * (self.n_rat-1)  # [4, 1]\n",
    "        \n",
    "        allsent_emb = torch.cat(batch, dim=0)\n",
    "        aspect_dist = self.aspect(allsent_emb)         # [n_sentence, aspect]\n",
    "        sentiment_dist = (self.sentiment(allsent_emb) + 0.5) * (self.n_rat-1)   # [n_sentence, 1]\n",
    "        sent_bmm = aspect_dist * sentiment_dist  # [n_sentence, aspect]\n",
    "        \n",
    "        result = []\n",
    "        aspect_doc = []\n",
    "        sentim_doc = []\n",
    "        cur = 0\n",
    "        for doci in range(0, len(batch)):\n",
    "            sn = batch[doci].shape[0]\n",
    "            doc = torch.sum(sent_bmm[cur:(cur+sn),:], dim=0, keepdim=True) # [1, 6]\n",
    "            asp_w_sum = torch.sum(aspect_dist[cur:(cur+sn),:], dim=0, keepdim=True) # [1, 6]\n",
    "            doc = doc / asp_w_sum\n",
    "            result.append(doc)\n",
    "            \n",
    "            aspect_doc.append( aspect_dist[cur:(cur+sn), :] )\n",
    "            sentim_doc.append( sentiment_dist[cur:(cur+sn)] )\n",
    "            \n",
    "            cur = cur + sn\n",
    "        \n",
    "        result = torch.cat( result, dim=0 )\n",
    "        result = torch.cat( [overall_dist, result], dim=1 )\n",
    "        \n",
    "        return result,raw_outputs,outputs,aspect_doc,sentim_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGR 02: OFFSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_tag(\"REGR02\")\n",
    "def masked_concat_pool(outputs, mask):\n",
    "    \"Pool MultiBatchEncoder outputs into one vector [last_hidden, max_pool, avg_pool].\"\n",
    "    output = outputs[-1]\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).mean(dim=1)\n",
    "    avg_pool *= output.size(1) / (output.size(1)-mask.type(avg_pool.dtype).sum(dim=1))[:,None]\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    x = torch.cat([output[:,-1], max_pool, avg_pool], 1)\n",
    "    return x\n",
    "class Regr2Module1200(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int, layers:Collection[int], drops:Collection[float]):\n",
    "        print(\"REGR 2 init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1200, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, 1, p=0, actn=None )\n",
    "        self.overall = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1201, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, self.n_asp, p=0, actn=torch.nn.Softmax(dim=1) )\n",
    "#         mod_layers += bn_drop_lin( 50, self.n_asp, p=0, actn=torch.nn.Sigmoid() )\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 1201, 50, p=0.5, actn=nn.ReLU(inplace=True) )\n",
    "        mod_layers += bn_drop_lin( 50, 1, p=0, actn=None )\n",
    "        self.sentiment = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "        \n",
    "        doc_batch = masked_concat_pool(outputs, mask)    # use original combo pool\n",
    "        overall_dist = self.overall(doc_batch)  # [4, 1]\n",
    "\n",
    "        batch = sentence_extract_pool(outputs, mask, p_index) # list of [80?, 1200]\n",
    "    \n",
    "        temp_all = []\n",
    "        for doci in range(0, len(batch)):\n",
    "            one_doc_overall = overall_dist[doci,None]\n",
    "            sn = batch[doci].shape[0]\n",
    "            temp_all.append( one_doc_overall.expand(sn,-1) )\n",
    "        temp_all = torch.cat( temp_all , dim=0)\n",
    "        \n",
    "        allsent_emb = torch.cat(batch, dim=0)\n",
    "        allsent_emb = torch.cat( [temp_all, allsent_emb], dim=1 )\n",
    "\n",
    "        aspect_dist = self.aspect(allsent_emb)         # [n_sentence, aspect]\n",
    "        sentiment_dist = self.sentiment(allsent_emb)   # [n_sentence, 1]\n",
    "        sent_bmm = aspect_dist * sentiment_dist  # [n_sentence, aspect]\n",
    "        \n",
    "        result = []\n",
    "        aspect_doc = []\n",
    "        sentim_doc = []\n",
    "        cur = 0\n",
    "        for doci in range(0, len(batch)):\n",
    "            sn = batch[doci].shape[0]\n",
    "            doc = torch.sum(sent_bmm[cur:(cur+sn),:], dim=0, keepdim=True) # [1, 6]\n",
    "            asp_w_sum = torch.sum(aspect_dist[cur:(cur+sn),:], dim=0, keepdim=True) # [1, 6]\n",
    "            doc = doc / asp_w_sum\n",
    "            result.append(doc)\n",
    "            \n",
    "            aspect_doc.append( aspect_dist[cur:(cur+sn), :] )\n",
    "            sentim_doc.append( sentiment_dist[cur:(cur+sn)] )\n",
    "            \n",
    "            cur = cur + sn\n",
    "        \n",
    "        result = torch.cat( result, dim=0 )\n",
    "        \n",
    "        overall_dist = (overall_dist + 0.5) * (self.n_rat-1) # normalize from 0 mean to 0-4 range\n",
    "        result = result + overall_dist\n",
    "        result = torch.cat( [overall_dist, result], dim=1 )\n",
    "        \n",
    "        return result,raw_outputs,outputs,aspect_doc,sentim_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_classifier(arch:Callable, vocab_sz:int, vocab, n_class:int, bptt:int=70, max_len:int=20*70, config:dict=None,\n",
    "                        drop_mult:float=1., lin_ftrs:Collection[int]=None, ps:Collection[float]=None,\n",
    "                        pad_idx:int=1) -> nn.Module:\n",
    "    \"Create a text classifier from `arch` and its `config`, maybe `pretrained`.\"\n",
    "    print(\"CUSTOM DEFINED CLASSIFIER\")\n",
    "    meta = text.learner._model_meta[arch]\n",
    "    config = ifnone(config, meta['config_clas']).copy()\n",
    "    for k in config.keys():\n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "    if lin_ftrs is None: lin_ftrs = [50]\n",
    "    if ps is None:  ps = [0.1]*len(lin_ftrs)\n",
    "    layers = [config[meta['hid_name']] * 3] + lin_ftrs + [n_class]\n",
    "    ps = [config.pop('output_p')] + ps\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    encoder = SentenceEncoder(bptt, max_len, arch(vocab_sz, **config), vocab, pad_idx=pad_idx)\n",
    "    cls_layer = Regr1Module1200(n_asp=hyper_params[\"num_aspect\"], n_rat=hyper_params[\"num_rating\"], layers=layers, drops=ps)\n",
    "    model = SequentialRNN(encoder, cls_layer)\n",
    "    return model if init is None else model.apply(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classifier_learner(data:DataBunch, arch:Callable, bptt:int=70, max_len:int=20*70, config:dict=None,\n",
    "                            pretrained:bool=True, drop_mult:float=1., lin_ftrs:Collection[int]=None,\n",
    "                            ps:Collection[float]=None, **learn_kwargs) -> 'TextClassifierLearner':\n",
    "    \"Create a `Learner` with a text classifier from `data` and `arch`.\"\n",
    "    model = get_text_classifier(arch, len(data.vocab.itos), data.vocab, data.c, bptt=bptt, max_len=max_len,\n",
    "                                config=config, drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps)\n",
    "    meta = text.learner._model_meta[arch]\n",
    "    learn = RNNLearner(data, model, split_func=meta['split_clas'], **learn_kwargs)\n",
    "    if pretrained:\n",
    "        if 'url' not in meta:\n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], data=False)\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn = learn.load_pretrained(*fnames, strict=False)\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelCEL(nn.CrossEntropyLoss):\n",
    "    def forward(self, input, target, nasp=6):\n",
    "        target = target.long()\n",
    "        loss = 0\n",
    "        for i in range(nasp):\n",
    "            loss = loss + super(MultiLabelCEL, self).forward(input[:,i,:], target[:,i])\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(preds, targs, nasp=6, nrat=5):\n",
    "    preds = preds[:,0:nasp,:]\n",
    "    preds = preds.contiguous().view(-1, nrat)\n",
    "    preds = torch.max(preds, dim=1)[1]\n",
    "    targs = targs.contiguous().view(-1).long()\n",
    "    return (preds==targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_0(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,0]==targs[:,0]).float().mean()\n",
    "def acc_1(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,1]==targs[:,1]).float().mean()\n",
    "def acc_2(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,2]==targs[:,2]).float().mean()\n",
    "def acc_3(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,3]==targs[:,3]).float().mean()\n",
    "def acc_4(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,4]==targs[:,4]).float().mean()\n",
    "def acc_5(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,5]==targs[:,5]).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelMSE(nn.MSELoss):\n",
    "    def forward(self, input, target, nasp=6):\n",
    "        target = target.float()\n",
    "        loss = 0\n",
    "        for i in range(nasp):\n",
    "            loss = loss + super(MultiLabelMSE, self).forward(input[:,i], target[:,i])\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_0(preds, targs):\n",
    "    preds = preds[:,0]\n",
    "    targs = targs.contiguous().float()[:,0]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)\n",
    "def mse_1(preds, targs):\n",
    "    preds = preds[:,1]\n",
    "    targs = targs.contiguous().float()[:,1]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)\n",
    "def mse_2(preds, targs):\n",
    "    preds = preds[:,2]\n",
    "    targs = targs.contiguous().float()[:,2]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)\n",
    "def mse_3(preds, targs):\n",
    "    preds = preds[:,3]\n",
    "    targs = targs.contiguous().float()[:,3]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)\n",
    "def mse_4(preds, targs):\n",
    "    preds = preds[:,4]\n",
    "    targs = targs.contiguous().float()[:,4]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)\n",
    "def mse_5(preds, targs):\n",
    "    preds = preds[:,5]\n",
    "    targs = targs.contiguous().float()[:,5]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_regr_acc(preds, targs, nasp=6, nrat=5):\n",
    "    preds = preds[:,0:nasp]\n",
    "    preds = preds.contiguous().view(-1)\n",
    "    preds = preds.round()\n",
    "    targs = targs.contiguous().view(-1).long()\n",
    "    return (preds==targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regr_acc0(preds, targs):\n",
    "    return (preds[:,0].round()==targs[:,0].long()).float().mean()\n",
    "def regr_acc1(preds, targs):\n",
    "    return (preds[:,1].round()==targs[:,0].long()).float().mean()\n",
    "def regr_acc2(preds, targs):\n",
    "    return (preds[:,2].round()==targs[:,0].long()).float().mean()\n",
    "def regr_acc3(preds, targs):\n",
    "    return (preds[:,3].round()==targs[:,0].long()).float().mean()\n",
    "def regr_acc4(preds, targs):\n",
    "    return (preds[:,4].round()==targs[:,0].long()).float().mean()\n",
    "def regr_acc5(preds, targs):\n",
    "    return (preds[:,5].round()==targs[:,0].long()).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM DEFINED CLASSIFIER\n",
      "Encoder init\n",
      "REGR 1 init\n",
      "Num Aspect: 6\n",
      "Num Rating: 5\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): Embedding(23008, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(23008, 400, padding_idx=1)\n",
      "  )\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Regr1Module1200(\n",
      "    (overall): Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=50, out_features=1, bias=True)\n",
      "    )\n",
      "    (aspect): Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=50, out_features=6, bias=True)\n",
      "      (6): Softmax(dim=1)\n",
      "    )\n",
      "    (sentiment): Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=50, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# mloss = MultiLabelCEL()\n",
    "mloss = MultiLabelMSE()\n",
    "cls_learn = text_classifier_learner(cls_db, AWD_LSTM, \n",
    "                                    loss_func=mloss,\n",
    "#                                     metrics=[multi_acc,acc_0,acc_1,acc_2,acc_3,acc_4,acc_5],\n",
    "                                    metrics=[mse_0,mse_1,mse_2,mse_3,mse_4,mse_5,\n",
    "                                             multi_regr_acc,regr_acc0,regr_acc1,regr_acc2,regr_acc3,regr_acc4,regr_acc5],\n",
    "                                    bptt=70,\n",
    "                                    max_len=hyper_params[\"max_sequence_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREEZING\n",
      "Sequential(\n",
      "  (0): Embedding(23008, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(23008, 400, padding_idx=1)\n",
      "  )\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Regr1Module1200(\n",
      "    (overall): Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=50, out_features=1, bias=True)\n",
      "    )\n",
      "    (aspect): Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=50, out_features=6, bias=True)\n",
      "      (6): Softmax(dim=1)\n",
      "    )\n",
      "    (sentiment): Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=50, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "_=cls_learn.load_encoder('lm_enc_hotel.1115')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGR 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mse_0</th>\n",
       "      <th>mse_1</th>\n",
       "      <th>mse_2</th>\n",
       "      <th>mse_3</th>\n",
       "      <th>mse_4</th>\n",
       "      <th>mse_5</th>\n",
       "      <th>multi_regr_acc</th>\n",
       "      <th>regr_acc0</th>\n",
       "      <th>regr_acc1</th>\n",
       "      <th>regr_acc2</th>\n",
       "      <th>regr_acc3</th>\n",
       "      <th>regr_acc4</th>\n",
       "      <th>regr_acc5</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.918314</td>\n",
       "      <td>5.318137</td>\n",
       "      <td>0.760882</td>\n",
       "      <td>0.784878</td>\n",
       "      <td>0.846248</td>\n",
       "      <td>1.036169</td>\n",
       "      <td>0.894724</td>\n",
       "      <td>0.995238</td>\n",
       "      <td>0.399661</td>\n",
       "      <td>0.433003</td>\n",
       "      <td>0.445039</td>\n",
       "      <td>0.414014</td>\n",
       "      <td>0.265846</td>\n",
       "      <td>0.342070</td>\n",
       "      <td>0.397700</td>\n",
       "      <td>01:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.598637</td>\n",
       "      <td>4.893555</td>\n",
       "      <td>0.564469</td>\n",
       "      <td>0.736182</td>\n",
       "      <td>0.816203</td>\n",
       "      <td>1.007366</td>\n",
       "      <td>0.866276</td>\n",
       "      <td>0.903058</td>\n",
       "      <td>0.457163</td>\n",
       "      <td>0.516716</td>\n",
       "      <td>0.515913</td>\n",
       "      <td>0.481145</td>\n",
       "      <td>0.280556</td>\n",
       "      <td>0.445574</td>\n",
       "      <td>0.469644</td>\n",
       "      <td>01:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.329746</td>\n",
       "      <td>4.499112</td>\n",
       "      <td>0.484033</td>\n",
       "      <td>0.680830</td>\n",
       "      <td>0.783671</td>\n",
       "      <td>0.938388</td>\n",
       "      <td>0.769219</td>\n",
       "      <td>0.842971</td>\n",
       "      <td>0.471249</td>\n",
       "      <td>0.532495</td>\n",
       "      <td>0.548007</td>\n",
       "      <td>0.462156</td>\n",
       "      <td>0.284301</td>\n",
       "      <td>0.403584</td>\n",
       "      <td>0.468307</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.030967</td>\n",
       "      <td>4.448998</td>\n",
       "      <td>0.501068</td>\n",
       "      <td>0.676945</td>\n",
       "      <td>0.733243</td>\n",
       "      <td>0.941980</td>\n",
       "      <td>0.757807</td>\n",
       "      <td>0.837955</td>\n",
       "      <td>0.476108</td>\n",
       "      <td>0.550949</td>\n",
       "      <td>0.546403</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.287510</td>\n",
       "      <td>0.436748</td>\n",
       "      <td>0.498262</td>\n",
       "      <td>01:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.862262</td>\n",
       "      <td>4.444495</td>\n",
       "      <td>0.544047</td>\n",
       "      <td>0.716060</td>\n",
       "      <td>0.721153</td>\n",
       "      <td>0.912195</td>\n",
       "      <td>0.731214</td>\n",
       "      <td>0.819826</td>\n",
       "      <td>0.440983</td>\n",
       "      <td>0.534635</td>\n",
       "      <td>0.487564</td>\n",
       "      <td>0.429794</td>\n",
       "      <td>0.279487</td>\n",
       "      <td>0.389141</td>\n",
       "      <td>0.435678</td>\n",
       "      <td>01:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.816375</td>\n",
       "      <td>4.323479</td>\n",
       "      <td>0.518021</td>\n",
       "      <td>0.676121</td>\n",
       "      <td>0.704813</td>\n",
       "      <td>0.903192</td>\n",
       "      <td>0.725731</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.460818</td>\n",
       "      <td>0.534635</td>\n",
       "      <td>0.518053</td>\n",
       "      <td>0.451993</td>\n",
       "      <td>0.290987</td>\n",
       "      <td>0.399037</td>\n",
       "      <td>0.458144</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.857069</td>\n",
       "      <td>4.219900</td>\n",
       "      <td>0.477644</td>\n",
       "      <td>0.656627</td>\n",
       "      <td>0.693910</td>\n",
       "      <td>0.893325</td>\n",
       "      <td>0.715415</td>\n",
       "      <td>0.782977</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>0.532763</td>\n",
       "      <td>0.539182</td>\n",
       "      <td>0.458946</td>\n",
       "      <td>0.289650</td>\n",
       "      <td>0.415352</td>\n",
       "      <td>0.468040</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.807860</td>\n",
       "      <td>4.205840</td>\n",
       "      <td>0.462178</td>\n",
       "      <td>0.649671</td>\n",
       "      <td>0.697231</td>\n",
       "      <td>0.890254</td>\n",
       "      <td>0.722065</td>\n",
       "      <td>0.784442</td>\n",
       "      <td>0.482705</td>\n",
       "      <td>0.553891</td>\n",
       "      <td>0.557368</td>\n",
       "      <td>0.473389</td>\n",
       "      <td>0.296336</td>\n",
       "      <td>0.428724</td>\n",
       "      <td>0.482749</td>\n",
       "      <td>01:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle(hyper_params[\"num_epochs1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mse_0</th>\n",
       "      <th>mse_1</th>\n",
       "      <th>mse_2</th>\n",
       "      <th>mse_3</th>\n",
       "      <th>mse_4</th>\n",
       "      <th>mse_5</th>\n",
       "      <th>multi_regr_acc</th>\n",
       "      <th>regr_acc0</th>\n",
       "      <th>regr_acc1</th>\n",
       "      <th>regr_acc2</th>\n",
       "      <th>regr_acc3</th>\n",
       "      <th>regr_acc4</th>\n",
       "      <th>regr_acc5</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.736481</td>\n",
       "      <td>5.266975</td>\n",
       "      <td>0.589894</td>\n",
       "      <td>0.836705</td>\n",
       "      <td>0.886988</td>\n",
       "      <td>1.050790</td>\n",
       "      <td>0.912635</td>\n",
       "      <td>0.989964</td>\n",
       "      <td>0.416511</td>\n",
       "      <td>0.485156</td>\n",
       "      <td>0.443167</td>\n",
       "      <td>0.398502</td>\n",
       "      <td>0.253811</td>\n",
       "      <td>0.321476</td>\n",
       "      <td>0.389676</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.823283</td>\n",
       "      <td>4.730232</td>\n",
       "      <td>0.562928</td>\n",
       "      <td>0.740457</td>\n",
       "      <td>0.770713</td>\n",
       "      <td>0.952099</td>\n",
       "      <td>0.805361</td>\n",
       "      <td>0.898674</td>\n",
       "      <td>0.443167</td>\n",
       "      <td>0.523402</td>\n",
       "      <td>0.485424</td>\n",
       "      <td>0.454934</td>\n",
       "      <td>0.282696</td>\n",
       "      <td>0.373094</td>\n",
       "      <td>0.420433</td>\n",
       "      <td>02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.275262</td>\n",
       "      <td>4.550611</td>\n",
       "      <td>0.488211</td>\n",
       "      <td>0.716877</td>\n",
       "      <td>0.758482</td>\n",
       "      <td>0.938299</td>\n",
       "      <td>0.769825</td>\n",
       "      <td>0.878918</td>\n",
       "      <td>0.467638</td>\n",
       "      <td>0.526076</td>\n",
       "      <td>0.520995</td>\n",
       "      <td>0.475796</td>\n",
       "      <td>0.296336</td>\n",
       "      <td>0.429794</td>\n",
       "      <td>0.457342</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.184959</td>\n",
       "      <td>4.310203</td>\n",
       "      <td>0.469122</td>\n",
       "      <td>0.661571</td>\n",
       "      <td>0.719676</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.735483</td>\n",
       "      <td>0.806850</td>\n",
       "      <td>0.453775</td>\n",
       "      <td>0.542926</td>\n",
       "      <td>0.521262</td>\n",
       "      <td>0.461086</td>\n",
       "      <td>0.286440</td>\n",
       "      <td>0.412142</td>\n",
       "      <td>0.460283</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.204101</td>\n",
       "      <td>4.310496</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>0.672245</td>\n",
       "      <td>0.714447</td>\n",
       "      <td>0.920404</td>\n",
       "      <td>0.722384</td>\n",
       "      <td>0.805514</td>\n",
       "      <td>0.474325</td>\n",
       "      <td>0.548542</td>\n",
       "      <td>0.531426</td>\n",
       "      <td>0.459214</td>\n",
       "      <td>0.276812</td>\n",
       "      <td>0.413747</td>\n",
       "      <td>0.457609</td>\n",
       "      <td>02:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.881109</td>\n",
       "      <td>4.370720</td>\n",
       "      <td>0.459732</td>\n",
       "      <td>0.683114</td>\n",
       "      <td>0.726821</td>\n",
       "      <td>0.936620</td>\n",
       "      <td>0.744168</td>\n",
       "      <td>0.820265</td>\n",
       "      <td>0.485246</td>\n",
       "      <td>0.559775</td>\n",
       "      <td>0.545066</td>\n",
       "      <td>0.462691</td>\n",
       "      <td>0.278952</td>\n",
       "      <td>0.386734</td>\n",
       "      <td>0.456807</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.955631</td>\n",
       "      <td>4.251304</td>\n",
       "      <td>0.444076</td>\n",
       "      <td>0.654888</td>\n",
       "      <td>0.713236</td>\n",
       "      <td>0.917204</td>\n",
       "      <td>0.725788</td>\n",
       "      <td>0.796112</td>\n",
       "      <td>0.488054</td>\n",
       "      <td>0.561113</td>\n",
       "      <td>0.555764</td>\n",
       "      <td>0.466970</td>\n",
       "      <td>0.287243</td>\n",
       "      <td>0.416422</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>02:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.876136</td>\n",
       "      <td>4.262345</td>\n",
       "      <td>0.465820</td>\n",
       "      <td>0.665276</td>\n",
       "      <td>0.708613</td>\n",
       "      <td>0.904972</td>\n",
       "      <td>0.718121</td>\n",
       "      <td>0.799542</td>\n",
       "      <td>0.481100</td>\n",
       "      <td>0.552019</td>\n",
       "      <td>0.538379</td>\n",
       "      <td>0.463493</td>\n",
       "      <td>0.289115</td>\n",
       "      <td>0.408131</td>\n",
       "      <td>0.461353</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#with experiment.train():\n",
    "#    cls_learn.fit_one_cycle(hyper_params[\"num_epochs1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web': 'https://www.comet.ml/api/image/download?imageId=657208724b184529aa8d58c894353ee5&experimentKey=0940d41d0c1e4e4e9c999b97b3dc1c81',\n",
       " 'api': 'https://www.comet.ml/api/rest/v1/image/get-image?imageId=657208724b184529aa8d58c894353ee5&experimentKey=0940d41d0c1e4e4e9c999b97b3dc1c81',\n",
       " 'imageId': '657208724b184529aa8d58c894353ee5'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = cls_learn.recorder.plot_losses()\n",
    "experiment.log_figure(figure_name=\"train loss 01\", figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.save('cls_learn.REGR1.DOCPOLL4ALL.200101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_learn.load('cls_learn.1200CLS2.191202')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Embedding(23008, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(23008, 400, padding_idx=1)\n",
      "  )\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      ")\n",
      "UN-FREEZING\n",
      "Sequential(\n",
      "  (0): Regr1Module1200(\n",
      "    (overall): Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=50, out_features=1, bias=True)\n",
      "    )\n",
      "    (aspect): Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=50, out_features=6, bias=True)\n",
      "      (6): Softmax(dim=1)\n",
      "    )\n",
      "    (sentiment): Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Linear(in_features=50, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cls_learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mse_0</th>\n",
       "      <th>mse_1</th>\n",
       "      <th>mse_2</th>\n",
       "      <th>mse_3</th>\n",
       "      <th>mse_4</th>\n",
       "      <th>mse_5</th>\n",
       "      <th>multi_regr_acc</th>\n",
       "      <th>regr_acc0</th>\n",
       "      <th>regr_acc1</th>\n",
       "      <th>regr_acc2</th>\n",
       "      <th>regr_acc3</th>\n",
       "      <th>regr_acc4</th>\n",
       "      <th>regr_acc5</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.799436</td>\n",
       "      <td>4.260633</td>\n",
       "      <td>0.457997</td>\n",
       "      <td>0.656352</td>\n",
       "      <td>0.710068</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>0.735312</td>\n",
       "      <td>0.787705</td>\n",
       "      <td>0.485290</td>\n",
       "      <td>0.541054</td>\n",
       "      <td>0.554694</td>\n",
       "      <td>0.465900</td>\n",
       "      <td>0.278952</td>\n",
       "      <td>0.408933</td>\n",
       "      <td>0.473121</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.417687</td>\n",
       "      <td>4.082174</td>\n",
       "      <td>0.448406</td>\n",
       "      <td>0.631651</td>\n",
       "      <td>0.680468</td>\n",
       "      <td>0.860865</td>\n",
       "      <td>0.702469</td>\n",
       "      <td>0.758315</td>\n",
       "      <td>0.486806</td>\n",
       "      <td>0.569404</td>\n",
       "      <td>0.541321</td>\n",
       "      <td>0.470447</td>\n",
       "      <td>0.304894</td>\n",
       "      <td>0.397165</td>\n",
       "      <td>0.463760</td>\n",
       "      <td>04:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.402441</td>\n",
       "      <td>3.951401</td>\n",
       "      <td>0.409222</td>\n",
       "      <td>0.597708</td>\n",
       "      <td>0.645708</td>\n",
       "      <td>0.868459</td>\n",
       "      <td>0.686118</td>\n",
       "      <td>0.744185</td>\n",
       "      <td>0.499733</td>\n",
       "      <td>0.578497</td>\n",
       "      <td>0.570473</td>\n",
       "      <td>0.470179</td>\n",
       "      <td>0.303557</td>\n",
       "      <td>0.406258</td>\n",
       "      <td>0.501471</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.162312</td>\n",
       "      <td>3.898916</td>\n",
       "      <td>0.371607</td>\n",
       "      <td>0.576505</td>\n",
       "      <td>0.646734</td>\n",
       "      <td>0.873216</td>\n",
       "      <td>0.685574</td>\n",
       "      <td>0.745279</td>\n",
       "      <td>0.515646</td>\n",
       "      <td>0.600695</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.489168</td>\n",
       "      <td>0.313720</td>\n",
       "      <td>0.395293</td>\n",
       "      <td>0.465365</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.866946</td>\n",
       "      <td>3.827533</td>\n",
       "      <td>0.389891</td>\n",
       "      <td>0.559167</td>\n",
       "      <td>0.649768</td>\n",
       "      <td>0.839925</td>\n",
       "      <td>0.670435</td>\n",
       "      <td>0.718347</td>\n",
       "      <td>0.516716</td>\n",
       "      <td>0.568066</td>\n",
       "      <td>0.600160</td>\n",
       "      <td>0.494785</td>\n",
       "      <td>0.323348</td>\n",
       "      <td>0.464830</td>\n",
       "      <td>0.527949</td>\n",
       "      <td>04:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.784092</td>\n",
       "      <td>3.798485</td>\n",
       "      <td>0.349788</td>\n",
       "      <td>0.568773</td>\n",
       "      <td>0.643759</td>\n",
       "      <td>0.830500</td>\n",
       "      <td>0.669239</td>\n",
       "      <td>0.736426</td>\n",
       "      <td>0.531024</td>\n",
       "      <td>0.638673</td>\n",
       "      <td>0.595881</td>\n",
       "      <td>0.490773</td>\n",
       "      <td>0.323081</td>\n",
       "      <td>0.426050</td>\n",
       "      <td>0.482749</td>\n",
       "      <td>04:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.645323</td>\n",
       "      <td>3.646075</td>\n",
       "      <td>0.350809</td>\n",
       "      <td>0.545268</td>\n",
       "      <td>0.605300</td>\n",
       "      <td>0.804737</td>\n",
       "      <td>0.649145</td>\n",
       "      <td>0.690816</td>\n",
       "      <td>0.529776</td>\n",
       "      <td>0.635197</td>\n",
       "      <td>0.605777</td>\n",
       "      <td>0.493715</td>\n",
       "      <td>0.334314</td>\n",
       "      <td>0.463493</td>\n",
       "      <td>0.511634</td>\n",
       "      <td>04:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.572599</td>\n",
       "      <td>3.677935</td>\n",
       "      <td>0.343300</td>\n",
       "      <td>0.549174</td>\n",
       "      <td>0.618993</td>\n",
       "      <td>0.823154</td>\n",
       "      <td>0.642459</td>\n",
       "      <td>0.700855</td>\n",
       "      <td>0.530891</td>\n",
       "      <td>0.625301</td>\n",
       "      <td>0.612731</td>\n",
       "      <td>0.481680</td>\n",
       "      <td>0.320674</td>\n",
       "      <td>0.430864</td>\n",
       "      <td>0.504146</td>\n",
       "      <td>04:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.485589</td>\n",
       "      <td>3.632818</td>\n",
       "      <td>0.332642</td>\n",
       "      <td>0.529108</td>\n",
       "      <td>0.615968</td>\n",
       "      <td>0.817624</td>\n",
       "      <td>0.643289</td>\n",
       "      <td>0.694186</td>\n",
       "      <td>0.540296</td>\n",
       "      <td>0.640546</td>\n",
       "      <td>0.622091</td>\n",
       "      <td>0.504680</td>\n",
       "      <td>0.327895</td>\n",
       "      <td>0.443434</td>\n",
       "      <td>0.508960</td>\n",
       "      <td>04:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.329951</td>\n",
       "      <td>3.606784</td>\n",
       "      <td>0.360438</td>\n",
       "      <td>0.525102</td>\n",
       "      <td>0.595009</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.636025</td>\n",
       "      <td>0.684498</td>\n",
       "      <td>0.535616</td>\n",
       "      <td>0.630115</td>\n",
       "      <td>0.613266</td>\n",
       "      <td>0.497192</td>\n",
       "      <td>0.335384</td>\n",
       "      <td>0.449318</td>\n",
       "      <td>0.521797</td>\n",
       "      <td>04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.318602</td>\n",
       "      <td>3.584395</td>\n",
       "      <td>0.342470</td>\n",
       "      <td>0.524202</td>\n",
       "      <td>0.595809</td>\n",
       "      <td>0.807037</td>\n",
       "      <td>0.629980</td>\n",
       "      <td>0.684896</td>\n",
       "      <td>0.532629</td>\n",
       "      <td>0.630650</td>\n",
       "      <td>0.616475</td>\n",
       "      <td>0.487296</td>\n",
       "      <td>0.329767</td>\n",
       "      <td>0.441562</td>\n",
       "      <td>0.509762</td>\n",
       "      <td>04:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.314388</td>\n",
       "      <td>3.595933</td>\n",
       "      <td>0.343286</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>0.597588</td>\n",
       "      <td>0.810935</td>\n",
       "      <td>0.632740</td>\n",
       "      <td>0.688783</td>\n",
       "      <td>0.536552</td>\n",
       "      <td>0.641615</td>\n",
       "      <td>0.622894</td>\n",
       "      <td>0.489971</td>\n",
       "      <td>0.333512</td>\n",
       "      <td>0.443969</td>\n",
       "      <td>0.514844</td>\n",
       "      <td>04:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle(hyper_params[\"num_epochs2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/aeryen/2019nn/0940d41d0c1e4e4e9c999b97b3dc1c81\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     sys.cpu.percent.01 [2]              : (6.2, 6.3)\n",
      "COMET INFO:     sys.cpu.percent.02 [2]              : (3.0, 5.8)\n",
      "COMET INFO:     sys.cpu.percent.03 [2]              : (2.5, 5.2)\n",
      "COMET INFO:     sys.cpu.percent.04 [2]              : (2.5, 3.9)\n",
      "COMET INFO:     sys.cpu.percent.05 [2]              : (4.9, 7.1)\n",
      "COMET INFO:     sys.cpu.percent.06 [2]              : (6.9, 11.3)\n",
      "COMET INFO:     sys.cpu.percent.07 [2]              : (3.4, 6.1)\n",
      "COMET INFO:     sys.cpu.percent.08 [2]              : (2.0, 5.4)\n",
      "COMET INFO:     sys.cpu.percent.09 [2]              : (2.0, 6.7)\n",
      "COMET INFO:     sys.cpu.percent.10 [2]              : (5.4, 9.2)\n",
      "COMET INFO:     sys.cpu.percent.11 [2]              : (4.4, 24.4)\n",
      "COMET INFO:     sys.cpu.percent.12 [2]              : (4.4, 5.5)\n",
      "COMET INFO:     sys.cpu.percent.avg [2]             : (5.633333333333333, 6.408333333333334)\n",
      "COMET INFO:     sys.gpu.0.free_memory [3]           : (24975966208.0, 24987500544.0)\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization [3]       : (3.0, 4.0)\n",
      "COMET INFO:     sys.gpu.0.total_memory              : (25373310976.0, 25373310976.0)\n",
      "COMET INFO:     sys.gpu.0.used_memory [3]           : (385810432.0, 397344768.0)\n",
      "COMET INFO:     sys.load.avg [2]                    : (0.56, 0.62)\n",
      "COMET INFO:     sys.ram.total [2]                   : (16703754240.0, 16703754240.0)\n",
      "COMET INFO:     sys.ram.used [2]                    : (6408376320.0, 6686097408.0)\n",
      "COMET INFO:     train_curr_epoch [20]               : 11\n",
      "COMET INFO:     train_loss [698]                    : (2.2040047645568848, 21.70929718017578)\n",
      "COMET INFO:     train_mse_0 [20]                    : (0.3326423168182373, 0.7608822584152222)\n",
      "COMET INFO:     train_mse_1 [20]                    : (0.5225998163223267, 0.7848777174949646)\n",
      "COMET INFO:     train_mse_2 [20]                    : (0.5950087308883667, 0.8462477326393127)\n",
      "COMET INFO:     train_mse_3 [20]                    : (0.8047369718551636, 1.0361685752868652)\n",
      "COMET INFO:     train_mse_4 [20]                    : (0.6299803256988525, 0.8947240710258484)\n",
      "COMET INFO:     train_mse_5 [20]                    : (0.6844977140426636, 0.9952375292778015)\n",
      "COMET INFO:     train_multi_regr_acc [20]           : (0.39966118335723877, 0.5402960181236267)\n",
      "COMET INFO:     train_regr_acc0 [20]                : (0.4330034852027893, 0.6416153907775879)\n",
      "COMET INFO:     train_regr_acc1 [20]                : (0.445038765668869, 0.6228938102722168)\n",
      "COMET INFO:     train_regr_acc2 [20]                : (0.41401442885398865, 0.5046803951263428)\n",
      "COMET INFO:     train_regr_acc3 [20]                : (0.26584649085998535, 0.335383802652359)\n",
      "COMET INFO:     train_regr_acc4 [20]                : (0.342070072889328, 0.4648301601409912)\n",
      "COMET INFO:     train_regr_acc5 [20]                : (0.39769992232322693, 0.5279486775398254)\n",
      "COMET INFO:     train_sys.cpu.percent.01 [58]       : (9.7, 41.3)\n",
      "COMET INFO:     train_sys.cpu.percent.02 [58]       : (2.2, 52.9)\n",
      "COMET INFO:     train_sys.cpu.percent.03 [58]       : (2.2, 39.6)\n",
      "COMET INFO:     train_sys.cpu.percent.04 [58]       : (3.1, 45.6)\n",
      "COMET INFO:     train_sys.cpu.percent.05 [58]       : (2.0, 50.7)\n",
      "COMET INFO:     train_sys.cpu.percent.06 [58]       : (2.2, 64.9)\n",
      "COMET INFO:     train_sys.cpu.percent.07 [58]       : (1.7, 31.1)\n",
      "COMET INFO:     train_sys.cpu.percent.08 [58]       : (2.2, 56.7)\n",
      "COMET INFO:     train_sys.cpu.percent.09 [58]       : (2.1, 57.4)\n",
      "COMET INFO:     train_sys.cpu.percent.10 [58]       : (2.1, 38.7)\n",
      "COMET INFO:     train_sys.cpu.percent.11 [58]       : (2.0, 54.4)\n",
      "COMET INFO:     train_sys.cpu.percent.12 [58]       : (1.8, 40.8)\n",
      "COMET INFO:     train_sys.cpu.percent.avg [58]      : (6.658333333333334, 13.766666666666666)\n",
      "COMET INFO:     train_sys.gpu.0.free_memory [65]    : (9651027968.0, 18239848448.0)\n",
      "COMET INFO:     train_sys.gpu.0.gpu_utilization [65]: (14.0, 100.0)\n",
      "COMET INFO:     train_sys.gpu.0.used_memory [65]    : (7133462528.0, 15722283008.0)\n",
      "COMET INFO:     train_sys.load.avg [58]             : (0.73, 2.07)\n",
      "COMET INFO:     train_sys.ram.total [58]            : (16703754240.0, 16703754240.0)\n",
      "COMET INFO:     train_sys.ram.used [58]             : (8795377664.0, 9166315520.0)\n",
      "COMET INFO:     train_val_loss [20]                 : (3.584395408630371, 5.318137168884277)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     figures  : 2\n",
      "COMET INFO:     git-patch: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web': 'https://www.comet.ml/api/image/download?imageId=43ea27c68d2a4011be3205614e1db743&experimentKey=0940d41d0c1e4e4e9c999b97b3dc1c81',\n",
       " 'api': 'https://www.comet.ml/api/rest/v1/image/get-image?imageId=43ea27c68d2a4011be3205614e1db743&experimentKey=0940d41d0c1e4e4e9c999b97b3dc1c81',\n",
       " 'imageId': '43ea27c68d2a4011be3205614e1db743'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3hUVfrA8e9J7wkJIZQIofcWiiJdFGk2QMWyKq4N1172h7r2VXHdVexdsCBYUalKR0TpvQcILZAQSkhIT87vj3unJTNpzKQw7+d55smde+/cOXOVeee09yitNUIIIbyXT00XQAghRM2SQCCEEF5OAoEQQng5CQRCCOHlJBAIIYSX86vpAlRW/fr1dUJCQk0XQwgh6pR169ala61jnR2rc4EgISGBtWvX1nQxhBCiTlFKHXB1zKOBQCmVDGQCRUCh1rpnieODgJ+B/eauH7XWL3iyTEIIIRxVR41gsNY6vYzjv2utR1VDOYQQQjghncVCCOHlPF0j0MBvSikNfKi1/sjJOX2UUpuAFOAxrfW2kicope4C7gJo2rSpJ8srhKhGBQUFHD58mNzc3JouynkjKCiI+Ph4/P39K/wa5clcQ0qpxlrrFKVUA2ABcL/Wernd8QigWGudpZQaAbyptW5d1jV79uyppbNYiPPD/v37CQ8PJyYmBqVUTRenztNac+LECTIzM2nevLnDMaXUupL9tBYebRrSWqeYf9OAmUDvEsfPaK2zzO25gL9Sqr4nyySEqD1yc3MlCLiRUoqYmJhK17A8FgiUUqFKqXDLNjAU2FrinIbK/D9AKdXbLM8JT5VJCFH7SBBwr6rcT0/2EcQBM81C+QFfa63nK6XuAdBafwCMBSYopQqBHGCc9lBb1e7UTGZvSuGWixOoHxboibcQQog6yWOBQGu9D+jqZP8HdtvvAO94qgz29qRm8dbiJEZ1bSyBQAgBwIkTJxgyZAgAx44dw9fXl9hYY/Lt6tWrCQgIKPca48ePZ+LEibRt29ajZfWkOjezWAgh3CUmJoaNGzcC8NxzzxEWFsZjjz3mcI7WGq01Pj7OW9KnTJni8XJ6mswjEEKIEpKSkujUqRP33HMPiYmJHD16lLvuuouePXvSsWNHXnjBlgChX79+bNy4kcLCQqKiopg4cSJdu3alT58+pKWl1eCnqDivqxHIypxC1E7Pz9rG9pQzbr1mh8YRPHtFxyq9dvv27UyZMoUPPjBasydNmkR0dDSFhYUMHjyYsWPH0qFDB4fXZGRkMHDgQCZNmsQjjzzCZ599xsSJE8/5c3ia19QIZGCCEKIyWrZsSa9evazPp0+fTmJiIomJiezYsYPt27eXek1wcDDDhw8HoEePHiQnJ1dXcc+J19UIhBC1U1V/uXtKaGiodXvPnj28+eabrF69mqioKG6++WanY/XtO5d9fX0pLCyslrKeK6+pEQghRFWdOXOG8PBwIiIiOHr0KL/++mtNF8mtvK5GoJFOAiFE5SQmJtKhQwc6depEixYt6Nu3b00Xya08mmvIE6qaa2jelqNMmLae+Q/1p13DCA+UTAhRWTt27KB9+/Y1XYzzjrP7WmO5hoQQQtR+EgiEEMLLeV0gqGMtYUII4XFeEwgs8wgkEAghhCOvCQRrkk8B8OVfB2q4JEIIUbt4TSA4cOIsAJsPn67hkgghRO3iNYHAslhDsTQNCSFMgwYNKjU5bPLkydx7770uXxMWFgZASkoKY8eOdXnd8oa5T548mezsbOvzESNGcPp0zfxQ9Z5AYP6ta/MmhBCec8MNNzBjxgyHfTNmzOCGG24o97WNGzfm+++/r/J7lwwEc+fOJSoqqsrXOxfeEwiks1gIUcLYsWOZPXs2eXl5ACQnJ5OSkkK3bt0YMmQIiYmJdO7cmZ9//rnUa5OTk+nUqRMAOTk5jBs3ji5dunD99deTk5NjPW/ChAnW9NXPPvssAG+99RYpKSkMHjyYwYMHA5CQkEB6ejoAr7/+Op06daJTp05MnjzZ+n7t27fnzjvvpGPHjgwdOtThfc6F16SY8LE2DUkkEKJWmjcRjm1x7zUbdobhk1wejomJoXfv3syfP5+rrrqKGTNmcP311xMcHMzMmTOJiIggPT2diy66iCuvvNLlesDvv/8+ISEhbN68mc2bN5OYmGg99tJLLxEdHU1RURFDhgxh8+bNPPDAA7z++ussWbKE+vXrO1xr3bp1TJkyhVWrVqG15sILL2TgwIHUq1ePPXv2MH36dD7++GOuu+46fvjhB26++eZzvk1eUyOQQCCEcMa+ecjSLKS15sknn6RLly5ceumlHDlyhNTUVJfXWL58ufULuUuXLnTp0sV67NtvvyUxMZHu3buzbds2p+mr7a1YsYJrrrmG0NBQwsLCGD16NL///jsAzZs3p1u3boB701x7TY3A0kkgYUCIWqqMX+6edPXVV/PII4+wfv16cnJySExMZOrUqRw/fpx169bh7+9PQkKC07TT9pzVFvbv389///tf1qxZQ7169bjtttvKvU5Z/ZiBgbb11n19fd3WNOQ1NQLrfyKJBEIIO2FhYQwaNIjbb7/d2kmckZFBgwYN8Pf3Z8mSJRw4UPb8owEDBjBt2jQAtm7dyubNmwEjfXVoaCiRkZGkpqYyb94862vCw8PJzMx0eq2ffvqJ7Oxszp49y8yZM+nfv7+7Pq5TXlMjkKYhIYQrN9xwA6NHj7Y2Ed10001cccUV9OzZk27dutGuXbsyXz9hwgTGjx9Ply5d6NatG7179waga9eudO/enY4dO5ZKX33XXXcxfPhwGjVqxJIlS6z7ExMTue2226zXuOOOO+jevbtHVzvzmjTUD83YwE8bU2gWE8Kyxwd7oGRCiMqSNNSeIWmoXbC039WxuCeEEB7nPYHA/CtNQ0II4ch7AoHUCISolepa83RtV5X76TWBwNf8pEWSbEiIWiMoKIgTJ05IMHATrTUnTpwgKCioUq/zmlFDvj4yakiI2iY+Pp7Dhw9z/Pjxmi7KeSMoKIj4+PhKvcZrAoGS4aNC1Dr+/v40b968povh9bynacgMBNI0JIQQjrwnEPjIegRCCOGM1wQCSxqQYokEQgjhwGsCga/0EQghhFNeEwh8zKahIgkEQgjhwHsCgaVGUFzDBRFCiFrGo4FAKZWslNqilNqolCqVKU4Z3lJKJSmlNiulEp1dxx0sE8qkaUgIIRxVxzyCwVrrdBfHhgOtzceFwPvmX7dTSB+BEEI4U9NNQ1cBX2jDX0CUUqqRJ99QBg0JIYQjTwcCDfymlFqnlLrLyfEmwCG754fNfQ6UUncppdYqpdZWdSq6izWnhRDC63k6EPTVWidiNAH9Qyk1oMRxZ1/PpX6za60/0lr31Fr3jI2NrVJBpEVICCGc82gg0FqnmH/TgJlA7xKnHAYusHseD6R4skwAxzPzPP0WQghRZ3gsECilQpVS4ZZtYCiwtcRpvwC3mKOHLgIytNZHPVUmiyW70jz9FkIIUWd4ctRQHDDTzPrpB3yttZ6vlLoHQGv9ATAXGAEkAdnAeA+Wx0oSzwkhhI3HAoHWeh/Q1cn+D+y2NfAPT5XBlUIJBEIIYVXTw0drRFGRTC8WQggLrwwEQgghbLwyEEjDkBBC2HhlIBBCCGHjlYHgw2X7yC0oquliCCFEreCVgeDYmVzeW7q3poshhBC1glcGAoAzOQVuv6bWmoXbUyksKiYtM5cLX17IntRMt7+PEEK4k9cGAu2B5ENLdx/nji/W8u6SvSzakUbqmTw+/n2fR95LCCHcxWsDwfEs9+cbemdxEgA/bzpCSIAvAN+uPUzzJ+a6/b2EEMJdvDYQ5BW4f1JZk6hg699gf1+HY4UyiU0IUUt5bSDwxEpliU2jAPh9TzqBJQLB2XwZpSSEqJ28JhDoEtPInOUbSkrL5L6v15OZW7WOZF8f2/IKczY7ZtM+m1dYpWsKIYSnVceaxbVSyQykWmsufX05AG3jwrl/SOtKXzPHbm7Ct2sPOxzLkkAghKilvKZGoEoshlZy6crv1tm+uDWQW1BEwsQ5fLCs4vMNXp670+UxCQRCiNrKawJByaahxKb1HJ7vTz9r3T6akWv94p40z/WXu8P1y+lzyMqVQCCEqJ28JhCUVLKzuKDQNqpn+uqDDk1Hf+49Ue71ylvjoDJ9BK8v2E3CxDmsO3Cqwq8RQoiq8tpA8O4SW5NPXmERn6zY73B82qqD1u0bPv6LrUcyrM+11iRMnEOPFxcAMPKt32n91DwA+reuzyujO1vPfehSo68h00kgeHvRHhImzinVX/HWoj0APPtLyZU9hRDC/bwmEJTsI7D3zZpDpfZZvowtTpzNt27P3HDEuu/XbcfYlnLGemxox4a0axhufX7zRc0AW41gbfJJEibOYeof+/nfgt2AEYic2XrkjNP9QgjhTl4TCOz7CLrER9I7Idr6PCO7/OGiP204Yu0HiAz2t+5fs/+kw3kxoQG0qB8GQLcLoogyz31+1nbSs/IY+8GfADw3a7v1NbkemNwmhBAV5TWBwF6Qn6/Dr/D46GDrdp8WMU5fM3PDEWvHsY/dfIHtRx1/tSsgMsSf5Ekj+ekfffHztd3inv9e6PTawyYvd3jeuUlkxT6IEEK4gVcGgkB/H/LsOocf/mYTAFPH9+LPfa47hj9cvo9Nh047ZC5dWaIjOSYssNLlSct0zHtk3/EsCeuEEJ7mlYEgMtif/MLSzTFxEUG0iA0t87Unz+bz176TLo/3bh7t8lhFFRXbyibNRkIIT/O6QHBl18YE+vk61Ags/H19+OGei63Pb+3TrNQ5c7YcZfrqg6X2l2XVk0Mqdb59jeBsvsw/EEJ4ltcFgjZxYQ5NQwV2WUED/XyoFxpgfZ7YrB7T77zI4fXfr3NMHVERcRFBzLqvn/X5f8Z24Z/D2jqck5SWBcA9X65j33Hb5DaZiCaE8DSvCwQAAb4+5Judxa/9usu6v2FkEADX97wAgFFdGtOnZQwLHh7Anf2bl3nN2ff3Y8HDA1web1Y/xLrdo1k9JgxsSdJLw2lkvuc7i43hqvO3HXN43VuLHYexCiGEu3llILCvEXzy+z7rfn9zhM8rozuz88Vh1myirePCrfMBLHo0c0xR0alJJK3jwnElNMCW3y86JAClFH6+PgT6Ge9Z5KJP+Mf1Ryr4qYQQomq8MxCYfQQ3fPQXzjJD+PgogkqsJ9AsxrET+dUxXXjpmk4ALHt8ULnv6eujuHtgC3o2q+cwD8HyPrlm5tI2ccYchNev62o9R2vNtFUHyJE1DYQQHuCVgeAvc4hoWUNFyxMa6MtNFzYjedLIUkHClSeGt+f7CRc7zEMIMGsElhpKsL8vA9vEMjoxHoDRiU1Ytvs4T83cyivzdlS5vPbSzuQyu8R6CUII7+U1geDqbk0AGNG5EZe0a1Cla/wwwTaiKMTfPUs5WH75Rwb7U1ys2XQ4g33pWdbjszalWGsLKadz3fKe46eu4b6vN9DnlUVuuZ4Qom7zmkDQOi6c5EkjaREbxt0DWpQ6Prp7k3KvYd8vEBzgW8aZFdeqgdGvMGtTCr1eMmYeHzqZYz1eUKSty14u3JF6Tu+17sAp0jJzrbOhj2a4J7AIIeo2r1yhTJVclQZ4amT7Sl3D0qTjTvaJ7QA6No5gW8oZt/UNjHl/JXERgdhPVs4rLCLQzz1BTQhRN3lNjaA8fj6171YM7dAQgEe/3WTdVzJldUUt3ZUGQOqZPIfV2famnXXxCiGEt6h93341JDLEv/yTcJ2Uzp1m329MPosINips9mshV3XJy9umrLFu/81uKGyuixTYQgjv4bWBwMf18gRl+uLvvdn+wuVuLctvdhPRHrmsDZ3M7KP2w0wt3LH2sf3qbBk55afgFkKc37w2EPxnrG2cfrB/xdvI/X19CAlwb9dKm7hwwgONa9a3y14aEeQkELgh5URBoS0QjLerKQghvJPHO4uVUr7AWuCI1npUiWO3Aa8Blumz72itP/F0mQDG9ohnbI949qRmEhUSUP4LPMwyj8DPrqpi31x128UJTF2ZTFbeuf+CLyguJtDPx2niPSGE96mOGsGDQFkzob7RWnczH9USBOy1jgsnNrzyawi4W76Z/G7HMdtCN/Y1go6NIwDIyqt8m759Yr1gf18KijQxdsn1dqdmVvqaQojzh0cDgVIqHhgJVPsXfF3zwc2JgJEm2yI00NZk1bKBkXoiKS3Luv5xRVmak4L9fckpKGLWphRS7OYQpJ6R+QRCeDNP1wgmA/8EymqDGKOU2qyU+l4pdYGzE5RSdyml1iql1h4/ftwjBa1pwzo1InnSSLo3tU1as6+pNIwwspS+OHs717z3R4WumZx+lr9PXUNqpvFFbz/6CODGC5sCxmI7Qgjv5bFAoJQaBaRprdeVcdosIEFr3QVYCHzu7CSt9Uda655a656xsbEeKG3tZD/RKzTQ1p2zO9WWgmLFnnR+3ug8Q+nzs7axaGcaV71jBI5OTSIcjj96WRsAHpyx0W1lFkLUPZ7sLO4LXKmUGgEEARFKqa+01jdbTtBa22d9+xh41YPlqdPCAkv/p0qYOMe6fWn7OFLP5NIiNsy6zzI2yNIp/I9BrZgwbT1gZDl1NjxVCOF9PBYItNZPAE8AKKUGAY/ZBwFzfyOt9VHz6ZWU3anslWbcdRFn8wqtayNYPDB9g8PzO79Yy8q9J9j/yghrCo2luxyb0RpFBVu3L+sQh5+vrUJYVKxLvYcQwjtUe64hpdQLwFqt9S/AA0qpK4FC4CRwW3WXp7a7yMVM5l82OaaRXrnXqFxl5RUSHuRPvpOhofXshqPm5BvHE5tGsf7gaXIKipzWOoQQ579qmVCmtV5qmUOgtX7GDAJorZ/QWnfUWnfVWg/WWu+sjvLUVSsnXlLuOV/8eQCANxftLnUs3G44qqXjeEwPY92DbDfMWBZC1E1eO7O4Lmps17TjimUN5rNO5huEB9l+8VvmFgSZHdJJx7NKnS+E8A4SCOqYvq0qlvSugzkB7VrzFz8Y6TGamMHE1+xH2GVOJrvx41UO6a43HDzFhK/WoXXVsp0KIeqOCgUCpVRLpVSguT1IKfWAUirKs0UTznx2Wy+H54+YQ0BLsowUenxYW4dOYMuKaN2aGv/5ou1mGLd/Zj4bD50mI6eAa95bybytx7jx41VuLb8QovapaI3gB6BIKdUK+BRoDnztsVIJlwL9fPng5h70a1Wfbc9fzvW9nM7BI8/sAwjy92XlxEuY84CR2vrCFjEsfGQA48zX3XZxgsPrrn73D1bZreV8Lus6CyHqhooOEynWWhcqpa4BJmut31ZKbSj3VcIjhnVqyLBOxqI1fr7Oh3xaagSBfj5EBPkTZ85MBtvymGAEirIS0F3WIc5dxRZC1FIVrREUKKVuAG4FZpv7ZDZSLWA/+/jS9g0AOJGVx2/bjgEQ4Fv+f2L7hWpiwwPJtusrWLD93NZJFkLUfhUNBOOBPsBLWuv9SqnmwFeeK5aorBb1Q9mfbiw7OWPNITYdzgCcr89c0sTh7azb9cMCOZPrmOr6ni+l01iI81mFAoHWervW+gGt9XSlVD0gXGs9ycNlExWUPGkkix8bxH/GdgGgZWxopV5vP8N417Ez1iR0dw9sAcD8bcdYYq55LIQ4/1R01NBSpVSEUioa2ARMUUq97tmiicqy9ANUZfnJNU9dysA2sRRrmLxwDwDzthyzHs8vlBqBEOerijYNRWqtzwCjgSla6x7ApZ4rlqgKSxK5//thC1C5jt7Y8ECW7XbMTXRLH7tF7gtkkXshzlcVDQR+SqlGwHXYOotFLVMyV5B9bqGKmPdgf4fnoxNtk9HWHzxV9YIJIWq1igaCF4Bfgb1a6zVKqRbAHs8VS1RFyY7hU9mVayJq3yiCq7sZK6TFhAYQHRpA1wuMiWeWHEZCiPNPRTuLv9Nad9FaTzCf79Naj/Fs0cS58qtCWulXx3ahflggL17dCYAfJ1zs7mIJIWqZinYWxyulZiql0pRSqUqpH8z1iEUtM8UuBUXJWcMVEejny9p/XcqIzo0A8PVRRFWyiUkIUbdUtGloCvAL0BhogrHE5BRPFarOKi6Gr8fB2inGdg0Y1DaWVg3C+O6ePlzoYi2Dyrq1TwIAxcVljxySuQZC1E0VDQSxWuspWutC8zEV8J7FgysqLwPys2D2Q/DZ5XBsS7UXQSnFwkcG0ish2m3XDA00Zi/nlBg5pLVmwfZUioo193y5juZPzHXbewohqk9FA0G6UupmpZSv+bgZkGxkJQXXg1tnwdUfwMm98OFA+PUpyKvbuf5DAozRSGdLLF5z5xdrufOLtTw/axvztx1z9lIhRB1Q0UBwO8bQ0WPAUWAsRtoJUZJS0O0GuG8tdL8Z/nwH3r0Qds4p/7W1lKVGMGme4wJyhWZTkY/daKW9lVjgZmVSusMaCEKImlHRUUMHtdZXaq1jtdYNtNZXY0wuE66ERMOVb8Htv0JQBMy4EabfAKcP1nTJKs3Px/jf5McNRxz2dzQXv6kXYlvT4JW5O8rtSwBYvf8kN36yivbPzHdjSYUQVXEuK5Q94rZSnM+aXgR3L4fLXoB9S43awR9vQVHl00DUlJAAX6f7LTUC+yR1C3eklQoYxzPz2JaS4bDvug//dHMphRBVdS6BoPKD1L2Vrz/0fRD+sQqaD4QFTxv9BwfrxupfQ9rbUlXsPHbGup1XYIyMOl1i4tpj321yeN7rpYWMfGtFqaymFpku9gshqse5BAIZK1hZUU3hhulw/TTIPQ2fDYVfHoDskzVdsnI9ai6JOWzy74AxlHTqymQAflh/uELXSDbTZJf0ye/7+XbtoXMvpBCiSsoMBEqpTKXUGSePTIw5BaKylIL2o+Afq6HPfbDhK3inF2yaAbV4HH7JSWUf/b6vzPO/WWP0hdhnQrWkvEg7k+tw7puL9vDP7ze7o5hCiCooMxBorcO11hFOHuFa64oucymcCQyDy1+Cu5dBdHOYeTd8fgUc313TJXPK327NghV70kuNIAK4todtsrklA6r9kNMPlu4FoPfLiwC4s39zj5RVCFE559I0JNyhYWe4/TcY9QYc2wzvXwyL/w0FOTVdMgeB/rb/VW7+1HnfxsTh7Xj88rYO+/Lt1kL+c98JEibahtFm5BQ45EOqyGgjIYT7SSCoDXx8oOftxtyDjtfA8tfgvT6QtKimS2bVIDzI6f53b0y0bocG+hETahtKWlBUzNDJy11ec9X+k3RqEml9XpUFdSzeWrSHNv+aR5EEEyEqTQJBbRLWAMZ8DLf8DD6+8NVo+G48ZNb8rN2LW8bwv2u7ltrfJd72RR7k70uQv22oaeun5jnUCEr6ccLFnLH78t+dmlnl8r2+YDf5hcXsOHqm/JOFEA4kENRGLQbBhJUw6EljRvI7vWD1x1Bcc7NwlVKM6eGYcPbNcd24IDqE5EkjSZ40EoBAv4r9L7XluaHEhAWyz24k0e1T15xzOZNPOB+ZJIRwTQJBbeUXCIP+D+79E5okwtzH4JMhkLKxRos1onND63bv5qUT27VsEOb0ddPuuJDXxnaxPg/0Kz1JbVC7Budcvvu+3nDO1xDC20ggqO1iWsLffoIxn0LGEfh4MMz7P8itmSaQN8d1B8DfV9EoMrjU8TZx4dQPCyy1P9DPhyZRtvP9fUvPRwzxdz6DuSIsy3L6VmExHiG8nQSCukAp6DwW7ltjdCqv+hDe6g5LX4Wz1ZsE1t/Xhynje/HH/13i8pzLO8Y5PG9eP5QezeoREWybi2BZVnPqeNtCOqey86tUpszcAuschW7m0ppCiIqTQFCXBEfByP/BHYugSQ9Y+jK80QFmPwIn9lZbMQa3bUCDCOejiACeHtWBH+yWuFz86ECUUkQElV7pbFBbW3PQwh1ppGXmUlBUuUV93l6cZN0+ebZqwUQIbyaBoC6K7wE3fQv3roLO18KGL+HtHjDjplqRvyjI35cezeoBxmgjy6//8CDncxCXPT7Iut37pUUV6jROO5NLwsQ5LNmZxs5jttFGEgiEqDyZHVyXNWgHV70DlzwNqz+CtZ/CztkQ3wsuvh/ajTKGodaQLc8NdegUtgSC56/s6HBes5hQh+e/70mnqFiX2d6/IikdgPElgkZGTgEFRcUOM6GFEGWTfy3ng/A4GPI0PLwNRvwXzh6Hb28xagmrP4b8mhlSGR7kT4DdcFI/Xx+SJ43k1osTSp379KgODs+PlchHVJJl1TR7CTEhAGw8dLoKpRXCe3k8EJhLW25QSs12cixQKfWNUipJKbVKKZXg6fKc1wJCofedcP96uO4LCIkxhp2+0dFIW5GVVtMldOmqbo45DPeVWOksr7CInPwiPlq+lwdnbOCer9aVusbILo0A+OqvA54rqBDnoepoGnoQ2AFEODn2d+CU1rqVUmoc8CpwfTWU6fzm4wsdroL2V8KhVbDybVj+X2NBnK7XG1lPY9uWf51qVHLI6TuLk3hx9nbeuymRVg3Cafuvslcym/tAfyJD/Hl3yV5+3phCz2b1+FufBA+WWIjzh0drBEqpeGAk8ImLU64CPje3vweGKKVkILi7KGWskDZumrmG8k2w+Vt4tzd8fT0kr6hVqa8v7xiHpVtg1f6T7E7N4s1FSWW/yNShcQShdiupPf3zNk8UUYjzkqebhiYD/wRcjQdsAhwC0FoXAhlATMmTlFJ3KaXWKqXWHj9+3FNlPb/Vb2VkOH14Gwx6Ag6vgakjjQlqW3+AosLyr+FhH9zcg70vj3DYN2tTCl+vqtg6z2GBjhVcXYuCnBC1mccCgVJqFJCmtS7dmGt3mpN9pf71aq0/0lr31Fr3jI2NdVsZvVJofRg00QgIo94wZih/f7sxQe2v9yGv6onfzpVSCmcVwidnbnH5mss7xvHcFUZHs5+vj0NivOz8msvNJERd4skaQV/gSqVUMjADuEQp9VWJcw4DFwAopfyASKD2r9t4PvAPtqW+Hvc1RDaB+RPh9Y6w4Fk4c7SmS1ghH/6tJ7f1tS1wM9RuVrP9TOWtRzJYvDO1WssmRF3hsUCgtX5Cax2vtU4AxgGLtdY3lzjtF+BWc3useY7U56uTjw+0Gwm3z4e/L4SWg2DlWzC5M/x4t7GUZsoGKCh7OKc7vTSyqfUAACAASURBVDqms9P98x/qz+5/D2fRowNdvjbYLl/RB8tss61Hvb2C26eudV8hhTiPVPuEMqXUC8BarfUvwKfAl0qpJIyawLjqLo+wc0EvuOALOLnPaCba+DVsnmEcU75QvzXEdYS4TsajYScIb2R0SrtRx8aRpfZ1jY+kXUNj4FnLWOcZTsFoHppx10WM++gvWtQvfd4nv+/jjv4t3FdYIc4D1RIItNZLgaXm9jN2+3OBa6ujDKISolvAiNdg2CQ4uR9St0DqNji2FQ6tMTqXLYKjjeDQsLMtSMS2A3/XuYjK4ywVRcm01Q8OaU3PhHpOX9813kg8l1NQuo/g33N2SCAQogRJMSFc8/E1RhvVb2UsoWmRcxrSthuBIdV8rJ0CheY6yyVrD5YgUcHaQ1SwsdxlTGgAJ8zcQQPbOg4SePiyNi5fHxzgS7C/L6eqIe9QTn4RX68+yO19E5x2dAtRF0ggEJUXHAXNLjYeFsVFbqs9RIb4s/CRgTSNDuGbNQeJCPbnii6OM4/LEx0a4DIBnTtzEXV5/lcKijRd4iPplVB6oR4h6gIJBMI9zqX20LATNOsHCX2haR8IiaaVudJZVWcHR4cGcNLF+gatn5rHkscG0bx+qNPjFXU0I4eCImNsw/HMvHO6lhA1SQKB8Kwyaw9b4dgWIw3G2k/hr3cBZdQUEvpCQj9o1hdCKv9LOyzQj7N5xiS5b9aUnpA2+L9LressV9Urc3dat++dtv6crydETZFAIKqfQ+3hamNfYR4cWWekvUheAes+h1UfGMcadDACgiUwhJU/qTA00I+U0zms2JPO//1gTEjrEh/J5sMZbvsYv2xKcXienpXndJlOIWo7CQSidvALtNUcBv4TCvON+QvJv8OBP4yhrGs+Ns6t39YICgl9jSal8LhSlwsN9CU7v5DP/0y27hvaIc4hEOQXFjukyXbmw2V7eWXeTpJeGo6f2a+QkV3Au0tL50B68sctfHRLz8p/diFqmKxHIGonvwBoeiEMeAz+NhMmHjAmvF36HEQ1NZLnfX87/K+Nse7CrAdh83dwxviVHhLgR1ZekcOcg5K/1m+fuoaiYtfzF9Myc3llntH8s3CHLYX3pPk7+Wj5PuvzUWb669+2p7InNZMHZ2wgv7Byy20KUZOkRiDqBl9/c8JbL+j3sJEk79gmSP7DqDFs/RHWTTXOjW5B75MJ5Oa35djBvoAPo7o0YmSXRnyyYj9JacZaByuS0tl57IzTCWz/mb+T95baZiZvPnyaYZ0aAjB9tWOfw+TruzF7s5GS47HvN7Pp0Gl+3pgifQaizpBAIOomXz9o0sN49H3A6IA+tsUICskrGHxiGdcELIaj7/N4UBxNAvrB8ib8lFifn5KK+HmvJpV6jHlrETsnjS51efsgYHl+Z/8W5Bc5/tJ/58bu+Pn6kNg0ipAAPw6dyvboxxbCEyQQiPODjy807mY8+vyD+auS+fynuVzks53BQXtocngNZM4mrCiPm4Gb7VuJXnkIwhuaj0YcKYpgvG8OqboeqboeaUSRpuvR/cUFpd62RzNjdnNooB+/70mvdLHP5hXioxTBATW3trQQEgjEeWlsz6ZMnNmMHUXNmHJ2OMlPjzQW4ck5BZnHuPnNX4jjFHHqFDe08qexXwa+WalwYCWxGSk86196fYbTOtQIDDqKNIwgEbP1AEQ15mzSfhoRw1G75TT2p58td65Cx2d/JSzQj63PX+72eyBERUkgEOclP2czh5Uy5iSERBPQ5iw/7DQ6gN/bahy2tOm/t2AXUxdtIE4ZgaKBOk0DM2jEqdM0UKcY4L+LGH0KnwW/APCjWcNYUdSR1wqvZ5Nuxe7UzDIDQZY5z8HyV4iaIoFAeKXPbuvFtR+sZE3yqVLHijRkqHCWPT2G0EBfWj01r9Q5i+4fSGxMCOSchMxj/LlpK8uXL+EOvzn8HPgM84t68dNv47m8401O3//PvSe46ZO/3P65hKgKGT4qvNbDlzpPXJdv5iKKDPHHz9eH929KLHWO1hhrOYTWh4adiO91Je8XXcmAvMl86j+Ovj5beef0P2DmBDh1oNTrb/j4L8oYuSpEtZJAIM5bb1xvLFv56a3OJ3l1jrcNGw00J5ZNW3WAD5ftc5gH0L9N6ZnM0aEBDs9jw422obME0/WmVxiQ9wafFo1Ab/3BmOcw95+QZTRFbThYuhZSLFFB1CBpGhLnrWu6x3NN93iXx8OD/K3beYXFFBYV89TMraXOC7Ub0bP/lREUFutS2UuD7FZG65kQzSkieLnwJsbd9zIRq/8Haz4xVnvrcy+3/NYWCHF4fWZuIZEh/ghRE6RGIIRp+Z7jTvcrpfjP2C7Mf6g/SimXKaznPtCflRMvAeDFqzsBkBkUB1e+Df9YDW2GwvLXWB74EHf5ziIQW3bUpOOZLstVVKx5b2mSdCoLj5FAILzawkcG4O9rLChT1prG1/W8wLpUpisdGkfQOCoYgOgQo+koK9f88q7fCq6dSsYtC9lU3JIn/aezLPBhbvRdhB+FTJq309VlGfTfJfxn/i7u/lLWXBaeIYFAeLVWDcKZfudFbr9uaKDRVFTyV3x+bBduK/g/rs97miO6Pi/7f8qCgMdpdHAOFDvPT3TopLF2wx9JJ9xeTiFAAoEQ9LRbWcyyXvIVXSu3IlpJluuMeX8luXZrJ/+2/RgAm/06MSb/OeZ1foNcAngr4B0K3usHu38zhyQ5N2/L0XMqlxDOSCAQAnjo0taA0Wl728UJvH1D93O6XligreP3sjeWWbd/3ZYKwK0XJ5A8aRTDx9zOyPxXeDD/XnLOZsDX18KU4XDgT6fX/WnjkXMqlxDOyKghIYB6IbbhoIH+5/77qFmMbVRQkJ9tRNHy3UaH9JV2NY5dL42k9VM+tOr+N+6P+gOWvwZThkHroXDJ0wxsE8sy83XZ+bbahdsUF0FWqpHC+8wRyDgC+VkQ3QLqt4aYVhBwbst6itpNAoEQOK5VEOh37gng7IeT7knLIjn9LAl26SYsTUeAdRTSwYwCGHondLsRVn0If0yGD/tzT8hgktUVHNANy1w/wamiQsg6ZvuSP5NifNFbts+kQOZR0OUEmMgLjKBQv43d3zYQFmek7hB1mgQCIYDh5loDACuT0nnkMuezjivjf9d25dHvNgFwz1freNduhrJ9ILD4bt1hXru2K1vSCrliTlvm3Pk7kRvep+uWz1gYsJxviwbx5t7RfLoijr/3aw5FBZB5zPxSt/tizzhs2846BrpEJ7RfMEQ2gYjG0Lw/RJjblr+R8eAfAif3QfpuSN9j/t0N67+EgrO2awVGOA8Q9ZobiwuJOkECgRCAj4/tV21cRJBbrjmmR7w1EOw8lsnlbyy3HgsNdP5Pr6hYs2in0Y8w8uOtQH9i6cx9fjO50XcxY3yXs+u3C+CvbKM5hxI1BP8Q4ws9sgm0HGx+wTeGiHjbdnC9iv2Kj+tgPOxpbQSYkgFi3zLYNN12nvKF6OalA0T91sb7i1pFAoEQpln39ePGT/7i5Ws6u+2a/766E//6yZitXGjXrFNyUlrXC6LYdOg0Z3IK+HqV4wpox4ni2cLxfFI0gvt8f6axSodWF9t+1Uc0sf2aD4r0bFONUsb7WgKNvdwzcCLJDBC7bMFizwIoLrCdFxprCwoRTYznYQ0gtAGExRp/AxxnXgvPkkAghKlzfCRbnnPvugBjEuOtgcDizv7NS513y0XNePTQaQ6ezCYtM6/U8YggP54aO4J7voqjbVw4v149wK3lBFi6K434eiG0ahBW/snOBEVAk0TjYa+oEE4fsNUeLAFi+y9G9lZnAsLsAkSs0Rdh3S4RNAKrWF5hJYFACA9ytvLYMLv+CIvIYGO46VXv/uH0Otp8XdPoEHalZpKdX0hIgHv/+d42ZQ2A+9da9vWDmJbGo+1wx2OFeXD2uJGQz/o3DbKOm3/TjKBx4A9jUSFn/ENKBIgGJYJGLITEGI/gesZqdsKBBAIhPGzr85fT6dlfARjUNpYezaJLnRMRXDrh3OUd46zzDm68sCkAJ7KM2kKHZ35l54vDHEYnuXz/Ixl0bByBKqPJ6HR2vstjHuUXaHROR7pODmhVVFB+0Di1Hw6tguwTlOo/AUBBcBQER9uCQ0iMuWCRi+dBUUbKcU/TGoryoSAbCnKMR/5Zc9vcVy+hdL+NG0ggEMLDwgL9eP+mRCZMW4+rr2JLSgqLvS+PYOuRDGsgeHCIMeHthas6WTugZ21K4dqeF5T53kt2pjF+6hpGJzbh9eu6lTr+4uzt9G4ezd1frrPu+3njEeLrBTsNWDXK19/W4V2eokLITjeCQ3Y6ZJ80HyccH2cOw7HNcDYdiko3yQGgfIyahLVWEV06cARFGLUb65d4NuRnO36pF5y1284u8UVv7itvGG/fB+GyFyp/78ohgUCIauBrjkrycfGrvH2JhHa+Poo2ceGEBPjy+nVdrc1Aic1sI272Hj9Lec7kGp20P643ZiTbB4NdxzL5dMV+Pl2x3+E1D87YCHigiag6+fpBeEPjURFaG1/E1iDhInDknDT6O1LWG8+LyqlJ+YfYPYLNR4gRWCIa2+0PdTzuH2xM4nPYFwLhjc793jghgUCIamAZMOSqdcZ++OoAcyGc4ABftr8wzOE8+zWQP1uxn4nD25X5vvZNRz+uP+IQCJ75ufTaC/YKzJXavIJSxhdvQChENa3Ya7Q2ZmBnnzBGTPkFOX6B+wXVmcl2EgiEqAYXt4qhfaMIHh3a1uU5m54ZikYTFVL2RKz1T19G4osLyC8qZu/xLFrGuh41k+MiJUVmbgGr9rsYsWPannKGrhdElXmOV1MKAsONRx3nJeFeiJoVEeTPvAf7076R6zUNIkP8yw0C4LhM5gPTN7g8b+Oh0/x7zg6HffvTjeaklNO5pc5vXWLY6NuL95RblqrSZWRYFdVPAoEQdVD3psYv9U6NI50eLygq5up3/yDdHGVUP8wIHoP/uxSAOWY6a/ug0rdVfYdrLNyRds7lnLH6IEt2Ol7nl00pNH9iLimnc875+sI9PBYIlFJBSqnVSqlNSqltSqnnnZxzm1LquFJqo/m4w1PlEeJ8MvPevgAcz3I+0qX/q0scnt83uJV1u9+ri3lrkfFr/9GhtpxKJYeQ9isRGCpLa83EH7cwfuoah/2/mKm0Nx8+TX5hMR8s20teoQeyqooK82QfQR5widY6SynlD6xQSs3TWv9V4rxvtNb3ebAcQpy3Fu90/qv92BnHph/7gHH4lO2XeM9m0bw6pjP/98MWirWx7vIbC3dzOjvfWpuoqrxC5yuuhZl5lrYfzeT9pXvZdDiDgsJi7jeHyIrq57EagTZkmU/9zYc0DApRzTo2jmBkZ+dj79s2DKd/61iC/H24o39zOjSO4ONbetIyNowTZys3ySy3oIj1B22zf9POOA8kln6QtxbtYdPhDAAOnsyWWkEN8mgfgVLKVym1EUgDFmitVzk5bYxSarNS6nullNPZMUqpu5RSa5VSa48fP+7JIgtRZ9wzsCX+voqCIqN5JdOcM3DEru39hwl9mHVfPzo0juC2ixMcXn9p+wYANI4KZueLw+kSbxshFB0awKmz+RRXYv2DiyctZvR7K1l/8BTPz9rGgNdszVP2mVenrkwu9drv1h2m7b/mc/BEdoXfT7iPRwOB1rpIa90NiAd6K6U6lThlFpCgte4CLAQ+d3Gdj7TWPbXWPWNjYz1ZZCHqjOhQfwqKNK2fmsekeTu5d9p6jmXk8o7daJ8ezaKtcxSeu7Kjw+ufvcLxueO1Aygs1tYJaeVJy8zlpFmDGP3eSqb8kexwfFdqJrkFRfztU2e/BW2+W3eoQu/3zM9bmbbqQIXOFeWrlnkEWuvTSqmlwDBgq93+E3anfQy8Wh3lEeJ8EBMa6PD89z3pXPTKIuvzr++4sNRrJg5vR2iAL1d1b0JEUOn8RhaW5pvDp3IqNKT1tfm7yj2n3dPzyz3nw2X7ypxrYfHFn0YQuOnCZuWeK8rnyVFDsUqpKHM7GLgU2FniHPv50lcCjoOehRAuZecXlnm8o5OhpfcMbMnf+iSUGQQAMnKMmsCot1eUOrYtJaPU0M/MXOdl6VDGvIm/nhjCM6M6MLCNrZafX1RM6pnScxzsyRwE9/Nk01AjYIlSajOwBqOPYLZS6gWl1JXmOQ+YQ0s3AQ8At3mwPEKcV67u3qTM486Ww6yomy60pVlImDiHfLsRQCPfWsHFkxZbnxcWFTN/2zGn1zmelcfel0eU2n9Ln2Y0jAzi9n7NS63D/Nv21DLLllMgncru5rGmIa31ZqC7k/3P2G0/ATzhqTIIcT4LL+dXvX3+osoqmd56TfJJ+raq7zCk9ERWHjFhgZzKdt2PEB7oZ024Z+9Wu45r+5FGAE//tJVBbWK5INr5KmUvzt5u3c7JL3K65oOoHJlZLEQd1rdVDAATBrUE4JHL2tC9aRRf31m6f6Cywu3WVbbMV8jOs/0at8xNOGU3Ee2dG7tzdbfGbHzmMi5p14Cf7zMmvi1+dKDDtWPsZjTfd4kx2W3D05dZ9/X/j+OEOHvTV9s6lNs/M5+EiXMq3KkNcPBENvO3Hi21f9/xLGZvTmHdgVP8sO5wha93PlB1rb2tZ8+eeu3atTVdDCFqhay8Qg6fyiYhJpRZm1IY2yO+zAVoKiM9K4+e/15ofZ48aSRJaZlc+roxFPR/13Zl8+HTDGrXgPFT1jBpdGfG9S47c2e3F37jdHYB+14e4VBj0VqjlCJh4hyH97M3c8Nh/jVzK2ddJNJLnjSSxTtT6dg4kriIIJdlsLzHlueGOtSq7N8bYP8rI9x2L2sDpdQ6rXVPZ8ck+6gQdVhYoB/tzLUMylukprLqhzmOSkpKy+KuL2wL2FgWyPllUwoAPezWSnDlq79fSFJaVqlmK2dfuPmFxQT4GY0WRcWah7/ZVOa1M7ILuH2q8SOxImsp/Ln3BC/M3s5nt/WiTVzpDKJvLNzDI5e1cfLK8480DQkhXPrg5h7W7UtfX8a+9NKL4Vj6CEoGDmc6NYkss5P77/2aW7fb/Gsen/y+j1Nn80uNkLq6W+mZ0l1f+M26fehk+RPTHpyxkcOncnh3SZLT43kV6JTOzC3gv7/usk7mq6skEAghXBrWqYIrfOF83eXKenpUB14d09n6/N9zdtD9xQVkl2gOmjyu1DgUB676GM7m2QKKZfRRelaew2xsizMuhsTam/pHMu8sSWLaqoPlnlsVOflFPPfLNjLK6JB3B2kaEkK4hbPRQVVhaQ6yV5WU1ZZ+B3v2gcB27VwW7Sg9ZHXv8SyKinWZn+t/C3YDkO3kuu7QZ9IiTptBoOTMcHeSGoEQokwf/q1HqX3v35RI+0YRNAgvvzmosq7qWrrp6N5p6wF4ZXRn1tuNLgJ44SrnX5CvL9jNe0uTHIabOutozswtLPVl3/WCKFbvP0nLJ+e6LKenB9psPZJhDQKuMrm6iwQCIUSZLu/o2DzUMjaU4Z0bMe/B/sy+vx8A/Vuf29oF9nx8FGv/danDvqMZxmzj+HrB1sV0Pr6lJ5Ov78b1vWyd5HMe6Gfd/mj5Pv4zfxefrthPx2fmk56V5/SXf2Swn3Vm9KgujXj2ig5sOnTa4ZzcgiLunbaOZLOP5N5p62j+hC1IFHkgKKxISrdubygx18LdpGlICFEuHwWWCcBf2eUwahARxKz7+tE8NtSt7+eq4znMbm7DZR3iSh3v2DiSGy9syterDjr8ij6bX+QwFPbNcd0I9PPl+3WHWbQzlTM5Bfj5KN6+oTtKKX5cf4QtRzKs51vyJM3dcox9L49g7hbHmdTvLtnL45e3q9qHLWHWphQ+WLaXbSlnrPt2Hst0y7VdkRqBEKJc9lkgGkUGOxzrHB/p8AXtLoFO+gq62qXKttc4MshaK3msAknrLogOYVinhkSF+KM1HM/MIyLY39qnMHV8L1qZazhb8i5ZrD3g/Nf59NVldxjnFhRx+RvL+cPul74zD3+z0SEIWEz8YTO/7/FMGn4JBEKIWum9mxLpGu+YOM9V2oyVTwzhy78bNRX7dZgB7h7QotT5IWZaigvqGWksvlt32JpGGyAmLNCaq6nr8785vHb+VsfaQFtzDsITP27hu7WH2JaSwSvzdjj0IWw6dJppqw6yKzWTmz5ZxWPfuZ4TEeui32XGmkMkpWU5PXauJBAIISrMsphNdRjSPo6f7+vHOLMPYPqdF1X4tfZDUBOdTHSzfEe3iQtzeY32LjKnfvbHfofnn9/e27r97pIkxn34Fx8u20eW3Uiiu75c69Bp/f26w9a5Dh8s28vOY2dYmZRO2pncUoFgml1TXL6HOo0lEAghynVxSyOnUcmO4+owaUwXkieNpI9ZhooYkxhv3e7bqj6bnxvqcLxJPaN5y342dLMYxyR3L13tuI7W3Af6O32vhpFB1tFTySeyyTQDwOnsAmsOpFQny3Ze894fnDybz6R5Oxk2+Xdu/GQVI9763SE9xuqnhtC3la0jviKT9qpCOouFEOX6+s6LSM/K89gXkbv5+fowoE0sBYXF1v6L9U9fxrS/DqAU1vUY7H99P3ypYzoJpRTjel3AjDVGkruWDUp3iFtGTS3/52DaPT2fS9o1sCbos0xq+/RWp+l9SM/K54oS6z2kZ+WjtaZj4wjm2AWe1U8OYerKZEYnlp16vKokEAghKqSuBAGLL+yabMDoO7h/SGuHfUopnhrRnpfm7nA6Cunmi5pZA0Ggn2O668nXd6NTE6MPI8jfl5AAX2sQsPf3z10nyXQ2ozkrr9Dah2HRICKIfw5zz6gkZ6RpSAjh1e4c0ILkSSMJdTLyqVOTSF66phP/u7Yr4DjruXfzaIdzS6bBcObyjnEkTxrpdESUxensggotD+pOEgiEEKIMN13YjDE9jD6HuXYT1ipbQ7qqW2NeusboxLZPrlfSzmOZRLkhb1NlSCAQQogKatUgnKdGtGfaHReWyon0+nVGrWH544Odvva1sV2twePREnMdEptGUT/MVguICpFAIIQQtdadA1o4jOSxGJ0YT/KkkTS1G330b7uRR/aBw9dHWVeVA/jx3r70aWm7ZmQ11wiks1gIIdxs54vD2J2aSZf4KOqHBRIbXrrN/+4BLSgq1txhNhM9M6oDs8xFfqp7ZTSpEQghhJsF+fvSxUyHMaxTQ3o0iy51TlRIAE+OaE8Dc95AbHgg/xxmNBmV1ZnsCVIjEEKIWuLWPgmczMrnul7uXXa0PBIIhBCilggN9ONfozpU+/tK05AQQng5CQRCCOHlJBAIIYSXk0AghBBeTgKBEEJ4OQkEQgjh5SQQCCGEl5NAIIQQXk7ZL7BcFyiljgMHqvjy+kC6G4tzPpJ7VD65R2WT+1O+mrhHzbTWsc4O1LlAcC6UUmu11s7XjROA3KOKkHtUNrk/5att90iahoQQwstJIBBCCC/nbYHgo5ouQB0g96h8co/KJvenfLXqHnlVH4EQQojSvK1GIIQQogQJBEII4eW8JhAopYYppXYppZKUUhNrujzVSSn1mVIqTSm11W5ftFJqgVJqj/m3nrlfKaXeMu/TZqVUot1rbjXP36OUurUmPosnKKUuUEotUUrtUEptU0o9aO6Xe2RSSgUppVYrpTaZ9+h5c39zpdQq8/N+o5QKMPcHms+TzOMJdtd6wty/Syl1ec18Is9QSvkqpTYopWabz+vG/dFan/cPwBfYC7QAAoBNQIeaLlc1fv4BQCKw1W7ff4CJ5vZE4FVzewQwD1DARcAqc380sM/8W8/crlfTn81N96cRkGhuhwO7gQ5yjxzukQLCzG1/YJX52b8Fxpn7PwAmmNv3Ah+Y2+OAb8ztDua/v0Cgufnv0remP58b79MjwNfAbPN5nbg/3lIj6A0kaa33aa3zgRnAVTVcpmqjtV4OnCyx+yrgc3P7c+Bqu/1faMNfQJRSqhFwObBAa31Sa30KWAAM83zpPU9rfVRrvd7czgR2AE2Qe2RlftYs86m/+dDAJcD35v6S98hy774HhiillLl/htY6T2u9H0jC+PdZ5yml4oGRwCfmc0UduT/eEgiaAIfsnh8293mzOK31UTC+CIEG5n5X98or7qFZRe+O8YtX7pEds9ljI5CGEeT2Aqe11oXmKfaf13ovzOMZQAzn9z2aDPwTKDafx1BH7o+3BALlZJ+Mm3XO1b067++hUioM+AF4SGt9pqxTnew77++R1rpIa90NiMf4ldre2WnmX6+6R0qpUUCa1nqd/W4np9bK++MtgeAwcIHd83ggpYbKUlukms0ZmH/TzP2u7tV5fQ+VUv4YQWCa1vpHc7fcIye01qeBpRh9BFFKKT/zkP3ntd4L83gkRvPk+XqP+gJXKqWSMZqeL8GoIdSJ++MtgWAN0NrswQ/A6Jz5pYbLVNN+ASyjWm4Ffrbbf4s5MuYiIMNsFvkVGKqUqmeOnhlq7qvzzLbZT4EdWuvX7Q7JPTIppWKVUlHmdjBwKUZfyhJgrHlayXtkuXdjgcXa6A39BRhnjpppDrQGVlfPp/AcrfUTWut4rXUCxvfLYq31TdSV+1PTvezV9cAY6bEbo13zqZouTzV/9unAUaAA4xfH3zHaIxcBe8y/0ea5CnjXvE9bgJ5217kdo/MqCRhf05/LjfenH0b1ezOw0XyMkHvkcI+6ABvMe7QVeMbc3wLjiyoJ+A4INPcHmc+TzOMt7K71lHnvdgHDa/qzeeBeDcI2aqhO3B9JMSGEEF7OW5qGhBBCuCCBQAghvJwEAiGE8HISCIQQwstJIBBCCC8ngUDUOkqpIqXURjPT5Xql1MXlnB+llLq3AtddqpSqNQuG1wZKqalKqbHlnynOZxIIRG2Uo7XuprXuCjwBvFLO+VEY2RxrJbuZpULUShIIRG0XAZwCIxeQUmqRWUvYopSyZJCdBLQ0axGvmef+0zxnk1Jqkt31rjXz6u9WSvU3z/VVSr2mlFpjEC675AAAA5BJREFUri9wt7m/kVJquXndrZbz7SmlkpVSr5rXXK2UamXun6qUel0ptQR4VRlrG/xkXv8vpVQXu880xSzrZqXUGHP/UKXUn+Zn/c7Mg4RSapJSart57n/Nfdea5duklFpezmdSSql3zGvMwZZIT3izmp6FJw95lHwARRize3diZGXsYe73AyLM7foYszIVkIDjWgvDgZVAiPncMiN4KfA/c3sEsNDcvgv4l7kdCKzFyAX/KOYsdIw1LcKdlDXZ7pxbsM0onQrMxswlD7wNPGtuXwJsNLdfBSbbXa+e+dmWA6Hmvv8DnsFY52AXtrXGo8y/W4AmJfa5+kyjMTKH+gKNgdPA2Jr+by6Pmn1IlVXURjnayHKJUqoP8IVSqhPGl/7LSqkBGKl+mwBxTl5/KTBFa50NoLW2X4vBklBuHUYAASMnUBe7tvJIjBwva4DPzIR0P2mtN7oo73S7v2/Y7f9Oa11kbvcDxpjlWayUilFKRZplHWd5gdb6lJnJsgPwh5EGiQDgT+AMkAt8Yv6an22+7A9gqlLqW7vP5+ozDQCmm+VKUUotdvGZhBeRQCBqNa31n0qp+kAsxq/4WIwaQoGZ6THIycsUrlP35pl/i7D9/6+A+7XWpRLEmUFnJPClUuo1rfUXzorpYvtsiTI5e52zsiqMBW5ucFKe3sAQjOBxH3CJ1voepdSFZjk3KqW6ufpMSqkRTt5PeDnpIxC1mlKqHUYzxgmMX7VpZhAYDDQzT8vEWGLS4jfgdqVUiHmN6HLe5ldggvnLH6VUG6VUqFKqmfl+H2NkJ0108frr7f7+6eKc5cBN5vUHAenaWPPgN4wvdMvnrQf8BfS1628IMcsUBkRqrecCDwGWWlNLrfUqrfUzQDpGGmOnn8ksxzizD6ERMLiceyO8gNQIRG0UrIyVsMD4ZXur1rpIKTUNmKWUWoutDwGt9Qml1B9Kqa3APK314+av4rVKqXxgLvBkGe/3CUYz0XpltMUcx1hScBDwuFKqAMjC6ANwJlAptQrjh1WpX/Gm54ApSqnNQDa2FMT/Bt41y14EPK+1/lEpdRswXSkVaJ73L4yA97NSKsi8Lw+bx15TSrU29y3CWPN2s4vPNBOjj2ILRjbeZWXcF+ElJPuoEOfAbJ7qqbVOr+myCFFV0jQkhBBeTmoEQgjh5aRGIIQQXk4CgRBCeDkJBEII4eUkEAghhJeTQCCEEF7u/wGNobykEwrZQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_learn.recorder.plot_losses()\n",
    "experiment.log_figure(figure_name=\"train loss 02\", figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.save('cls_learn.REGR1.DOCPOLL4ALL.200101.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls_learn.load('cls_learn.REGR1.200101.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(self, ds_type:DatasetType=DatasetType.Valid, activ:nn.Module=None, with_loss:bool=False, n_batch:Optional[int]=None,\n",
    "              pbar:Optional[PBar]=None, ordered:bool=False) -> List[Tensor]:\n",
    "    \"Return predictions and targets on the valid, train, or test set, depending on `ds_type`.\"\n",
    "    self.model.reset()\n",
    "    if ordered: np.random.seed(42)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outs = []\n",
    "        asps = []\n",
    "        rats = []\n",
    "        text = []\n",
    "        truth = []\n",
    "        for xb,yb in progress_bar(cls_learn.dl(DatasetType.Valid)):\n",
    "            out,raw_enc,enc,asp,rat = cls_learn.model(xb)\n",
    "    #         text.append(xb)\n",
    "            truth.append(yb)\n",
    "            outs.append(out)\n",
    "            for doc in asp:\n",
    "                asps.append( to_float(doc.cpu()))\n",
    "            for doc in rat:\n",
    "                rats.extend( to_float(doc.cpu()) )\n",
    "\n",
    "    # text = to_float(torch.cat(text).cpu())\n",
    "    truth = to_float(torch.cat(truth).cpu())\n",
    "    outs = to_float(torch.cat(outs).cpu())\n",
    "    \n",
    "    if ordered and hasattr(self.dl(ds_type), 'sampler'):\n",
    "        np.random.seed(42)\n",
    "        sampler = [i for i in self.dl(ds_type).sampler]\n",
    "        reverse_sampler = np.argsort(sampler)\n",
    "        \n",
    "        truth = truth[reverse_sampler]\n",
    "        outs = outs[reverse_sampler]\n",
    "        \n",
    "        asps = [asps[i] for i in reverse_sampler]\n",
    "        rats = [rats[i] for i in reverse_sampler]\n",
    "    return (truth,outs,asps,rats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='117' class='' max='117', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [117/117 00:23<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "truth,outs,asps,rats = get_preds(self=cls_learn, ds_type=DatasetType.Valid, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 1, 0, 0, 4, 5, 2, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.argmax(asps[0],dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_num_file = [\"aspect_0.count\", \"test_aspect_0.count\"]\n",
    "rating_file = [\"aspect_0.rating\", \"test_aspect_0.rating\"]\n",
    "content_file = [\"aspect_0.txt\", \"test_aspect_0.txt\"]\n",
    "\n",
    "dataset_dir = \"./data/hotel_balance_LengthFix1_3000per/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_to_doc(sent_list, sent_count):\n",
    "    start_index = 0\n",
    "    docs = []\n",
    "    for s in sent_count:\n",
    "#         doc = \" xxPERIOD \".join(sent_list[start_index:start_index + s])\n",
    "#         doc = doc + \" xxPERIOD \"\n",
    "        docs.append(sent_list[start_index:start_index + s])\n",
    "        start_index = start_index + s\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = 0\n",
    "TEST_DATA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 30, 25, 33, 29]\n",
      "   0  1  2  3  4  5\n",
      "0  1  0  0  3  1  1\n",
      "1  2  2  1  2  3  3\n",
      "2  4  4  4  3  4  4\n",
      "3  3  2  3  3  3  4\n",
      "4  3  4  3  4  4  4\n"
     ]
    }
   ],
   "source": [
    "# Load Count\n",
    "sent_count_test = list(open(dataset_dir + sent_num_file[TEST_DATA], \"r\").readlines())\n",
    "sent_count_test = [int(s) for s in sent_count_test if (len(s) > 0 and s != \"\\n\")]\n",
    "print( sent_count_test[0:5] )\n",
    "\n",
    "# Load Ratings\n",
    "aspect_rating_test = list(open(dataset_dir + rating_file[TEST_DATA], \"r\").readlines())\n",
    "aspect_rating_test = [s for s in aspect_rating_test if (len(s) > 0 and s != \"\\n\")]\n",
    "\n",
    "aspect_rating_test = [s.split(\" \") for s in aspect_rating_test]\n",
    "aspect_rating_test = np.array(aspect_rating_test)[:, 0:-1]\n",
    "aspect_rating_test = aspect_rating_test.astype(np.int) - 1\n",
    "aspect_rating_test = pd.DataFrame(aspect_rating_test)\n",
    "print( aspect_rating_test.head() )\n",
    "\n",
    "# Load Sents\n",
    "sents_test = list(open(dataset_dir + content_file[TEST_DATA], \"r\").readlines())\n",
    "sents_test = [s.strip() for s in sents_test]\n",
    "\n",
    "# Sents to Doc\n",
    "docs_test = concat_to_doc(sents_test, sent_count_test)\n",
    "\n",
    "docs_test = pd.DataFrame({doc:docs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[definitely not a 5 star resort i 'm dumbfound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[facilities need work, we visited excellence f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[excellence was exactly that, my family and i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[great service , nice hotel , mediocre food, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[very relaxing experience just returned from m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5                                                  6\n",
       "0  1  0  0  3  1  1  [definitely not a 5 star resort i 'm dumbfound...\n",
       "1  2  2  1  2  3  3  [facilities need work, we visited excellence f...\n",
       "2  4  4  4  3  4  4  [excellence was exactly that, my family and i ...\n",
       "3  3  2  3  3  3  4  [great service , nice hotel , mediocre food, m...\n",
       "4  3  4  3  4  4  4  [very relaxing experience just returned from m..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.concat( [aspect_rating_test, docs_test], axis=1, ignore_index=True )\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth:\n",
      "[1, 0, 0, 3, 1, 1]\n",
      "prediction:\n",
      "tensor(3)\n",
      "doc:\n",
      "definitely not a 5 star resort i 'm dumbfounded that this hotel gets good reviews and is so highly rated\n",
      "          +++ location +++ [0.201 0.198 0.205 0.198 0.198]\n",
      "it 's decidedly a 3 star property , not 5 stars as indicated\n",
      "          +++ value +++ [0.207 0.197 0.203 0.197 0.197]\n",
      "the rooms are very dated and run down , old crappy beds and pillows , an old tv and overall poorly maintained\n",
      "          +++ room +++ [0.19  0.336 0.156 0.162 0.156]\n",
      "the whole property is pretty run down and old - looking\n",
      "          +++ value +++ [0.295 0.204 0.163 0.175 0.163]\n",
      "the food is subpar , not one meal i had would be called great\n",
      "          +++ value +++ [0.34  0.165 0.166 0.167 0.162]\n",
      "the service is uneven and the staff is poorly trained and uninformed\n",
      "          +++ service +++ [0.207 0.159 0.159 0.159 0.316]\n",
      "many do not comprehend english\n",
      "          +++ service +++ [0.244 0.169 0.169 0.169 0.248]\n",
      "the beach is great , it 's the only redeeming factor\n",
      "          +++ location +++ [0.175 0.172 0.31  0.172 0.172]\n",
      "however the resort is a 1- hour taxi trip from the airport\n",
      "          +++ location +++ [0.209 0.191 0.218 0.191 0.191]\n",
      "===========\n",
      "truth:\n",
      "[2, 2, 1, 2, 3, 3]\n",
      "prediction:\n",
      "tensor(4)\n",
      "doc:\n",
      "facilities need work\n",
      "          +++ location +++ [0.201 0.179 0.246 0.185 0.189]\n",
      "we visited excellence for 5 nights in december\n",
      "          +++ location +++ [0.179 0.173 0.279 0.183 0.186]\n",
      "our first room , #1112, had a safe that did not work and so - so air conditioning\n",
      "          +++ value +++ [0.215 0.206 0.206 0.186 0.187]\n",
      "when we went to the front desk to complain , we were told to go to the room and someone would be there within 15 minutes\n",
      "          +++ service +++ [0.206 0.189 0.188 0.188 0.228]\n",
      "45 minutes later , the safe guy showed up , but nobody for the a/c\n",
      "          +++ service +++ [0.202 0.196 0.196 0.196 0.209]\n",
      "the safe guy could not fix it\n",
      "          +++ value +++ [0.205 0.198 0.197 0.197 0.204]\n",
      "when he left , the electricity went out\n",
      "          +++ value +++ [0.207 0.198 0.197 0.197 0.201]\n",
      "it went out a second time before we finally went to the front desk to change rooms\n",
      "          +++ service +++ [0.208 0.195 0.194 0.194 0.209]\n",
      "we had dinner that night in the lobster house\n",
      "          +++ location +++ [0.186 0.181 0.261 0.184 0.187]\n",
      "do not waste your time on this one\n",
      "          +++ location +++ [0.198 0.198 0.207 0.198 0.199]\n",
      "the lobster tails had about 2 bites of food included\n",
      "          +++ value +++ [0.227 0.184 0.214 0.189 0.186]\n",
      "while we were in there , the electricity went out again\n",
      "          +++ value +++ [0.207 0.197 0.2   0.195 0.202]\n",
      "room 3002 served us pretty well , until night #3 when my partner got up to go to the bathroom and stepped into an inch of water\n",
      "          +++ service +++ [0.207 0.199 0.192 0.191 0.211]\n",
      "a hose had broken on the back of the toilet and flooded our room\n",
      "          +++ room +++ [0.232 0.267 0.164 0.171 0.166]\n",
      "it would 've been ok , but when we went to the front desk we were told that we needed to wait until noon to see if perhaps they could move us to another room\n",
      "          +++ service +++ [0.203 0.197 0.193 0.194 0.213]\n",
      "the front desk clerks were not empowered to just move us\n",
      "          +++ service +++ [0.199 0.189 0.189 0.189 0.233]\n",
      "my partner was infuriated that they wanted us to wait 4 hours for a new room\n",
      "          +++ service +++ [0.2   0.192 0.192 0.192 0.224]\n",
      "finally , matias at the front desk finally arranged to have us moved to another upgraded room - 3109\n",
      "          +++ service +++ [0.202 0.194 0.199 0.193 0.212]\n",
      "we walked in and saw the leak coming from the ceiling and nearly flipped\n",
      "          +++ room +++ [0.224 0.227 0.184 0.185 0.18 ]\n",
      "we finally got into #3110, which was a gorgeous suite with a beautiful view\n",
      "          +++ location +++ [0.2   0.198 0.243 0.185 0.174]\n",
      "on the positive side , the food at the other restaurants was very good\n",
      "          +++ location +++ [0.186 0.169 0.292 0.172 0.181]\n",
      "i particularly liked the french restaurant , while my partner liked the asian restaurant\n",
      "          +++ location +++ [0.224 0.168 0.262 0.171 0.175]\n",
      "the breakfast buffet was like nothing i 'd ever seen before - lots of choices\n",
      "          +++ location +++ [0.206 0.187 0.229 0.188 0.191]\n",
      "the ocean was way too rough to enjoy , particularly if you 're not a strong swimmer\n",
      "          +++ location +++ [0.188 0.185 0.257 0.185 0.185]\n",
      "much of the beach was black flagged the entire time we were there , so if you 're a big ocean fan , i do not recommend this resort\n",
      "          +++ location +++ [0.194 0.192 0.23  0.192 0.192]\n",
      "my favorite part , by far , though , were the beds next to the pools and ocean\n",
      "          +++ location +++ [0.171 0.16  0.338 0.163 0.168]\n",
      "they were amazing\n",
      "          +++ location +++ [0.182 0.17  0.301 0.177 0.17 ]\n",
      "i guess you could particularly say so since the beds in the rooms were hard as rocks\n",
      "          +++ location +++ [0.198 0.199 0.219 0.192 0.192]\n",
      "all in all , a good trip - highly recommend the zip line tour\n",
      "          +++ location +++ [0.189 0.172 0.288 0.174 0.177]\n",
      "it was worth every penny\n",
      "          +++ location +++ [0.196 0.194 0.218 0.195 0.197]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 3, 4, 4]\n",
      "prediction:\n",
      "tensor(1)\n",
      "doc:\n",
      "excellence was exactly that\n",
      "          +++ location +++ [0.187 0.167 0.25  0.184 0.212]\n",
      "my family and i stayed at the excellence punta cana from december 22 to december 29 of this year\n",
      "          +++ location +++ [0.18  0.167 0.277 0.181 0.194]\n",
      "it was an amazing time had by all that attended\n",
      "          +++ location +++ [0.183 0.164 0.289 0.175 0.188]\n",
      "we arrived at the resort around 4 am because of a delay at the airport in vancouver , but even at 4 am , the service of the bellhops and the front desk was up to par\n",
      "          +++ location +++ [0.19  0.189 0.229 0.191 0.201]\n",
      "our bags were unloaded and immediately tagged and set to one side of the lobby while we were handed cold scented towels to cool off with\n",
      "          +++ location +++ [0.196 0.196 0.213 0.196 0.2  ]\n",
      "check in was fairly expedient and we were in our rooms within twenty minutes\n",
      "          +++ location +++ [0.186 0.176 0.272 0.179 0.187]\n",
      "i had never been to an all inclusive resort before , and wasted no time enjoying the pleasures of the mini bar in the room , as well as the ample storage space for our things\n",
      "          +++ location +++ [0.195 0.195 0.218 0.196 0.196]\n",
      "room service even at 4 am was great , the girl on the phone said it would be about 40 minutes for the food , which seemed a little long , but i think they only say that to cover there butts , because it took about 25 minutes at most\n",
      "          +++ service +++ [0.186 0.182 0.197 0.194 0.241]\n",
      "the activities during the day were well thought out , though , there was some delays and cancellations due to weather conditions ( beach volleyball cancelled to due strong winds\n",
      "          +++ location +++ [0.193 0.193 0.224 0.194 0.195]\n",
      "the entertainment staff was amazing and extremely friendly , a special thanks to all my friends , ines ( my fiance , ) altagracia ( who lovingly reffered to me as flaco loco , which translates into crazy skinny guy , ) eliza and johanna ( my disco dance partners , ) sexy cesar ( who taught me all the sexy dance moves i\n",
      "          +++ service +++ [0.167 0.163 0.22  0.172 0.278]\n",
      "now know , ) julio cesar ( the mc for the games and parties\n",
      "          +++ location +++ [0.193 0.194 0.224 0.195 0.195]\n",
      "the restaurants i ca not offer too much help with , i did not eat at all of them , but of the few i did eat at , i recommend toscana for the huge buffet everyday , breakfast here is well prepared and quite delicious ( although i do not recommend the scrambled eggs\n",
      "          +++ location +++ [0.198 0.187 0.219 0.194 0.203]\n",
      "the omlettes are delicious and you have to try one\n",
      "          +++ location +++ [0.193 0.169 0.25  0.18  0.206]\n",
      "for lunch , you have to check out the grill on the beach , different food everyday , always good , and makes the beach smell amazing\n",
      "          +++ location +++ [0.19  0.175 0.241 0.187 0.207]\n",
      "for dinner , i liked spice ( asian cuisine , ) agave ( mexican , but do not eat the calimari from here , very rubbery , ) the pizza that is delivered to the pool and the beach is awesome , make sure you try that\n",
      "          +++ location +++ [0.195 0.178 0.231 0.193 0.203]\n",
      "the bars were awesome , you get accustomed to speaking the language when ordering drinks , instead of drinking your usual bacardi and coke , try the brugal extra anejo , they call it the dominican babymaker , and it 's obvious why once you try it\n",
      "          +++ location +++ [0.194 0.194 0.219 0.195 0.199]\n",
      "the stuff tastes amazing and it does magic for someone trying to loosen up on the dance floor\n",
      "          +++ location +++ [0.199 0.199 0.204 0.199 0.199]\n",
      "the disco is great too , although sometimes a little empty , but still worth checking out\n",
      "          +++ location +++ [0.198 0.199 0.206 0.199 0.198]\n",
      "the worst part of my trip was the vendors , they do not let up , and i am a very well mannered person , which makes it hard to shut them down over and over again , make sure you do not tell them you like anything until you know you 're going to buy it , otherwise you 'll have to beat\n",
      "          +++ location +++ [0.2   0.2   0.201 0.2   0.2  ]\n",
      "them off with a stick to get away\n",
      "          +++ location +++ [0.2   0.2   0.201 0.2   0.2  ]\n",
      "try to make it to the theatre for the shows at 10pm every night , they are worth it\n",
      "          +++ location +++ [0.198 0.199 0.205 0.199 0.199]\n",
      "the ice breaker shows are fun too , it gets people into the swing of things\n",
      "          +++ location +++ [0.197 0.197 0.211 0.197 0.197]\n",
      "all in all , i would highly recommend this resort for anyone going on a honeymoon or a romantic time with the better half , there is not a very big single crowd , so parents beware taking your single sons and daughters to this resort if they 're looking to party with other singles\n",
      "          +++ location +++ [0.189 0.189 0.238 0.191 0.193]\n",
      "hope this helps you\n",
      "          +++ location +++ [0.195 0.196 0.215 0.196 0.197]\n",
      "adios amigos and amigas\n",
      "          +++ location +++ [0.187 0.188 0.246 0.189 0.191]\n",
      "===========\n",
      "truth:\n",
      "[3, 2, 3, 3, 3, 4]\n",
      "prediction:\n",
      "tensor(5)\n",
      "doc:\n",
      "great service , nice hotel , mediocre food\n",
      "          +++ location +++ [0.174 0.165 0.297 0.182 0.182]\n",
      "my husband and i stayed at excellence for five nights mid - november\n",
      "          +++ location +++ [0.18  0.167 0.304 0.174 0.176]\n",
      "we booked our trip at the very last minute so we were not able to do a ton of research on the dominican but the hotel receives high ratings thorughout the web\n",
      "          +++ location +++ [0.192 0.189 0.236 0.19  0.192]\n",
      "after the one hour ride from the airport we arrived at the hotel and were greeted by everyone we met\n",
      "          +++ location +++ [0.196 0.194 0.219 0.194 0.197]\n",
      "i have to say that the staff at the hotel were very nice and made every effort to learn our names and greet us by name each time they saw us\n",
      "          +++ service +++ [0.16  0.156 0.181 0.163 0.34 ]\n",
      "we opted to upgrade to the excellence club and we are still trying to decide if we think it was worth it or not\n",
      "          +++ location +++ [0.194 0.193 0.227 0.193 0.194]\n",
      "as part of the excellence club , you are ushered to the club 's private lobby for check - in but , really , it almost just creates an unneccesary step in the check - in process and adds another person or two you feel like you should tip\n",
      "          +++ location +++ [0.202 0.196 0.208 0.195 0.2  ]\n",
      "the biggest benefits of the excellence club for us were the unlimited internet access , beach towels in the room ( they were hard to get otherwise ) , and the beach bag in our room\n",
      "          +++ location +++ [0.179 0.179 0.281 0.178 0.182]\n",
      "we did eat breakfast each morning in the excellence club which was nice because it was a small buffet and you did not have to deal with a crowd\n",
      "          +++ location +++ [0.199 0.196 0.213 0.195 0.198]\n",
      "the hotel itself was clean , the staff was very friendly , and nothing ever felt crowded\n",
      "          +++ service +++ [0.17  0.164 0.206 0.193 0.266]\n",
      "however , the food was not great\n",
      "          +++ location +++ [0.199 0.194 0.216 0.194 0.196]\n",
      "it was not bad - but it was not great\n",
      "          +++ location +++ [0.203 0.197 0.208 0.196 0.197]\n",
      "i 'm not a big eater but i was prepared to indulge on my vacation and there just was not anything i was crazy about\n",
      "          +++ value +++ [0.203 0.2   0.201 0.198 0.198]\n",
      "the presentation of the food was nice but it was just bland\n",
      "          +++ location +++ [0.22  0.171 0.262 0.172 0.176]\n",
      "i think that is the best way to describe it\n",
      "          +++ location +++ [0.204 0.193 0.216 0.193 0.194]\n",
      "the pizzas that were delivered to the pool area were good but it was unpredictable because you never knew when they would arrive\n",
      "          +++ location +++ [0.203 0.195 0.21  0.196 0.197]\n",
      "we went on two excursions - swimming with the sting - rays/sharks and the zip - line tour\n",
      "          +++ location +++ [0.198 0.198 0.208 0.198 0.198]\n",
      "we loved the zip - line excursion\n",
      "          +++ location +++ [0.188 0.19  0.248 0.187 0.187]\n",
      "the staff was great and our bus driver and tour guide were great\n",
      "          +++ location +++ [0.166 0.163 0.279 0.166 0.226]\n",
      "it was interesting to visit the sting - rays and swim with the sharks but the reef where we snorkeled was disappointing\n",
      "          +++ location +++ [0.198 0.198 0.207 0.198 0.198]\n",
      "the fish were very small and there was not much to see\n",
      "          +++ location +++ [0.199 0.198 0.207 0.198 0.198]\n",
      "the electricity went out in our room a handful of times , especially when i used the hairdryer\n",
      "          +++ room +++ [0.2   0.201 0.2   0.199 0.2  ]\n",
      "also , our ac was terrible\n",
      "          +++ room +++ [0.201 0.207 0.198 0.197 0.197]\n",
      "they tried to repair it but it just never got cool\n",
      "          +++ room +++ [0.2   0.214 0.195 0.195 0.195]\n",
      "our room was big , though , and clean\n",
      "          +++ room +++ [0.206 0.276 0.172 0.179 0.168]\n",
      "we always got housekeeping service twice a day and they refilled our mini - bar daily\n",
      "          +++ service +++ [0.183 0.172 0.214 0.21  0.22 ]\n",
      "in many of the reviews , people said they got sick\n",
      "          +++ location +++ [0.197 0.194 0.208 0.197 0.203]\n",
      "our representative at the hotel ( through aaa ) warned us that many people think they get sick from the water or the food but they do not realize that having too many drinks with coconut in them will also do it\n",
      "          +++ location +++ [0.2   0.2   0.201 0.2   0.2  ]\n",
      "coconut is a natural laxative so you need to limit your consumption\n",
      "          +++ location +++ [0.2   0.199 0.203 0.199 0.199]\n",
      "i would still pack the immodium just to be sure\n",
      "          +++ location +++ [0.2   0.199 0.203 0.199 0.199]\n",
      "i could not decide if i wanted to give this hotel a 3/5 or a 4/5 but i decided to go up because of the friendly staff and the cleanliness of our room\n",
      "          +++ location +++ [0.199 0.198 0.206 0.198 0.2  ]\n",
      "i do not think i would go back because of the food but we had a nice time while we were there\n",
      "          +++ location +++ [0.199 0.197 0.21  0.197 0.197]\n",
      "we met a lot of great people at the swim up bar\n",
      "          +++ location +++ [0.188 0.184 0.249 0.184 0.194]\n",
      "===========\n",
      "truth:\n",
      "[3, 4, 3, 4, 4, 4]\n",
      "prediction:\n",
      "tensor(5)\n",
      "doc:\n",
      "very relaxing experience just returned from my 40th birthday romantic getaway with my husband\n",
      "          +++ location +++ [0.185 0.164 0.308 0.169 0.174]\n",
      "this was our first time in the dominican republic , and we have literally been to every single island in the caribbean\n",
      "          +++ location +++ [0.2   0.177 0.26  0.18  0.182]\n",
      "so i can assure you that my review will be short , sweet , and comprehensive\n",
      "          +++ location +++ [0.204 0.186 0.229 0.186 0.195]\n",
      "in general , we liked the dr , and the excellence was very nice\n",
      "          +++ location +++ [0.185 0.164 0.302 0.17  0.179]\n",
      "the top reasons why we liked excellence were : 1) no kids ( ie\n",
      "          +++ location +++ [0.198 0.189 0.231 0.189 0.194]\n",
      ", if i want to get away from my own kids , i definitely do not want to vacation with other peoples ' kids )\n",
      "          +++ location +++ [0.2   0.199 0.202 0.199 0.2  ]\n",
      "it was so quiet\n",
      "          +++ location +++ [0.203 0.183 0.244 0.185 0.186]\n",
      "2) the staff and the people in the dr in general\n",
      "          +++ location +++ [0.194 0.169 0.251 0.18  0.205]\n",
      "so genuinely friendly , helpful , and wonderful\n",
      "          +++ service +++ [0.154 0.152 0.159 0.153 0.383]\n",
      "believe me , this is not true in most other areas of the caribbean\n",
      "          +++ location +++ [0.199 0.199 0.203 0.199 0.199]\n",
      "the all - inclusive feature\n",
      "          +++ location +++ [0.202 0.195 0.212 0.195 0.195]\n",
      "loved being served , served , served\n",
      "          +++ location +++ [0.193 0.162 0.299 0.169 0.177]\n",
      "i wanted to sit on my [ - - ] all day and just be a gluttonous pig\n",
      "          +++ location +++ [0.199 0.197 0.208 0.197 0.198]\n",
      "and this is the perfect place to do it\n",
      "          +++ location +++ [0.177 0.163 0.326 0.165 0.17 ]\n",
      "4) the best selection of beach and pool lounge chairs , beds , and hammocks i 've ever seen\n",
      "          +++ location +++ [0.187 0.17  0.282 0.177 0.184]\n",
      "there were palapas everywhere , so there was no shortage of shade\n",
      "          +++ location +++ [0.2   0.193 0.218 0.194 0.195]\n",
      "there were so many beds , you did not have to worry about not getting one\n",
      "          +++ location +++ [0.2   0.199 0.202 0.199 0.199]\n",
      "i 'd never had the chance to sleep on a beach bed , because usually hotels have only a few , so you end up looking longingly at the lucky few who get them\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "ok , so here 's what i did not love about excellence :1) beach is not swimmable\n",
      "          +++ location +++ [0.198 0.197 0.211 0.197 0.197]\n",
      "way too rough most of the time\n",
      "          +++ location +++ [0.181 0.18  0.279 0.18  0.18 ]\n",
      "for this reason alone , i 'd not return here\n",
      "          +++ location +++ [0.2   0.199 0.203 0.199 0.199]\n",
      "i 'm a beach fan , and love to swim in the warm caribbean sea\n",
      "          +++ location +++ [0.197 0.196 0.214 0.196 0.197]\n",
      "i 'm no food snob , but some of the food was downright bad\n",
      "          +++ location +++ [0.2   0.199 0.203 0.199 0.199]\n",
      "and you end up eating in the same spot for breakfast and lunch\n",
      "          +++ value +++ [0.205 0.198 0.2   0.198 0.198]\n",
      "even though they have like 7 restaurants - the majority of them are only open for dinner\n",
      "          +++ value +++ [0.218 0.194 0.2   0.194 0.195]\n",
      "we only stayed 4 nights , and we were definitely getting very tired of the breakfast/lunch selection by the 3rd day\n",
      "          +++ value +++ [0.206 0.198 0.199 0.197 0.2  ]\n",
      "overall , if you just want to relax by the pool and you do not care about not going in the beach , this is a very beautiful resort\n",
      "          +++ location +++ [0.2   0.193 0.219 0.193 0.194]\n",
      "the staff is wonderful\n",
      "          +++ service +++ [0.163 0.155 0.168 0.162 0.351]\n",
      "if you are looking for a place to party and be loud and crazy , this is not your place\n",
      "          +++ location +++ [0.2   0.198 0.205 0.198 0.2  ]\n",
      "===========\n",
      "truth:\n",
      "[1, 0, 2, 3, 1, 0]\n",
      "prediction:\n",
      "tensor(3)\n",
      "doc:\n",
      "5- star views\n",
      "          +++ location +++ [0.199 0.2   0.201 0.2   0.2  ]\n",
      "2- star service i do not know where to start\n",
      "          +++ location +++ [0.199 0.199 0.203 0.2   0.199]\n",
      "the roaches in the room , the rude waiters , bartenders , front desk , the dead flies that stayed on our friends ' mirror the entire stay , the average at best food ( only one morning in the bathroom for longer than you would want ) , the 6,7,8 times i had to trip the breakers so my wife could use the\n",
      "          +++ value +++ [0.28  0.178 0.16  0.214 0.167]\n",
      "hair dryer without our power going out , or the waste of money the excellence club turned out to be\n",
      "          +++ value +++ [0.29  0.185 0.171 0.182 0.173]\n",
      "i guess i 'll start with the good\n",
      "          +++ location +++ [0.199 0.2   0.201 0.2   0.2  ]\n",
      "the beach was fabulous\n",
      "          +++ location +++ [0.173 0.173 0.305 0.175 0.173]\n",
      "the resort itself , d?cor , pool , beach access was great\n",
      "          +++ value +++ [0.221 0.181 0.22  0.199 0.18 ]\n",
      "ok now for the rest of the trip\n",
      "          +++ value +++ [0.202 0.198 0.202 0.198 0.2  ]\n",
      "we booked the excellence after changing from another resort we booked\n",
      "          +++ location +++ [0.199 0.199 0.202 0.2   0.199]\n",
      "we booked the other one a little quickly and then read some really bad reviews\n",
      "          +++ location +++ [0.199 0.2   0.202 0.2   0.2  ]\n",
      "so we were able to get out of that one and do a little more homework\n",
      "          +++ location +++ [0.201 0.199 0.201 0.199 0.2  ]\n",
      "we read about the excellence from trip advisor and were really excited to go\n",
      "          +++ value +++ [0.201 0.198 0.201 0.198 0.201]\n",
      "now , we 've been to the caribbean plenty and are low maintenance travelers\n",
      "          +++ location +++ [0.2   0.199 0.201 0.199 0.2  ]\n",
      "we 'll check in and the hotel usually does not hear from us until we leave\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "one thing we usually like to do is get to the front desk and see about upgrading rooms\n",
      "          +++ location +++ [0.2   0.199 0.201 0.199 0.2  ]\n",
      "we were originally booked in a garden view room\n",
      "          +++ location +++ [0.199 0.2   0.203 0.199 0.199]\n",
      "our first question at the front desk was , ??o you have any ocean view rooms available\n",
      "          +++ value +++ [0.204 0.199 0.198 0.198 0.201]\n",
      "he said , ??es , let me tell you about the excellence club\n",
      "          +++ value +++ [0.201 0.199 0.2   0.199 0.2  ]\n",
      "the excellence club rooms are identical to every other room in the resort ( but with a plasma tv ) and you have access to what?? basically another room where you can eat ( the same food that?? served everywhere else ) , have premium drinks , and check out dvds\n",
      "          +++ location +++ [0.201 0.199 0.203 0.198 0.2  ]\n",
      "he showed us where we would be staying\n",
      "          +++ location +++ [0.2   0.2   0.201 0.2   0.2  ]\n",
      "it was perfect , right in front of the pool over looking the ocean\n",
      "          +++ location +++ [0.175 0.183 0.292 0.176 0.174]\n",
      "much to our dismay , we show up and still have a garden view\n",
      "          +++ location +++ [0.2   0.199 0.201 0.199 0.2  ]\n",
      "we call the front desk and ask where the ocean view room is and he commences to telling us how wonderful the excellence club is\n",
      "          +++ location +++ [0.201 0.199 0.201 0.199 0.2  ]\n",
      "so great , the hustle is on\n",
      "          +++ location +++ [0.196 0.195 0.217 0.194 0.197]\n",
      "after spending much of the first night arguing back and forth while he?? ??ooking into it?\n",
      "          +++ value +++ [0.201 0.199 0.2   0.199 0.2  ]\n",
      "we finally gave up and waited until the morning\n",
      "          +++ service +++ [0.2   0.199 0.2   0.2   0.201]\n",
      "they finally moved us to a pool front room but i still don?? why we had to pay $400 extra per couple for a plasma tv and movies\n",
      "          +++ value +++ [0.201 0.199 0.2   0.199 0.2  ]\n",
      "the movies ended up being ok since one morning i stayed in bed with an upset stomach and my wife did the same thing a couple of nights\n",
      "          +++ value +++ [0.205 0.199 0.198 0.198 0.199]\n",
      "the resort was pretty much empty so we were a little confused why there were no ocean front rooms available\n",
      "          +++ value +++ [0.209 0.196 0.202 0.196 0.197]\n",
      "the restaurants were empty , the bars were empty\n",
      "          +++ value +++ [0.216 0.194 0.201 0.195 0.195]\n",
      "i guess enjoying their ocean front rooms\n",
      "          +++ location +++ [0.199 0.198 0.208 0.198 0.198]\n",
      "another strange phenomenon was with it being so empty , why were the waits so long\n",
      "          +++ value +++ [0.206 0.198 0.2   0.198 0.198]\n",
      "waiting for service , waiting for a table , waiting for a drink , waiting at the front desk\n",
      "          +++ value +++ [0.334 0.162 0.162 0.162 0.179]\n",
      "the place was empty\n",
      "          +++ value +++ [0.223 0.193 0.195 0.194 0.196]\n",
      "there were a few nice waiters but most of them were rude , acted as if we were bothering them and sometimes just stood there and looked at us like we were stupid or something\n",
      "          +++ service +++ [0.19  0.157 0.157 0.157 0.34 ]\n",
      "i was amazed\n",
      "          +++ value +++ [0.218 0.193 0.193 0.193 0.202]\n",
      "i would recommend bringing an electrician with you as well because you??l need to get the power turned on if you want to dry your hair and run the air conditioner at the same time\n",
      "          +++ value +++ [0.201 0.2   0.2   0.2   0.2  ]\n",
      "watch your step walking underneath the vent as well so you won?? slip in the leaking water from the vent\n",
      "          +++ value +++ [0.209 0.2   0.197 0.197 0.197]\n",
      "we had roaches in our bar , the couple that went with us noticed dead flies stuck to their mirror in clear sight and they stayed there the whole time we were there\n",
      "          +++ clean +++ [0.224 0.195 0.161 0.258 0.162]\n",
      "average at best\n",
      "          +++ value +++ [0.207 0.199 0.197 0.2   0.197]\n",
      "the quality wasn?? very good\n",
      "          +++ value +++ [0.226 0.193 0.195 0.194 0.192]\n",
      "we spent the whole trip scared of it after we were in the bathroom the next morning\n",
      "          +++ value +++ [0.213 0.196 0.198 0.197 0.195]\n",
      "we called room service one night and they were out of pepperonis for the pizza\n",
      "          +++ value +++ [0.301 0.164 0.164 0.164 0.208]\n",
      "( that was more funny than annoying ) and our friends went to the italian restaurant and ate dinner\n",
      "          +++ value +++ [0.249 0.182 0.182 0.182 0.204]\n",
      "after , they ordered dessert : ??e??e out of tiramisu\n",
      "          +++ value +++ [0.22  0.194 0.194 0.194 0.199]\n",
      "??e have one cheesecake?\n",
      "          +++ value +++ [0.201 0.2   0.2   0.2   0.2  ]\n",
      "ok , what kind\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "??o , we have one piece of cheesecake left\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "ran out of dessert\n",
      "          +++ value +++ [0.248 0.188 0.188 0.188 0.189]\n",
      "like i said before , we??e low maintenance travelers\n",
      "          +++ value +++ [0.205 0.198 0.198 0.199 0.2  ]\n",
      "we go to a cheaper place and have to wait or deal with annoyances , we don?? mind\n",
      "          +++ value +++ [0.204 0.199 0.199 0.198 0.2  ]\n",
      "you get what you pay for\n",
      "          +++ value +++ [0.203 0.199 0.199 0.199 0.201]\n",
      "but this is supposed to be a 4-5 star establishment and it seemed as though they didn?? know what the heck they were doing\n",
      "          +++ location +++ [0.201 0.199 0.202 0.199 0.2  ]\n",
      "it was just a comedy of issues from the time we showed up to the time we left\n",
      "          +++ value +++ [0.238 0.192 0.189 0.189 0.192]\n",
      "the whole trip was ruined and that?? a few thousand dollars we wish we had back\n",
      "          +++ location +++ [0.199 0.2   0.201 0.2   0.199]\n",
      "there are so many other options available down there , look elsewhere\n",
      "          +++ location +++ [0.199 0.2   0.201 0.2   0.2  ]\n",
      "i know we??l never return\n",
      "          +++ location +++ [0.2   0.2   0.201 0.2   0.2  ]\n",
      "===========\n",
      "truth:\n",
      "[3, 3, 3, 4, 3, 4]\n",
      "prediction:\n",
      "tensor(4)\n",
      "doc:\n",
      "we just got back yesterday from a one week stay at the excellence resort in punta cana\n",
      "          +++ location +++ [0.174 0.171 0.288 0.184 0.182]\n",
      "i had done my research online and found that with the exception of the food , there were almost 100% positive things that people had to say about this resort\n",
      "          +++ location +++ [0.205 0.175 0.242 0.18  0.197]\n",
      "it all lived up to be true\n",
      "          +++ location +++ [0.217 0.17  0.247 0.179 0.189]\n",
      "the resort was magical\n",
      "          +++ location +++ [0.202 0.171 0.248 0.199 0.18 ]\n",
      "my wife and i went with 2 friends of ours and were blown away\n",
      "          +++ location +++ [0.179 0.169 0.303 0.171 0.178]\n",
      "the pool and beach were spectacular\n",
      "          +++ location +++ [0.17  0.159 0.347 0.164 0.161]\n",
      "as tropical beach rooms go , the rooms were nicely appointed with an open feel\n",
      "          +++ location +++ [0.18  0.188 0.278 0.184 0.169]\n",
      "bathtub in the room , shower in the bathroom , was a nice tough\n",
      "          +++ location +++ [0.2   0.21  0.238 0.183 0.169]\n",
      "the only negative thing i can say about this experience was the food\n",
      "          +++ location +++ [0.19  0.186 0.247 0.187 0.191]\n",
      "even though at all the restaurants you ordered from menus , the food was prepared banquet style\n",
      "          +++ value +++ [0.237 0.172 0.229 0.175 0.187]\n",
      "what i mean by this is that it was not as hot and fresh as food would typically be that is prepared for you\n",
      "          +++ value +++ [0.213 0.192 0.207 0.192 0.196]\n",
      "if you are a person that typically eats in nice restaurants in cities like new york , san francisco , etc\n",
      "          +++ location +++ [0.2   0.198 0.205 0.198 0.199]\n",
      "you will probably rate this food around a c+ or b -\n",
      "          +++ location +++ [0.204 0.19  0.22  0.191 0.195]\n",
      "though i am not typically a buffet type person , i would highly recommend you go to their buffet for both breakfast and lunch\n",
      "          +++ location +++ [0.222 0.179 0.227 0.179 0.193]\n",
      "you will not be disappointed as they will prepare eggs , meats , pastas individually for you\n",
      "          +++ location +++ [0.21  0.189 0.211 0.19  0.199]\n",
      "lastly on the food , steer clear of the lobster house\n",
      "          +++ location +++ [0.2   0.17  0.267 0.177 0.187]\n",
      "it was the only restaurant that highly disappointed us\n",
      "          +++ location +++ [0.197 0.196 0.212 0.197 0.197]\n",
      "every place else ( the mexican , french , italian , etc\n",
      "          +++ location +++ [0.223 0.166 0.255 0.178 0.178]\n",
      ") provided the ambiance , a decent meal , wine , etc\n",
      "          +++ location +++ [0.222 0.164 0.269 0.169 0.176]\n",
      "for a very nice evening\n",
      "          +++ location +++ [0.18  0.169 0.303 0.17  0.177]\n",
      "outside of the food , the resort and its people were truly amazing\n",
      "          +++ service +++ [0.175 0.164 0.237 0.183 0.241]\n",
      "we will definitely go back\n",
      "          +++ location +++ [0.184 0.168 0.29  0.173 0.185]\n",
      "this review was written by an ad agency executive who travels almost half the year in mostly big cities like la , new york , las vegas and san francisco\n",
      "          +++ location +++ [0.194 0.195 0.216 0.198 0.197]\n",
      "i am lucky enough to entertain clients in very fine restaurants and would probably be considered a food snob\n",
      "          +++ location +++ [0.195 0.195 0.217 0.196 0.197]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor(4)\n",
      "doc:\n",
      "life does not get better than this\n",
      "          +++ location +++ [0.193 0.192 0.219 0.197 0.199]\n",
      "my husband and i stayed at excellence punta cana from 10th nov - 24th nov\n",
      "          +++ location +++ [0.177 0.164 0.3   0.177 0.182]\n",
      "this was our first holiday in 2 years , so we were looking for somewhere where we can just relax on a beautiful beach or by the pool\n",
      "          +++ location +++ [0.186 0.169 0.283 0.178 0.184]\n",
      "we did not expect much with regards to the food after having read poor reviews and to be honest some of them had us a little worried\n",
      "          +++ location +++ [0.199 0.186 0.228 0.185 0.201]\n",
      "i am pleased to say that excellence punta cana exceeded our expectations by a mile\n",
      "          +++ location +++ [0.194 0.167 0.275 0.177 0.187]\n",
      "for the first couple of day , i just walked around with my mouth open\n",
      "          +++ location +++ [0.189 0.181 0.259 0.182 0.189]\n",
      "the place is absolutely breathtaking\n",
      "          +++ location +++ [0.179 0.167 0.302 0.182 0.17 ]\n",
      "we loved our room (3101) , clean , spacious and quiet\n",
      "          +++ location +++ [0.175 0.187 0.275 0.198 0.165]\n",
      "the beach is beautiful , the sea is so much fun to swim in ( we both like the waves\n",
      "          +++ location +++ [0.162 0.156 0.367 0.157 0.158]\n",
      ") , the pool is huge and sparkling clean with plenty of beds/loungers\n",
      "          +++ location +++ [0.176 0.162 0.314 0.175 0.173]\n",
      "as i said we did not expect much and we 're not sure why people are complaining so much ( bearing in mind that we are very fussy eaters )\n",
      "          +++ location +++ [0.2   0.2   0.202 0.199 0.199]\n",
      "for a start it is an all - inclusive resort not a michelin - star restaurant\n",
      "          +++ location +++ [0.198 0.192 0.227 0.192 0.192]\n",
      "the choices were plentiful and the food was fresh , well presented and well cooked\n",
      "          +++ location +++ [0.211 0.164 0.264 0.183 0.178]\n",
      "we were there for 2 weeks and could have probably stayed another 2 weeks without getting bored of the food\n",
      "          +++ location +++ [0.198 0.191 0.225 0.191 0.194]\n",
      "the entertainment team do a great job , the bar staff are fun , beach  pool servers always around , the waiting staff friendly and attentive , the housekeeping ladies kept our room spotlessly clean and tidy\n",
      "          +++ location +++ [0.178 0.166 0.224 0.214 0.217]\n",
      "i 'm not going to single out any staff in particular because i think every single one played a big part in having made our stay so enjoyable , and in particular those behind the scenes who works so hard but do not get the tips or the credit they derserve\n",
      "          +++ location +++ [0.196 0.196 0.213 0.196 0.199]\n",
      "we 've had a wonderful time and although we usually prefer not to return to the same place ( so as not to spoil it ) , excellence punta cana is one of those places that we would certainly not hesitate going back to\n",
      "          +++ location +++ [0.195 0.194 0.22  0.195 0.196]\n",
      "in fact we would have quite liked to move in\n",
      "          +++ location +++ [0.194 0.196 0.222 0.194 0.194]\n",
      "there are so much more to say , but i could go on forever\n",
      "          +++ location +++ [0.198 0.199 0.208 0.198 0.197]\n",
      "this place is a taste of heaven , go with the right attitude and you will not be dissapointed\n",
      "          +++ location +++ [0.191 0.188 0.24  0.189 0.191]\n",
      "everything that they offer must surely satisfy 90% of guests\n",
      "          +++ location +++ [0.188 0.188 0.245 0.19  0.189]\n",
      "as for the other 10% , there is no satisfaction no matter what you do\n",
      "          +++ location +++ [0.198 0.199 0.206 0.198 0.198]\n",
      "they should probably rather stay at home and certainly not travel to a 3rd world country\n",
      "          +++ location +++ [0.2   0.2   0.201 0.2   0.2  ]\n",
      "ps : 1\n",
      "          +++ location +++ [0.197 0.197 0.21  0.198 0.198]\n",
      "enjoy the ride from the airport (50minutes ) , its an excursion on its own , saves having to pay to go on one in precious holiday time\n",
      "          +++ location +++ [0.199 0.199 0.205 0.199 0.199]\n",
      "find the soft serve machine by cafe kafe bar\n",
      "          +++ location +++ [0.198 0.199 0.206 0.199 0.199]\n",
      "nancy 's shop is the best , she wo not rip you off\n",
      "          +++ location +++ [0.199 0.2   0.202 0.2   0.2  ]\n",
      "( with the other vendors everything always starts at $200, and you can get it down to $15, nancy gives a reasonable price from the start )4\n",
      "          +++ location +++ [0.199 0.199 0.203 0.199 0.199]\n",
      "go to the sports bar for ice cold water\n",
      "          +++ location +++ [0.198 0.199 0.206 0.199 0.198]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor(5)\n",
      "doc:\n",
      "everything is excellent at excellence punta cana we just got back from our honeymoon\n",
      "          +++ location +++ [0.171 0.165 0.286 0.187 0.191]\n",
      "we had an amazing time at excellence punta cana\n",
      "          +++ location +++ [0.182 0.167 0.286 0.18  0.185]\n",
      "we felt we were in paradise\n",
      "          +++ location +++ [0.176 0.174 0.285 0.179 0.186]\n",
      "the hotel and service was excellent\n",
      "          +++ service +++ [0.173 0.164 0.231 0.188 0.244]\n",
      "the whole staff was very friendly and so polite\n",
      "          +++ service +++ [0.153 0.152 0.159 0.156 0.38 ]\n",
      "my husband and i did not want to leave the resort\n",
      "          +++ location +++ [0.195 0.196 0.217 0.196 0.197]\n",
      "they made us feel very special\n",
      "          +++ location +++ [0.19  0.191 0.225 0.193 0.2  ]\n",
      "we have never experience a trip like this one\n",
      "          +++ location +++ [0.2   0.2   0.201 0.2   0.2  ]\n",
      "we loved it so much we are gathering a group of couple to go again early next summer\n",
      "          +++ location +++ [0.182 0.186 0.255 0.189 0.188]\n",
      "we never experience a problem while our stay\n",
      "          +++ location +++ [0.199 0.2   0.201 0.2   0.2  ]\n",
      "all of our belongings we safe , nothing was missing\n",
      "          +++ location +++ [0.196 0.197 0.211 0.198 0.197]\n",
      "ca not wait to go back\n",
      "          +++ location +++ [0.199 0.199 0.202 0.2   0.2  ]\n",
      "much love to maria isabel and carlos from excellence club concierge - - from mr\n",
      "          +++ location +++ [0.167 0.168 0.278 0.177 0.209]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor(1)\n",
      "doc:\n",
      "the pool and beach were beautiful this hotel catered to every persons needs\n",
      "          +++ location +++ [0.174 0.161 0.331 0.167 0.168]\n",
      "i love the fact that it is all inclusive food and alcohol\n",
      "          +++ location +++ [0.175 0.167 0.318 0.169 0.171]\n",
      "just because the hotel says it is all inclusive does not mean you dont have to tip\n",
      "          +++ location +++ [0.198 0.197 0.21  0.197 0.198]\n",
      "the people there work 12 days straight with 2 days off\n",
      "          +++ location +++ [0.193 0.192 0.221 0.194 0.2  ]\n",
      "they work up to 16 hours a day to make your experience there comfortable\n",
      "          +++ location +++ [0.194 0.194 0.218 0.195 0.199]\n",
      "so i made sure i tipped everyone who helped me\n",
      "          +++ location +++ [0.196 0.194 0.214 0.195 0.201]\n",
      "it is 35 pesos to one american dollar so you can do the math\n",
      "          +++ location +++ [0.2   0.2   0.201 0.2   0.2  ]\n",
      "the people that work in entertainment , cesar , mariel , ines , whinny were all very good\n",
      "          +++ location +++ [0.18  0.179 0.255 0.182 0.204]\n",
      "they taught me and my husband many things about there culture such as dancing\n",
      "          +++ location +++ [0.196 0.196 0.214 0.197 0.197]\n",
      "also there was a bartender named juan who made the best drinks there\n",
      "          +++ location +++ [0.197 0.197 0.21  0.198 0.198]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "asp_inc_overall = False\n",
    "if not asp_inc_overall: \n",
    "    nasp_analysis = hyper_params[\"num_aspect\"] - 1\n",
    "else:\n",
    "    nasp_analysis = hyper_params[\"num_aspect\"]\n",
    "    \n",
    "np.set_printoptions(precision=3)\n",
    "asp_name = [\"overall\", \"value\", \"room\", \"location\", \"clean\", \"service\"]\n",
    "for i in range(10):\n",
    "    print(\"truth:\")\n",
    "    print(df_test.iloc[i,0:6].values.flatten().tolist() )\n",
    "    print(\"prediction:\")\n",
    "    print( torch.argmax(outs[i][0:6],dim=1) )\n",
    "    print(\"doc:\")\n",
    "    dasp = torch.argmax(asps[i][:,0:nasp_analysis],dim=1).numpy()\n",
    "    if asp_inc_overall: dasp_noall = torch.argmax(asps[i][:,1:6],dim=1).numpy()\n",
    "    dasp_dist = torch.nn.functional.softmax(asps[i][:,0:nasp_analysis], dim=1).numpy()\n",
    "    for senti,s in enumerate(df_test.iloc[i,6]):\n",
    "        print(s)\n",
    "        if asp_inc_overall:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]] + \" +++ \" + asp_name[dasp_noall[senti]+1] + \" +++ \" + str(dasp_dist[senti]) )\n",
    "        else:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]+1] + \" +++ \" + str(dasp_dist[senti]) )\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize regression output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth:\n",
      "[1, 0, 0, 3, 1, 1]\n",
      "prediction:\n",
      "tensor([ 0.7459,  0.4020, -0.0947,  2.4229,  0.3754,  0.2812])\n",
      "doc:\n",
      "definitely not a 5 star resort i 'm dumbfounded that this hotel gets good reviews and is so highly rated\n",
      "          +++ location +++ [0.21 0.19 0.21 0.19 0.19]\n",
      "it 's decidedly a 3 star property , not 5 stars as indicated\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "the rooms are very dated and run down , old crappy beds and pillows , an old tv and overall poorly maintained\n",
      "          +++ room +++ [0.22 0.29 0.16 0.17 0.16]\n",
      "the whole property is pretty run down and old - looking\n",
      "          +++ value +++ [0.29 0.2  0.16 0.18 0.16]\n",
      "the food is subpar , not one meal i had would be called great\n",
      "          +++ value +++ [0.35 0.16 0.16 0.16 0.16]\n",
      "the service is uneven and the staff is poorly trained and uninformed\n",
      "          +++ service +++ [0.21 0.16 0.16 0.16 0.31]\n",
      "many do not comprehend english\n",
      "          +++ service +++ [0.21 0.18 0.18 0.18 0.26]\n",
      "the beach is great , it 's the only redeeming factor\n",
      "          +++ location +++ [0.17 0.17 0.33 0.17 0.17]\n",
      "however the resort is a 1- hour taxi trip from the airport\n",
      "          +++ location +++ [0.19 0.18 0.27 0.18 0.18]\n",
      "===========\n",
      "truth:\n",
      "[2, 2, 1, 2, 3, 3]\n",
      "prediction:\n",
      "tensor([1.9636, 2.4336, 1.9585, 3.0838, 2.7330, 2.1050])\n",
      "doc:\n",
      "facilities need work\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.19]\n",
      "we visited excellence for 5 nights in december\n",
      "          +++ location +++ [0.19 0.19 0.22 0.19 0.19]\n",
      "our first room , #1112, had a safe that did not work and so - so air conditioning\n",
      "          +++ room +++ [0.2  0.22 0.2  0.19 0.19]\n",
      "when we went to the front desk to complain , we were told to go to the room and someone would be there within 15 minutes\n",
      "          +++ value +++ [0.21 0.2  0.2  0.19 0.2 ]\n",
      "45 minutes later , the safe guy showed up , but nobody for the a/c\n",
      "          +++ service +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the safe guy could not fix it\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "when he left , the electricity went out\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "it went out a second time before we finally went to the front desk to change rooms\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "we had dinner that night in the lobster house\n",
      "          +++ location +++ [0.19 0.2  0.22 0.19 0.2 ]\n",
      "do not waste your time on this one\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the lobster tails had about 2 bites of food included\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "while we were in there , the electricity went out again\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "room 3002 served us pretty well , until night #3 when my partner got up to go to the bathroom and stepped into an inch of water\n",
      "          +++ value +++ [0.21 0.2  0.2  0.19 0.2 ]\n",
      "a hose had broken on the back of the toilet and flooded our room\n",
      "          +++ room +++ [0.22 0.27 0.17 0.18 0.17]\n",
      "it would 've been ok , but when we went to the front desk we were told that we needed to wait until noon to see if perhaps they could move us to another room\n",
      "          +++ value +++ [0.21 0.2  0.19 0.19 0.21]\n",
      "the front desk clerks were not empowered to just move us\n",
      "          +++ service +++ [0.21 0.19 0.2  0.19 0.21]\n",
      "my partner was infuriated that they wanted us to wait 4 hours for a new room\n",
      "          +++ service +++ [0.21 0.19 0.19 0.19 0.21]\n",
      "finally , matias at the front desk finally arranged to have us moved to another upgraded room - 3109\n",
      "          +++ location +++ [0.2  0.2  0.21 0.19 0.2 ]\n",
      "we walked in and saw the leak coming from the ceiling and nearly flipped\n",
      "          +++ room +++ [0.2  0.21 0.2  0.19 0.19]\n",
      "we finally got into #3110, which was a gorgeous suite with a beautiful view\n",
      "          +++ location +++ [0.18 0.23 0.24 0.18 0.17]\n",
      "on the positive side , the food at the other restaurants was very good\n",
      "          +++ location +++ [0.2  0.18 0.27 0.18 0.18]\n",
      "i particularly liked the french restaurant , while my partner liked the asian restaurant\n",
      "          +++ location +++ [0.23 0.17 0.25 0.17 0.17]\n",
      "the breakfast buffet was like nothing i 'd ever seen before - lots of choices\n",
      "          +++ location +++ [0.21 0.18 0.24 0.18 0.19]\n",
      "the ocean was way too rough to enjoy , particularly if you 're not a strong swimmer\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.19]\n",
      "much of the beach was black flagged the entire time we were there , so if you 're a big ocean fan , i do not recommend this resort\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "my favorite part , by far , though , were the beds next to the pools and ocean\n",
      "          +++ location +++ [0.19 0.17 0.3  0.17 0.17]\n",
      "they were amazing\n",
      "          +++ location +++ [0.2  0.18 0.27 0.18 0.17]\n",
      "i guess you could particularly say so since the beds in the rooms were hard as rocks\n",
      "          +++ room +++ [0.2  0.21 0.2  0.19 0.19]\n",
      "all in all , a good trip - highly recommend the zip line tour\n",
      "          +++ value +++ [0.24 0.18 0.23 0.17 0.18]\n",
      "it was worth every penny\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.19]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 3, 4, 4]\n",
      "prediction:\n",
      "tensor([3.7656, 3.7867, 3.5754, 3.7618, 3.7608, 3.7562])\n",
      "doc:\n",
      "excellence was exactly that\n",
      "          +++ location +++ [0.19 0.19 0.24 0.2  0.2 ]\n",
      "my family and i stayed at the excellence punta cana from december 22 to december 29 of this year\n",
      "          +++ location +++ [0.18 0.18 0.28 0.18 0.19]\n",
      "it was an amazing time had by all that attended\n",
      "          +++ service +++ [0.19 0.17 0.22 0.18 0.24]\n",
      "we arrived at the resort around 4 am because of a delay at the airport in vancouver , but even at 4 am , the service of the bellhops and the front desk was up to par\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "our bags were unloaded and immediately tagged and set to one side of the lobby while we were handed cold scented towels to cool off with\n",
      "          +++ service +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "check in was fairly expedient and we were in our rooms within twenty minutes\n",
      "          +++ service +++ [0.19 0.2  0.2  0.19 0.2 ]\n",
      "i had never been to an all inclusive resort before , and wasted no time enjoying the pleasures of the mini bar in the room , as well as the ample storage space for our things\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "room service even at 4 am was great , the girl on the phone said it would be about 40 minutes for the food , which seemed a little long , but i think they only say that to cover there butts , because it took about 25 minutes at most\n",
      "          +++ service +++ [0.18 0.19 0.18 0.19 0.26]\n",
      "the activities during the day were well thought out , though , there was some delays and cancellations due to weather conditions ( beach volleyball cancelled to due strong winds\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "the entertainment staff was amazing and extremely friendly , a special thanks to all my friends , ines ( my fiance , ) altagracia ( who lovingly reffered to me as flaco loco , which translates into crazy skinny guy , ) eliza and johanna ( my disco dance partners , ) sexy cesar ( who taught me all the sexy dance moves i\n",
      "          +++ service +++ [0.17 0.16 0.18 0.17 0.32]\n",
      "now know , ) julio cesar ( the mc for the games and parties\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "the restaurants i ca not offer too much help with , i did not eat at all of them , but of the few i did eat at , i recommend toscana for the huge buffet everyday , breakfast here is well prepared and quite delicious ( although i do not recommend the scrambled eggs\n",
      "          +++ location +++ [0.19 0.19 0.22 0.2  0.21]\n",
      "the omlettes are delicious and you have to try one\n",
      "          +++ location +++ [0.18 0.17 0.27 0.19 0.19]\n",
      "for lunch , you have to check out the grill on the beach , different food everyday , always good , and makes the beach smell amazing\n",
      "          +++ location +++ [0.19 0.17 0.25 0.19 0.2 ]\n",
      "for dinner , i liked spice ( asian cuisine , ) agave ( mexican , but do not eat the calimari from here , very rubbery , ) the pizza that is delivered to the pool and the beach is awesome , make sure you try that\n",
      "          +++ location +++ [0.18 0.19 0.24 0.19 0.2 ]\n",
      "the bars were awesome , you get accustomed to speaking the language when ordering drinks , instead of drinking your usual bacardi and coke , try the brugal extra anejo , they call it the dominican babymaker , and it 's obvious why once you try it\n",
      "          +++ service +++ [0.18 0.19 0.2  0.19 0.23]\n",
      "the stuff tastes amazing and it does magic for someone trying to loosen up on the dance floor\n",
      "          +++ room +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the disco is great too , although sometimes a little empty , but still worth checking out\n",
      "          +++ room +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the worst part of my trip was the vendors , they do not let up , and i am a very well mannered person , which makes it hard to shut them down over and over again , make sure you do not tell them you like anything until you know you 're going to buy it , otherwise you 'll have to beat\n",
      "          +++ room +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "them off with a stick to get away\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "try to make it to the theatre for the shows at 10pm every night , they are worth it\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the ice breaker shows are fun too , it gets people into the swing of things\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "all in all , i would highly recommend this resort for anyone going on a honeymoon or a romantic time with the better half , there is not a very big single crowd , so parents beware taking your single sons and daughters to this resort if they 're looking to party with other singles\n",
      "          +++ location +++ [0.19 0.2  0.22 0.19 0.2 ]\n",
      "hope this helps you\n",
      "          +++ room +++ [0.2  0.21 0.2  0.2  0.2 ]\n",
      "adios amigos and amigas\n",
      "          +++ location +++ [0.18 0.18 0.25 0.18 0.21]\n",
      "===========\n",
      "truth:\n",
      "[3, 2, 3, 3, 3, 4]\n",
      "prediction:\n",
      "tensor([2.6946, 3.0415, 2.6796, 3.1548, 3.3594, 3.4948])\n",
      "doc:\n",
      "great service , nice hotel , mediocre food\n",
      "          +++ value +++ [0.23 0.18 0.22 0.19 0.19]\n",
      "my husband and i stayed at excellence for five nights mid - november\n",
      "          +++ location +++ [0.2  0.19 0.23 0.19 0.19]\n",
      "we booked our trip at the very last minute so we were not able to do a ton of research on the dominican but the hotel receives high ratings thorughout the web\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "after the one hour ride from the airport we arrived at the hotel and were greeted by everyone we met\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "i have to say that the staff at the hotel were very nice and made every effort to learn our names and greet us by name each time they saw us\n",
      "          +++ service +++ [0.17 0.16 0.18 0.17 0.33]\n",
      "we opted to upgrade to the excellence club and we are still trying to decide if we think it was worth it or not\n",
      "          +++ location +++ [0.2  0.19 0.23 0.19 0.19]\n",
      "as part of the excellence club , you are ushered to the club 's private lobby for check - in but , really , it almost just creates an unneccesary step in the check - in process and adds another person or two you feel like you should tip\n",
      "          +++ location +++ [0.21 0.19 0.21 0.19 0.2 ]\n",
      "the biggest benefits of the excellence club for us were the unlimited internet access , beach towels in the room ( they were hard to get otherwise ) , and the beach bag in our room\n",
      "          +++ location +++ [0.2  0.18 0.25 0.18 0.19]\n",
      "we did eat breakfast each morning in the excellence club which was nice because it was a small buffet and you did not have to deal with a crowd\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.2 ]\n",
      "the hotel itself was clean , the staff was very friendly , and nothing ever felt crowded\n",
      "          +++ service +++ [0.17 0.17 0.17 0.2  0.29]\n",
      "however , the food was not great\n",
      "          +++ location +++ [0.2  0.19 0.21 0.19 0.2 ]\n",
      "it was not bad - but it was not great\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "i 'm not a big eater but i was prepared to indulge on my vacation and there just was not anything i was crazy about\n",
      "          +++ value +++ [0.21 0.2  0.2  0.19 0.2 ]\n",
      "the presentation of the food was nice but it was just bland\n",
      "          +++ location +++ [0.21 0.17 0.26 0.17 0.18]\n",
      "i think that is the best way to describe it\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.19]\n",
      "the pizzas that were delivered to the pool area were good but it was unpredictable because you never knew when they would arrive\n",
      "          +++ location +++ [0.21 0.19 0.21 0.19 0.2 ]\n",
      "we went on two excursions - swimming with the sting - rays/sharks and the zip - line tour\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "we loved the zip - line excursion\n",
      "          +++ location +++ [0.19 0.18 0.28 0.18 0.18]\n",
      "the staff was great and our bus driver and tour guide were great\n",
      "          +++ location +++ [0.17 0.16 0.25 0.16 0.25]\n",
      "it was interesting to visit the sting - rays and swim with the sharks but the reef where we snorkeled was disappointing\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the fish were very small and there was not much to see\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the electricity went out in our room a handful of times , especially when i used the hairdryer\n",
      "          +++ room +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "also , our ac was terrible\n",
      "          +++ room +++ [0.2  0.21 0.2  0.2  0.2 ]\n",
      "they tried to repair it but it just never got cool\n",
      "          +++ room +++ [0.2  0.21 0.19 0.19 0.2 ]\n",
      "our room was big , though , and clean\n",
      "          +++ room +++ [0.19 0.28 0.18 0.18 0.17]\n",
      "we always got housekeeping service twice a day and they refilled our mini - bar daily\n",
      "          +++ service +++ [0.19 0.18 0.19 0.2  0.24]\n",
      "in many of the reviews , people said they got sick\n",
      "          +++ service +++ [0.2  0.2  0.2  0.2  0.21]\n",
      "our representative at the hotel ( through aaa ) warned us that many people think they get sick from the water or the food but they do not realize that having too many drinks with coconut in them will also do it\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "coconut is a natural laxative so you need to limit your consumption\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "i would still pack the immodium just to be sure\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "i could not decide if i wanted to give this hotel a 3/5 or a 4/5 but i decided to go up because of the friendly staff and the cleanliness of our room\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "i do not think i would go back because of the food but we had a nice time while we were there\n",
      "          +++ location +++ [0.2  0.2  0.21 0.19 0.2 ]\n",
      "we met a lot of great people at the swim up bar\n",
      "          +++ location +++ [0.19 0.17 0.27 0.17 0.19]\n",
      "===========\n",
      "truth:\n",
      "[3, 4, 3, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([3.2171, 3.2191, 3.3011, 3.1749, 3.6753, 3.9237])\n",
      "doc:\n",
      "very relaxing experience just returned from my 40th birthday romantic getaway with my husband\n",
      "          +++ location +++ [0.21 0.17 0.26 0.18 0.18]\n",
      "this was our first time in the dominican republic , and we have literally been to every single island in the caribbean\n",
      "          +++ location +++ [0.21 0.17 0.26 0.18 0.18]\n",
      "so i can assure you that my review will be short , sweet , and comprehensive\n",
      "          +++ location +++ [0.21 0.19 0.21 0.19 0.2 ]\n",
      "in general , we liked the dr , and the excellence was very nice\n",
      "          +++ value +++ [0.24 0.17 0.19 0.19 0.2 ]\n",
      "the top reasons why we liked excellence were : 1) no kids ( ie\n",
      "          +++ location +++ [0.21 0.18 0.24 0.18 0.19]\n",
      ", if i want to get away from my own kids , i definitely do not want to vacation with other peoples ' kids )\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "it was so quiet\n",
      "          +++ location +++ [0.21 0.19 0.23 0.19 0.19]\n",
      "2) the staff and the people in the dr in general\n",
      "          +++ service +++ [0.21 0.17 0.2  0.18 0.25]\n",
      "so genuinely friendly , helpful , and wonderful\n",
      "          +++ service +++ [0.16 0.15 0.15 0.16 0.38]\n",
      "believe me , this is not true in most other areas of the caribbean\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the all - inclusive feature\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "loved being served , served , served\n",
      "          +++ value +++ [0.25 0.16 0.23 0.17 0.18]\n",
      "i wanted to sit on my [ - - ] all day and just be a gluttonous pig\n",
      "          +++ value +++ [0.23 0.18 0.22 0.18 0.19]\n",
      "and this is the perfect place to do it\n",
      "          +++ location +++ [0.24 0.16 0.26 0.16 0.17]\n",
      "4) the best selection of beach and pool lounge chairs , beds , and hammocks i 've ever seen\n",
      "          +++ value +++ [0.26 0.16 0.22 0.17 0.18]\n",
      "there were palapas everywhere , so there was no shortage of shade\n",
      "          +++ location +++ [0.21 0.19 0.21 0.19 0.19]\n",
      "there were so many beds , you did not have to worry about not getting one\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "i 'd never had the chance to sleep on a beach bed , because usually hotels have only a few , so you end up looking longingly at the lucky few who get them\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "ok , so here 's what i did not love about excellence :1) beach is not swimmable\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "way too rough most of the time\n",
      "          +++ location +++ [0.19 0.19 0.26 0.19 0.19]\n",
      "for this reason alone , i 'd not return here\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "i 'm a beach fan , and love to swim in the warm caribbean sea\n",
      "          +++ location +++ [0.2  0.19 0.23 0.19 0.19]\n",
      "i 'm no food snob , but some of the food was downright bad\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "and you end up eating in the same spot for breakfast and lunch\n",
      "          +++ value +++ [0.22 0.19 0.2  0.19 0.2 ]\n",
      "even though they have like 7 restaurants - the majority of them are only open for dinner\n",
      "          +++ value +++ [0.25 0.18 0.19 0.18 0.19]\n",
      "we only stayed 4 nights , and we were definitely getting very tired of the breakfast/lunch selection by the 3rd day\n",
      "          +++ value +++ [0.26 0.18 0.19 0.18 0.19]\n",
      "overall , if you just want to relax by the pool and you do not care about not going in the beach , this is a very beautiful resort\n",
      "          +++ location +++ [0.22 0.18 0.23 0.18 0.18]\n",
      "the staff is wonderful\n",
      "          +++ service +++ [0.17 0.16 0.16 0.17 0.34]\n",
      "if you are looking for a place to party and be loud and crazy , this is not your place\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "===========\n",
      "truth:\n",
      "[1, 0, 2, 3, 1, 0]\n",
      "prediction:\n",
      "tensor([0.9469, 0.6124, 1.0473, 2.9372, 0.7080, 0.5512])\n",
      "doc:\n",
      "5- star views\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "2- star service i do not know where to start\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "the roaches in the room , the rude waiters , bartenders , front desk , the dead flies that stayed on our friends ' mirror the entire stay , the average at best food ( only one morning in the bathroom for longer than you would want ) , the 6,7,8 times i had to trip the breakers so my wife could use the\n",
      "          +++ value +++ [0.3  0.18 0.16 0.2  0.17]\n",
      "hair dryer without our power going out , or the waste of money the excellence club turned out to be\n",
      "          +++ value +++ [0.3  0.19 0.17 0.17 0.17]\n",
      "i guess i 'll start with the good\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the beach was fabulous\n",
      "          +++ location +++ [0.18 0.18 0.28 0.18 0.18]\n",
      "the resort itself , d?cor , pool , beach access was great\n",
      "          +++ location +++ [0.22 0.17 0.25 0.19 0.17]\n",
      "ok now for the rest of the trip\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "we booked the excellence after changing from another resort we booked\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "we booked the other one a little quickly and then read some really bad reviews\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "so we were able to get out of that one and do a little more homework\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "we read about the excellence from trip advisor and were really excited to go\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "now , we 've been to the caribbean plenty and are low maintenance travelers\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "we 'll check in and the hotel usually does not hear from us until we leave\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "one thing we usually like to do is get to the front desk and see about upgrading rooms\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "we were originally booked in a garden view room\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "our first question at the front desk was , ??o you have any ocean view rooms available\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "he said , ??es , let me tell you about the excellence club\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the excellence club rooms are identical to every other room in the resort ( but with a plasma tv ) and you have access to what?? basically another room where you can eat ( the same food that?? served everywhere else ) , have premium drinks , and check out dvds\n",
      "          +++ value +++ [0.21 0.2  0.2  0.19 0.2 ]\n",
      "he showed us where we would be staying\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "it was perfect , right in front of the pool over looking the ocean\n",
      "          +++ location +++ [0.18 0.17 0.32 0.17 0.16]\n",
      "much to our dismay , we show up and still have a garden view\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "we call the front desk and ask where the ocean view room is and he commences to telling us how wonderful the excellence club is\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "so great , the hustle is on\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.19]\n",
      "after spending much of the first night arguing back and forth while he?? ??ooking into it?\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "we finally gave up and waited until the morning\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "they finally moved us to a pool front room but i still don?? why we had to pay $400 extra per couple for a plasma tv and movies\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the movies ended up being ok since one morning i stayed in bed with an upset stomach and my wife did the same thing a couple of nights\n",
      "          +++ value +++ [0.22 0.2  0.2  0.2  0.2 ]\n",
      "the resort was pretty much empty so we were a little confused why there were no ocean front rooms available\n",
      "          +++ location +++ [0.21 0.19 0.22 0.19 0.19]\n",
      "the restaurants were empty , the bars were empty\n",
      "          +++ location +++ [0.21 0.19 0.21 0.19 0.19]\n",
      "i guess enjoying their ocean front rooms\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "another strange phenomenon was with it being so empty , why were the waits so long\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "waiting for service , waiting for a table , waiting for a drink , waiting at the front desk\n",
      "          +++ value +++ [0.28 0.17 0.17 0.17 0.19]\n",
      "the place was empty\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "there were a few nice waiters but most of them were rude , acted as if we were bothering them and sometimes just stood there and looked at us like we were stupid or something\n",
      "          +++ service +++ [0.23 0.16 0.16 0.16 0.28]\n",
      "i was amazed\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "i would recommend bringing an electrician with you as well because you??l need to get the power turned on if you want to dry your hair and run the air conditioner at the same time\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "watch your step walking underneath the vent as well so you won?? slip in the leaking water from the vent\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "we had roaches in our bar , the couple that went with us noticed dead flies stuck to their mirror in clear sight and they stayed there the whole time we were there\n",
      "          +++ clean +++ [0.24 0.18 0.16 0.26 0.16]\n",
      "average at best\n",
      "          +++ value +++ [0.22 0.2  0.2  0.2  0.19]\n",
      "the quality wasn?? very good\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "we spent the whole trip scared of it after we were in the bathroom the next morning\n",
      "          +++ value +++ [0.25 0.19 0.18 0.19 0.18]\n",
      "we called room service one night and they were out of pepperonis for the pizza\n",
      "          +++ value +++ [0.27 0.17 0.17 0.18 0.21]\n",
      "( that was more funny than annoying ) and our friends went to the italian restaurant and ate dinner\n",
      "          +++ value +++ [0.26 0.18 0.18 0.18 0.21]\n",
      "after , they ordered dessert : ??e??e out of tiramisu\n",
      "          +++ value +++ [0.22 0.19 0.2  0.19 0.2 ]\n",
      "??e have one cheesecake?\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "ok , what kind\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "??o , we have one piece of cheesecake left\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "ran out of dessert\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "like i said before , we??e low maintenance travelers\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "we go to a cheaper place and have to wait or deal with annoyances , we don?? mind\n",
      "          +++ value +++ [0.23 0.19 0.19 0.19 0.19]\n",
      "you get what you pay for\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "but this is supposed to be a 4-5 star establishment and it seemed as though they didn?? know what the heck they were doing\n",
      "          +++ value +++ [0.23 0.19 0.19 0.19 0.2 ]\n",
      "it was just a comedy of issues from the time we showed up to the time we left\n",
      "          +++ value +++ [0.29 0.18 0.18 0.18 0.18]\n",
      "the whole trip was ruined and that?? a few thousand dollars we wish we had back\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "there are so many other options available down there , look elsewhere\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "i know we??l never return\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "===========\n",
      "truth:\n",
      "[3, 3, 3, 4, 3, 4]\n",
      "prediction:\n",
      "tensor([3.5368, 3.7500, 3.7826, 3.7086, 3.8880, 3.9359])\n",
      "doc:\n",
      "we just got back yesterday from a one week stay at the excellence resort in punta cana\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "i had done my research online and found that with the exception of the food , there were almost 100% positive things that people had to say about this resort\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "it all lived up to be true\n",
      "          +++ location +++ [0.21 0.18 0.22 0.19 0.2 ]\n",
      "the resort was magical\n",
      "          +++ value +++ [0.23 0.19 0.19 0.21 0.19]\n",
      "my wife and i went with 2 friends of ours and were blown away\n",
      "          +++ location +++ [0.2  0.18 0.24 0.18 0.19]\n",
      "the pool and beach were spectacular\n",
      "          +++ location +++ [0.19 0.17 0.3  0.17 0.17]\n",
      "as tropical beach rooms go , the rooms were nicely appointed with an open feel\n",
      "          +++ room +++ [0.2  0.25 0.19 0.2  0.17]\n",
      "bathtub in the room , shower in the bathroom , was a nice tough\n",
      "          +++ room +++ [0.19 0.25 0.2  0.19 0.17]\n",
      "the only negative thing i can say about this experience was the food\n",
      "          +++ location +++ [0.2  0.2  0.22 0.2  0.2 ]\n",
      "even though at all the restaurants you ordered from menus , the food was prepared banquet style\n",
      "          +++ value +++ [0.22 0.19 0.21 0.19 0.19]\n",
      "what i mean by this is that it was not as hot and fresh as food would typically be that is prepared for you\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "if you are a person that typically eats in nice restaurants in cities like new york , san francisco , etc\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "you will probably rate this food around a c+ or b -\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "though i am not typically a buffet type person , i would highly recommend you go to their buffet for both breakfast and lunch\n",
      "          +++ location +++ [0.22 0.17 0.26 0.17 0.18]\n",
      "you will not be disappointed as they will prepare eggs , meats , pastas individually for you\n",
      "          +++ value +++ [0.22 0.19 0.21 0.19 0.19]\n",
      "lastly on the food , steer clear of the lobster house\n",
      "          +++ location +++ [0.21 0.19 0.22 0.19 0.19]\n",
      "it was the only restaurant that highly disappointed us\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "every place else ( the mexican , french , italian , etc\n",
      "          +++ location +++ [0.2  0.18 0.26 0.18 0.18]\n",
      ") provided the ambiance , a decent meal , wine , etc\n",
      "          +++ location +++ [0.22 0.17 0.26 0.18 0.17]\n",
      "for a very nice evening\n",
      "          +++ location +++ [0.19 0.18 0.26 0.19 0.18]\n",
      "outside of the food , the resort and its people were truly amazing\n",
      "          +++ service +++ [0.2  0.17 0.21 0.18 0.25]\n",
      "we will definitely go back\n",
      "          +++ service +++ [0.23 0.16 0.19 0.18 0.23]\n",
      "this review was written by an ad agency executive who travels almost half the year in mostly big cities like la , new york , las vegas and san francisco\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "i am lucky enough to entertain clients in very fine restaurants and would probably be considered a food snob\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.2 ]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([3.8198, 3.7284, 3.6880, 3.7372, 3.8990, 3.7634])\n",
      "doc:\n",
      "life does not get better than this\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "my husband and i stayed at excellence punta cana from 10th nov - 24th nov\n",
      "          +++ location +++ [0.2  0.19 0.24 0.19 0.19]\n",
      "this was our first holiday in 2 years , so we were looking for somewhere where we can just relax on a beautiful beach or by the pool\n",
      "          +++ location +++ [0.2  0.18 0.25 0.18 0.19]\n",
      "we did not expect much with regards to the food after having read poor reviews and to be honest some of them had us a little worried\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "i am pleased to say that excellence punta cana exceeded our expectations by a mile\n",
      "          +++ value +++ [0.23 0.17 0.23 0.17 0.19]\n",
      "for the first couple of day , i just walked around with my mouth open\n",
      "          +++ location +++ [0.21 0.19 0.21 0.19 0.2 ]\n",
      "the place is absolutely breathtaking\n",
      "          +++ location +++ [0.21 0.18 0.26 0.18 0.17]\n",
      "we loved our room (3101) , clean , spacious and quiet\n",
      "          +++ clean +++ [0.2  0.21 0.19 0.22 0.18]\n",
      "the beach is beautiful , the sea is so much fun to swim in ( we both like the waves\n",
      "          +++ location +++ [0.18 0.16 0.34 0.16 0.16]\n",
      ") , the pool is huge and sparkling clean with plenty of beds/loungers\n",
      "          +++ location +++ [0.2  0.16 0.3  0.17 0.17]\n",
      "as i said we did not expect much and we 're not sure why people are complaining so much ( bearing in mind that we are very fussy eaters )\n",
      "          +++ room +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "for a start it is an all - inclusive resort not a michelin - star restaurant\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.2 ]\n",
      "the choices were plentiful and the food was fresh , well presented and well cooked\n",
      "          +++ value +++ [0.24 0.16 0.22 0.18 0.19]\n",
      "we were there for 2 weeks and could have probably stayed another 2 weeks without getting bored of the food\n",
      "          +++ location +++ [0.21 0.19 0.21 0.19 0.19]\n",
      "the entertainment team do a great job , the bar staff are fun , beach  pool servers always around , the waiting staff friendly and attentive , the housekeeping ladies kept our room spotlessly clean and tidy\n",
      "          +++ service +++ [0.18 0.16 0.18 0.19 0.3 ]\n",
      "i 'm not going to single out any staff in particular because i think every single one played a big part in having made our stay so enjoyable , and in particular those behind the scenes who works so hard but do not get the tips or the credit they derserve\n",
      "          +++ service +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "we 've had a wonderful time and although we usually prefer not to return to the same place ( so as not to spoil it ) , excellence punta cana is one of those places that we would certainly not hesitate going back to\n",
      "          +++ location +++ [0.21 0.18 0.23 0.18 0.2 ]\n",
      "in fact we would have quite liked to move in\n",
      "          +++ location +++ [0.22 0.19 0.22 0.18 0.19]\n",
      "there are so much more to say , but i could go on forever\n",
      "          +++ value +++ [0.21 0.2  0.2  0.19 0.2 ]\n",
      "this place is a taste of heaven , go with the right attitude and you will not be dissapointed\n",
      "          +++ location +++ [0.22 0.17 0.24 0.18 0.19]\n",
      "everything that they offer must surely satisfy 90% of guests\n",
      "          +++ value +++ [0.21 0.18 0.21 0.19 0.2 ]\n",
      "as for the other 10% , there is no satisfaction no matter what you do\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "they should probably rather stay at home and certainly not travel to a 3rd world country\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "ps : 1\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "enjoy the ride from the airport (50minutes ) , its an excursion on its own , saves having to pay to go on one in precious holiday time\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "find the soft serve machine by cafe kafe bar\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "nancy 's shop is the best , she wo not rip you off\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "( with the other vendors everything always starts at $200, and you can get it down to $15, nancy gives a reasonable price from the start )4\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "go to the sports bar for ice cold water\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([4.3483, 3.9729, 3.8877, 3.8620, 4.0023, 4.0763])\n",
      "doc:\n",
      "everything is excellent at excellence punta cana we just got back from our honeymoon\n",
      "          +++ location +++ [0.19 0.17 0.24 0.19 0.2 ]\n",
      "we had an amazing time at excellence punta cana\n",
      "          +++ service +++ [0.21 0.18 0.2  0.19 0.22]\n",
      "we felt we were in paradise\n",
      "          +++ service +++ [0.18 0.17 0.23 0.18 0.24]\n",
      "the hotel and service was excellent\n",
      "          +++ service +++ [0.18 0.16 0.17 0.19 0.29]\n",
      "the whole staff was very friendly and so polite\n",
      "          +++ service +++ [0.15 0.15 0.15 0.16 0.38]\n",
      "my husband and i did not want to leave the resort\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "they made us feel very special\n",
      "          +++ service +++ [0.18 0.18 0.21 0.19 0.24]\n",
      "we have never experience a trip like this one\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "we loved it so much we are gathering a group of couple to go again early next summer\n",
      "          +++ location +++ [0.17 0.18 0.26 0.18 0.21]\n",
      "we never experience a problem while our stay\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "all of our belongings we safe , nothing was missing\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "ca not wait to go back\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "much love to maria isabel and carlos from excellence club concierge - - from mr\n",
      "          +++ service +++ [0.17 0.17 0.24 0.18 0.25]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([3.5024, 3.5823, 3.6353, 3.5938, 3.5964, 3.5995])\n",
      "doc:\n",
      "the pool and beach were beautiful this hotel catered to every persons needs\n",
      "          +++ location +++ [0.19 0.17 0.3  0.17 0.17]\n",
      "i love the fact that it is all inclusive food and alcohol\n",
      "          +++ location +++ [0.2  0.17 0.28 0.17 0.18]\n",
      "just because the hotel says it is all inclusive does not mean you dont have to tip\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the people there work 12 days straight with 2 days off\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.2 ]\n",
      "they work up to 16 hours a day to make your experience there comfortable\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.2 ]\n",
      "so i made sure i tipped everyone who helped me\n",
      "          +++ location +++ [0.2  0.18 0.23 0.18 0.22]\n",
      "it is 35 pesos to one american dollar so you can do the math\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the people that work in entertainment , cesar , mariel , ines , whinny were all very good\n",
      "          +++ location +++ [0.17 0.16 0.26 0.17 0.23]\n",
      "they taught me and my husband many things about there culture such as dancing\n",
      "          +++ location +++ [0.19 0.18 0.26 0.18 0.19]\n",
      "also there was a bartender named juan who made the best drinks there\n",
      "          +++ location +++ [0.19 0.19 0.23 0.19 0.19]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "asp_inc_overall = False\n",
    "if not asp_inc_overall: \n",
    "    nasp_analysis = hyper_params[\"num_aspect\"] - 1\n",
    "else:\n",
    "    nasp_analysis = hyper_params[\"num_aspect\"]\n",
    "    \n",
    "np.set_printoptions(precision=3)\n",
    "asp_name = [\"overall\", \"value\", \"room\", \"location\", \"clean\", \"service\"]\n",
    "for i in range(10):\n",
    "    print(\"truth:\")\n",
    "    print( df_test.iloc[i,0:6].values.flatten().tolist() )\n",
    "    print(\"prediction:\")\n",
    "    print( outs[i][0:6] )\n",
    "    print(\"doc:\")\n",
    "    dasp = torch.argmax(asps[i][:,0:nasp_analysis],dim=1).numpy()\n",
    "    if asp_inc_overall: dasp_noall = torch.argmax(asps[i][:,1:6],dim=1).numpy()\n",
    "    dasp_dist = torch.nn.functional.softmax(asps[i][:,0:nasp_analysis], dim=1).numpy()\n",
    "#     dasp_dist = asps[i][:,0:nasp_analysis].numpy()\n",
    "    for senti,s in enumerate(df_test.iloc[i,6]):\n",
    "        print(s)\n",
    "        if asp_inc_overall:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]] + \" +++ \" + asp_name[dasp_noall[senti]+1] + \" +++ \" + str(dasp_dist[senti]) )\n",
    "        else:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]+1] + \" +++ \" + str( np.around(dasp_dist[senti], decimals=2) ) )\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hotel_asp(asp_pred, asp_true, asp_inc_overall):\n",
    "    asp_to_id = {\"value\":0, \"room\":1, \"location\":2, \"cleanliness\":3, \"service\":4, \"none\":-1}\n",
    "    asp_true = np.array( [asp_to_id[l] for l in asp_true] )\n",
    "    print(\"total true: \" + str(len(asp_true)) )\n",
    "    print(\"total not none: \" + str(sum(asp_true>0)) )\n",
    "    \n",
    "    asp_pred_index = []\n",
    "    if asp_inc_overall:\n",
    "        for i in range(1000):\n",
    "            asp_pred_index.append( asp_pred[i][:,1:6].numpy().argsort() )\n",
    "    else:\n",
    "        for i in range(1000):\n",
    "            asp_pred_index.append( asp_pred[i][:,0:5].numpy().argsort() )\n",
    "    asp_pred_index = np.concatenate( asp_pred_index , axis=0)\n",
    "    \n",
    "    result_index = []\n",
    "    for i,lbl in enumerate(asp_true):\n",
    "        if(lbl==-1):\n",
    "            result_index.append(-1)\n",
    "        else:\n",
    "            at = np.where(asp_pred_index[i,] == lbl)\n",
    "            result_index.append(at[0][0])\n",
    "    result_index = np.array(result_index)\n",
    "    \n",
    "    print(\"Top 1 ACC:\")\n",
    "    print( sum(result_index>=4) / sum(result_index>=0) )\n",
    "    print(\"Top 2 ACC:\")\n",
    "    print( sum(result_index>=3) / sum(result_index>=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "yifan_label = open(dataset_dir + \"test_aspect_0.yifan.aspect\", \"r\").readlines()\n",
    "yifan_label = [s.split()[0] for s in yifan_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true: 502\n",
      "total not none: 242\n",
      "Top 1 ACC:\n",
      "0.3952569169960474\n",
      "Top 2 ACC:\n",
      "0.6403162055335968\n"
     ]
    }
   ],
   "source": [
    "eval_hotel_asp(asps, yifan_label, asp_inc_overall=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_label = open(dataset_dir + \"test_aspect_0.fan.aspect\", \"r\").readlines()\n",
    "fan_label = [s.split()[0] for s in fan_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true: 621\n",
      "total not none: 288\n",
      "Top 1 ACC:\n",
      "0.41297935103244837\n",
      "Top 2 ACC:\n",
      "0.6784660766961652\n"
     ]
    }
   ],
   "source": [
    "eval_hotel_asp(asps, fan_label, asp_inc_overall=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
