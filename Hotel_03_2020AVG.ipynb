{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/aeryen/2019nn/985dcb33868d415f9bcf006757f1ae92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "experiment = comet_ml.Experiment(project_name=\"2019nn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from fastai.text import *\n",
    "from data_helpers.Data import *\n",
    "from fastai.text.transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    \"max_sequence_length\": 40*70,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs1\": 12,\n",
    "    \"num_epochs2\": 15,\n",
    "    \"num_aspect\": 6,\n",
    "    \"num_rating\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LM Databunch and LM Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cls_db = load_data(\"./data/\", \"hotel_clas_databunch_2020.TraValTes\")\n",
    "cls_db = load_data(\"./data/\", \"hotel_clas_databunch_2020.TraVal\")\n",
    "cls_db.batch_size=hyper_params[\"batch_size\"]\n",
    "cls_db.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23008"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( cls_db.vocab.itos )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Feature Combo Pooling (1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_emb(output, start, end):\n",
    "    avg_pool = output[start:end, :].mean(dim=0)\n",
    "    return avg_pool\n",
    "\n",
    "def sentence_avgpool(outputs, mask, p_index):\n",
    "    output = outputs[-1]\n",
    "    doc_start = mask.int().sum(dim=1)\n",
    "    \n",
    "    batch = []\n",
    "    for doci in range(0,output.shape[0]):\n",
    "        pi = p_index[doci,:].nonzero(as_tuple=True)[0].int()\n",
    "        doc = []\n",
    "        for senti in range( len(pi) ):\n",
    "            if senti==0:\n",
    "                # from start of doc to end of first sent\n",
    "                doc.append( average_emb(output[doci,:,:], doc_start[doci], pi[senti]) )\n",
    "            else:\n",
    "                # from previous period to next\n",
    "                doc.append( average_emb(output[doci,:,:], pi[senti-1]+1, pi[senti]) )\n",
    "            \n",
    "        batch.append( torch.stack(doc, 0) )\n",
    "\n",
    "    return batch\n",
    "\n",
    "def sentence_finalpool(outputs, mask, p_index):\n",
    "    output = outputs[-1]\n",
    "    \n",
    "    batch = []\n",
    "    for doci in range(0,output.shape[0]):\n",
    "        doc = output[doci,p_index[doci,:],:]\n",
    "        batch.append( doc )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(Module):\n",
    "    \"Create an encoder over `module` that can process a full sentence.\"\n",
    "    def __init__(self, bptt:int, max_len:int, module:nn.Module, vocab, pad_idx:int=1):\n",
    "        print(\"Encoder init\")\n",
    "        self.max_len,self.bptt,self.module,self.pad_idx = max_len,bptt,module,pad_idx\n",
    "        self.vocab = vocab\n",
    "        self.period_index = self.vocab.stoi[\"xxperiod\"]\n",
    "\n",
    "    def concat(self, arrs:Collection[Tensor])->Tensor:\n",
    "        \"Concatenate the `arrs` along the batch dimension.\"\n",
    "        return [torch.cat([l[si] for l in arrs], dim=1) for si in range_of(arrs[0])]\n",
    "\n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()\n",
    "\n",
    "    def forward(self, input:LongTensor)->Tuple[Tensor,Tensor]:\n",
    "        bs,sl = input.size()\n",
    "        self.reset()\n",
    "        raw_outputs,outputs,masks = [],[],[]\n",
    "        p_index = []\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            r, o = self.module(input[:,i: min(i+self.bptt, sl)])\n",
    "            if i>(sl-self.max_len):\n",
    "                masks.append(input[:,i: min(i+self.bptt, sl)] == self.pad_idx)\n",
    "                raw_outputs.append(r)\n",
    "                outputs.append(o)\n",
    "                p_index.append( input[:,i: min(i+self.bptt, sl)] == self.period_index )\n",
    "\n",
    "                \n",
    "        # print(\"number of sentences in docs:\")\n",
    "#         n_sent = torch.sum( x==self.vocab.stoi[\"xxperiod\"] , dim=1)\n",
    "        # print(n_sent)\n",
    "        \n",
    "        # print(\"locating period marks\")\n",
    "        period_index = torch.cat(p_index,dim=1)\n",
    "        \n",
    "        return self.concat(raw_outputs),self.concat(outputs), \\\n",
    "               torch.cat(masks,dim=1),period_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLS 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_tag(\"CLAS02\")\n",
    "experiment.add_tag(\"2020AVG\")\n",
    "\n",
    "# ATTENTIONAL AVERAGING, COMPLETELY INDEPENDENT SENTI OUT\n",
    "\n",
    "class Cls02ATT400(Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "    def __init__(self, n_asp:int, n_rat:int, layers:Collection[int], drops:Collection[float]):\n",
    "        print(\"CLS init\")\n",
    "        print(\"Num Aspect: \"+str(n_asp) )\n",
    "        print(\"Num Rating: \"+str(n_rat) )\n",
    "        self.n_asp = n_asp\n",
    "        self.n_rat = n_rat\n",
    "        \n",
    "        self.hid_size = 128\n",
    "        \n",
    "        mod_layers = []\n",
    "        mod_layers += bn_drop_lin( 400, self.hid_size, p=0.5, actn=nn.GELU() )\n",
    "        mod_layers += bn_drop_lin( self.hid_size, self.n_asp, p=0.2, actn=torch.nn.Softmax(dim=1) )\n",
    "        self.aspect = nn.Sequential(*mod_layers)\n",
    "        \n",
    "        self.s0 = nn.Sequential(* (bn_drop_lin( 400, self.hid_size, p=0.5, actn=nn.GELU() ) + \n",
    "                                   bn_drop_lin( self.hid_size, self.n_rat, p=0.2, actn=None ) ) )\n",
    "        self.s1 = nn.Sequential(* (bn_drop_lin( 400, self.hid_size, p=0.5, actn=nn.GELU() ) + \n",
    "                                   bn_drop_lin( self.hid_size, self.n_rat, p=0.2, actn=None ) ) )\n",
    "        self.s2 = nn.Sequential(* (bn_drop_lin( 400, self.hid_size, p=0.5, actn=nn.GELU() ) + \n",
    "                                   bn_drop_lin( self.hid_size, self.n_rat, p=0.2, actn=None ) ) )\n",
    "        self.s3 = nn.Sequential(* (bn_drop_lin( 400, self.hid_size, p=0.5, actn=nn.GELU() ) + \n",
    "                                   bn_drop_lin( self.hid_size, self.n_rat, p=0.2, actn=None ) ) )\n",
    "        self.s4 = nn.Sequential(* (bn_drop_lin( 400, self.hid_size, p=0.5, actn=nn.GELU() ) + \n",
    "                                   bn_drop_lin( self.hid_size, self.n_rat, p=0.2, actn=None ) ) )\n",
    "        self.s5 = nn.Sequential(* (bn_drop_lin( 400, self.hid_size, p=0.5, actn=nn.GELU() ) + \n",
    "                                   bn_drop_lin( self.hid_size, self.n_rat, p=0.2, actn=None ) ) )\n",
    "#         self.s6 = nn.Sequential(* (bn_drop_lin( 400, self.hid_size, p=0.5, actn=nn.GeLU(inplace=True) ) + \n",
    "#                                    bn_drop_lin( self.hid_size, self.n_rat, p=0.2, actn=None ) ) )\n",
    "        self.sentiments = []\n",
    "        self.sentiments.append( self.s0 )\n",
    "        self.sentiments.append( self.s1 )\n",
    "        self.sentiments.append( self.s2 )\n",
    "        self.sentiments.append( self.s3 )\n",
    "        self.sentiments.append( self.s4 )\n",
    "        self.sentiments.append( self.s5 )\n",
    "#         self.sentiments.append( self.s6 )\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs,mask,p_index = input\n",
    "        \n",
    "        batch = sentence_avgpool(outputs, mask, p_index)\n",
    "        \n",
    "        allsent_emb = torch.cat(batch, dim=0)          # [n_sentence, emb400]\n",
    "        aspect_dist = self.aspect(allsent_emb)         # [n_sentence, aspect6]\n",
    "\n",
    "        sent_bmm = torch.bmm(aspect_dist.unsqueeze(2), allsent_emb.unsqueeze(1))  # [319, 7, 400]\n",
    "        \n",
    "        all_doc_emb = []\n",
    "        aspect_doc = []\n",
    "        sentim_doc = []\n",
    "        cur = 0\n",
    "        for doci in range(0, len(batch)):\n",
    "            sn = batch[doci].shape[0]\n",
    "            doc_emb_avg = torch.sum(sent_bmm[cur:(cur+sn), :, : ], dim=0, keepdim=True) # [1, 7, 400]\n",
    "            asp_w_sum = torch.sum(aspect_dist[cur:(cur+sn),:], dim=0, keepdim=True) # [1, 7]\n",
    "            doc_emb_avg = doc_emb_avg / asp_w_sum[:,:,None]                                 # [1, 7, 400]\n",
    "            all_doc_emb.append( doc_emb_avg )\n",
    "            aspect_doc.append( aspect_dist[cur:(cur+sn), :] )\n",
    "            \n",
    "            cur = cur + sn\n",
    "\n",
    "        all_doc_emb = torch.cat( all_doc_emb, dim=0 )          # [batch, asp, 400]\n",
    "        \n",
    "        result_senti = [ self.sentiments[aspi]( all_doc_emb[:,aspi,:] ) for aspi in range(0,self.n_asp)] # [batch, ra]\n",
    "        \n",
    "        result = torch.stack(result_senti, dim=1)  # [batch, asp, sentiment5]\n",
    "        \n",
    "        return result,raw_outputs,outputs,aspect_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_classifier(arch:Callable, vocab_sz:int, vocab, n_class:int, bptt:int=70, max_len:int=20*70, config:dict=None,\n",
    "                        drop_mult:float=1., lin_ftrs:Collection[int]=None, ps:Collection[float]=None,\n",
    "                        pad_idx:int=1) -> nn.Module:\n",
    "    \"Create a text classifier from `arch` and its `config`, maybe `pretrained`.\"\n",
    "    print(\"CUSTOM DEFINED CLASSIFIER\")\n",
    "    meta = text.learner._model_meta[arch]\n",
    "    config = ifnone(config, meta['config_clas']).copy()\n",
    "    for k in config.keys():\n",
    "        if k.endswith('_p'): config[k] *= drop_mult\n",
    "    if lin_ftrs is None: lin_ftrs = [50]\n",
    "    if ps is None:  ps = [0.1]*len(lin_ftrs)\n",
    "    layers = [config[meta['hid_name']] * 3] + lin_ftrs + [n_class]\n",
    "    ps = [config.pop('output_p')] + ps\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    encoder = SentenceEncoder(bptt, max_len, arch(vocab_sz, **config), vocab, pad_idx=pad_idx)\n",
    "    cls_layer = Cls02ATT400(n_asp=hyper_params[\"num_aspect\"], n_rat=hyper_params[\"num_rating\"], layers=layers, drops=ps)\n",
    "    model = SequentialRNN(encoder, cls_layer)\n",
    "    return model if init is None else model.apply(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classifier_learner(data:DataBunch, arch:Callable, bptt:int=70, max_len:int=20*70, config:dict=None,\n",
    "                            pretrained:bool=True, drop_mult:float=1., lin_ftrs:Collection[int]=None,\n",
    "                            ps:Collection[float]=None, **learn_kwargs) -> 'TextClassifierLearner':\n",
    "    \"Create a `Learner` with a text classifier from `data` and `arch`.\"\n",
    "    model = get_text_classifier(arch, len(data.vocab.itos), data.vocab, data.c, bptt=bptt, max_len=max_len,\n",
    "                                config=config, drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps)\n",
    "    meta = text.learner._model_meta[arch]\n",
    "    learn = RNNLearner(data, model, split_func=meta['split_clas'], **learn_kwargs)\n",
    "    if pretrained:\n",
    "        if 'url' not in meta:\n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], data=False)\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn = learn.load_pretrained(*fnames, strict=False)\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelCEL(nn.CrossEntropyLoss):\n",
    "    def forward(self, input, target, nasp=6):\n",
    "        target = target.long()\n",
    "        loss = 0\n",
    "        for i in range(nasp):\n",
    "            loss = loss + super(MultiLabelCEL, self).forward(input[:,i,:], target[:,i])\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(preds, targs, nasp=6, nrat=5):\n",
    "    preds = preds[:,0:nasp,:]\n",
    "    preds = preds.contiguous().view(-1, nrat)\n",
    "    preds = torch.max(preds, dim=1)[1]\n",
    "    targs = targs.contiguous().view(-1).long()\n",
    "    return (preds==targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_0(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,0]==targs[:,0]).float().mean()\n",
    "def acc_1(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,1]==targs[:,1]).float().mean()\n",
    "def acc_2(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,2]==targs[:,2]).float().mean()\n",
    "def acc_3(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,3]==targs[:,3]).float().mean()\n",
    "def acc_4(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,4]==targs[:,4]).float().mean()\n",
    "def acc_5(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1]\n",
    "    targs = targs.contiguous().long()\n",
    "    return (preds[:,5]==targs[:,5]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clas_mse0(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1].float()[:,0]\n",
    "    targs = targs.contiguous().float()[:,0]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)\n",
    "def clas_mse1(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1].float()[:,1]\n",
    "    targs = targs.contiguous().float()[:,1]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)\n",
    "def clas_mse2(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1].float()[:,2]\n",
    "    targs = targs.contiguous().float()[:,2]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)\n",
    "def clas_mse3(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1].float()[:,3]\n",
    "    targs = targs.contiguous().float()[:,3]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)\n",
    "def clas_mse4(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1].float()[:,4]\n",
    "    targs = targs.contiguous().float()[:,4]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)\n",
    "def clas_mse5(preds, targs):\n",
    "    preds = torch.max(preds, dim=2)[1].float()[:,5]\n",
    "    targs = targs.contiguous().float()[:,5]\n",
    "    return torch.nn.functional.mse_loss(preds, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM DEFINED CLASSIFIER\n",
      "Encoder init\n",
      "CLS init\n",
      "Num Aspect: 6\n",
      "Num Rating: 5\n"
     ]
    }
   ],
   "source": [
    "mloss = MultiLabelCEL()\n",
    "# mloss = MultiLabelMSE()\n",
    "experiment.add_tag(\"drop1.1\")\n",
    "cls_learn = text_classifier_learner(cls_db, AWD_LSTM, \n",
    "                                    loss_func=mloss,\n",
    "                                    drop_mult=1.1,\n",
    "                                    metrics=[multi_acc,acc_0,acc_1,acc_2,acc_3,acc_4,acc_5,\n",
    "                                            clas_mse0,clas_mse1,clas_mse2,clas_mse3,clas_mse4,clas_mse5],\n",
    "#                                     metrics=[mse_0,mse_1,mse_2,mse_3,mse_4,mse_5,\n",
    "#                                              multi_regr_acc,regr_acc0,regr_acc1,regr_acc2,regr_acc3,regr_acc4,regr_acc5],\n",
    "                                    bptt=70,\n",
    "                                    max_len=hyper_params[\"max_sequence_length\"],\n",
    "                                    clip=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_=cls_learn.load_encoder('lm_enc_hotel.1115')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAS 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "cls_learn.lr_find(num_it=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>acc_0</th>\n",
       "      <th>acc_1</th>\n",
       "      <th>acc_2</th>\n",
       "      <th>acc_3</th>\n",
       "      <th>acc_4</th>\n",
       "      <th>acc_5</th>\n",
       "      <th>clas_mse0</th>\n",
       "      <th>clas_mse1</th>\n",
       "      <th>clas_mse2</th>\n",
       "      <th>clas_mse3</th>\n",
       "      <th>clas_mse4</th>\n",
       "      <th>clas_mse5</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.605640</td>\n",
       "      <td>6.915482</td>\n",
       "      <td>0.481457</td>\n",
       "      <td>0.559775</td>\n",
       "      <td>0.506018</td>\n",
       "      <td>0.441562</td>\n",
       "      <td>0.432469</td>\n",
       "      <td>0.451993</td>\n",
       "      <td>0.496924</td>\n",
       "      <td>0.682001</td>\n",
       "      <td>0.994918</td>\n",
       "      <td>1.091468</td>\n",
       "      <td>1.271998</td>\n",
       "      <td>1.047071</td>\n",
       "      <td>1.116341</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.620160</td>\n",
       "      <td>6.991351</td>\n",
       "      <td>0.483061</td>\n",
       "      <td>0.551217</td>\n",
       "      <td>0.500134</td>\n",
       "      <td>0.456004</td>\n",
       "      <td>0.434608</td>\n",
       "      <td>0.468307</td>\n",
       "      <td>0.488098</td>\n",
       "      <td>0.733886</td>\n",
       "      <td>1.074351</td>\n",
       "      <td>1.175181</td>\n",
       "      <td>1.273068</td>\n",
       "      <td>1.188018</td>\n",
       "      <td>1.254079</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.350976</td>\n",
       "      <td>6.864302</td>\n",
       "      <td>0.488945</td>\n",
       "      <td>0.570741</td>\n",
       "      <td>0.485959</td>\n",
       "      <td>0.460818</td>\n",
       "      <td>0.433003</td>\n",
       "      <td>0.483017</td>\n",
       "      <td>0.500134</td>\n",
       "      <td>0.707141</td>\n",
       "      <td>1.097620</td>\n",
       "      <td>1.001872</td>\n",
       "      <td>1.672907</td>\n",
       "      <td>1.086654</td>\n",
       "      <td>1.163145</td>\n",
       "      <td>01:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.363955</td>\n",
       "      <td>6.842858</td>\n",
       "      <td>0.496033</td>\n",
       "      <td>0.595079</td>\n",
       "      <td>0.522867</td>\n",
       "      <td>0.430596</td>\n",
       "      <td>0.454934</td>\n",
       "      <td>0.476331</td>\n",
       "      <td>0.496389</td>\n",
       "      <td>0.596951</td>\n",
       "      <td>0.956405</td>\n",
       "      <td>1.229206</td>\n",
       "      <td>1.380316</td>\n",
       "      <td>1.053758</td>\n",
       "      <td>1.442097</td>\n",
       "      <td>01:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.540530</td>\n",
       "      <td>6.899132</td>\n",
       "      <td>0.466346</td>\n",
       "      <td>0.515913</td>\n",
       "      <td>0.492378</td>\n",
       "      <td>0.406526</td>\n",
       "      <td>0.444504</td>\n",
       "      <td>0.440492</td>\n",
       "      <td>0.498262</td>\n",
       "      <td>0.648034</td>\n",
       "      <td>0.945440</td>\n",
       "      <td>1.138272</td>\n",
       "      <td>1.408933</td>\n",
       "      <td>1.137470</td>\n",
       "      <td>1.111260</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.364589</td>\n",
       "      <td>6.694529</td>\n",
       "      <td>0.501159</td>\n",
       "      <td>0.577695</td>\n",
       "      <td>0.519123</td>\n",
       "      <td>0.476598</td>\n",
       "      <td>0.441294</td>\n",
       "      <td>0.484622</td>\n",
       "      <td>0.507622</td>\n",
       "      <td>0.608451</td>\n",
       "      <td>0.813854</td>\n",
       "      <td>1.029955</td>\n",
       "      <td>1.349291</td>\n",
       "      <td>0.982348</td>\n",
       "      <td>1.335651</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.380649</td>\n",
       "      <td>6.668269</td>\n",
       "      <td>0.499242</td>\n",
       "      <td>0.582509</td>\n",
       "      <td>0.533030</td>\n",
       "      <td>0.443434</td>\n",
       "      <td>0.448248</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.510832</td>\n",
       "      <td>0.642418</td>\n",
       "      <td>0.899171</td>\n",
       "      <td>0.964696</td>\n",
       "      <td>1.241508</td>\n",
       "      <td>1.232415</td>\n",
       "      <td>1.269323</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.225429</td>\n",
       "      <td>6.742981</td>\n",
       "      <td>0.498529</td>\n",
       "      <td>0.585718</td>\n",
       "      <td>0.517785</td>\n",
       "      <td>0.459481</td>\n",
       "      <td>0.447446</td>\n",
       "      <td>0.470714</td>\n",
       "      <td>0.510029</td>\n",
       "      <td>0.631720</td>\n",
       "      <td>0.936614</td>\n",
       "      <td>1.075689</td>\n",
       "      <td>1.501738</td>\n",
       "      <td>1.131051</td>\n",
       "      <td>1.317732</td>\n",
       "      <td>01:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.235619</td>\n",
       "      <td>6.601601</td>\n",
       "      <td>0.507578</td>\n",
       "      <td>0.587323</td>\n",
       "      <td>0.524204</td>\n",
       "      <td>0.467237</td>\n",
       "      <td>0.455202</td>\n",
       "      <td>0.493715</td>\n",
       "      <td>0.517785</td>\n",
       "      <td>0.641080</td>\n",
       "      <td>0.871089</td>\n",
       "      <td>0.986093</td>\n",
       "      <td>1.260497</td>\n",
       "      <td>1.005884</td>\n",
       "      <td>1.282429</td>\n",
       "      <td>01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.158160</td>\n",
       "      <td>6.556427</td>\n",
       "      <td>0.516136</td>\n",
       "      <td>0.594277</td>\n",
       "      <td>0.533030</td>\n",
       "      <td>0.480342</td>\n",
       "      <td>0.461621</td>\n",
       "      <td>0.506553</td>\n",
       "      <td>0.520995</td>\n",
       "      <td>0.599893</td>\n",
       "      <td>0.810377</td>\n",
       "      <td>0.975394</td>\n",
       "      <td>1.249264</td>\n",
       "      <td>0.937951</td>\n",
       "      <td>1.158331</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.012423</td>\n",
       "      <td>6.538460</td>\n",
       "      <td>0.517473</td>\n",
       "      <td>0.595881</td>\n",
       "      <td>0.542391</td>\n",
       "      <td>0.480075</td>\n",
       "      <td>0.462958</td>\n",
       "      <td>0.503076</td>\n",
       "      <td>0.520460</td>\n",
       "      <td>0.585451</td>\n",
       "      <td>0.779353</td>\n",
       "      <td>0.929660</td>\n",
       "      <td>1.247660</td>\n",
       "      <td>0.961220</td>\n",
       "      <td>1.167959</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.043033</td>\n",
       "      <td>6.512929</td>\n",
       "      <td>0.520861</td>\n",
       "      <td>0.599893</td>\n",
       "      <td>0.550949</td>\n",
       "      <td>0.481412</td>\n",
       "      <td>0.465098</td>\n",
       "      <td>0.503611</td>\n",
       "      <td>0.524204</td>\n",
       "      <td>0.568869</td>\n",
       "      <td>0.769457</td>\n",
       "      <td>0.946777</td>\n",
       "      <td>1.204065</td>\n",
       "      <td>0.957475</td>\n",
       "      <td>1.166087</td>\n",
       "      <td>01:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "# good loss\n",
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle( hyper_params[\"num_epochs1\"] , max_lr=slice(2e-3,2e-2), wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>acc_0</th>\n",
       "      <th>acc_1</th>\n",
       "      <th>acc_2</th>\n",
       "      <th>acc_3</th>\n",
       "      <th>acc_4</th>\n",
       "      <th>acc_5</th>\n",
       "      <th>clas_mse0</th>\n",
       "      <th>clas_mse1</th>\n",
       "      <th>clas_mse2</th>\n",
       "      <th>clas_mse3</th>\n",
       "      <th>clas_mse4</th>\n",
       "      <th>clas_mse5</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.648799</td>\n",
       "      <td>6.922476</td>\n",
       "      <td>0.490550</td>\n",
       "      <td>0.576357</td>\n",
       "      <td>0.495854</td>\n",
       "      <td>0.456539</td>\n",
       "      <td>0.441294</td>\n",
       "      <td>0.478738</td>\n",
       "      <td>0.494517</td>\n",
       "      <td>0.745386</td>\n",
       "      <td>1.143354</td>\n",
       "      <td>1.179995</td>\n",
       "      <td>1.384327</td>\n",
       "      <td>1.224926</td>\n",
       "      <td>1.404654</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.492063</td>\n",
       "      <td>7.035731</td>\n",
       "      <td>0.482794</td>\n",
       "      <td>0.570741</td>\n",
       "      <td>0.491040</td>\n",
       "      <td>0.431399</td>\n",
       "      <td>0.437550</td>\n",
       "      <td>0.473121</td>\n",
       "      <td>0.492913</td>\n",
       "      <td>0.699117</td>\n",
       "      <td>1.023536</td>\n",
       "      <td>1.317197</td>\n",
       "      <td>1.654453</td>\n",
       "      <td>1.065258</td>\n",
       "      <td>1.404921</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.379690</td>\n",
       "      <td>6.925615</td>\n",
       "      <td>0.483151</td>\n",
       "      <td>0.564322</td>\n",
       "      <td>0.500669</td>\n",
       "      <td>0.443702</td>\n",
       "      <td>0.441829</td>\n",
       "      <td>0.470714</td>\n",
       "      <td>0.477668</td>\n",
       "      <td>0.720781</td>\n",
       "      <td>1.067130</td>\n",
       "      <td>1.135330</td>\n",
       "      <td>1.346082</td>\n",
       "      <td>1.253276</td>\n",
       "      <td>1.273870</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.257359</td>\n",
       "      <td>6.764968</td>\n",
       "      <td>0.497905</td>\n",
       "      <td>0.577695</td>\n",
       "      <td>0.493715</td>\n",
       "      <td>0.462423</td>\n",
       "      <td>0.451458</td>\n",
       "      <td>0.500936</td>\n",
       "      <td>0.501204</td>\n",
       "      <td>0.661407</td>\n",
       "      <td>1.078096</td>\n",
       "      <td>1.063653</td>\n",
       "      <td>1.325756</td>\n",
       "      <td>0.974057</td>\n",
       "      <td>1.151110</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.223039</td>\n",
       "      <td>7.011464</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.552287</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.445841</td>\n",
       "      <td>0.437818</td>\n",
       "      <td>0.456272</td>\n",
       "      <td>0.492913</td>\n",
       "      <td>0.719444</td>\n",
       "      <td>1.097085</td>\n",
       "      <td>1.106446</td>\n",
       "      <td>1.614603</td>\n",
       "      <td>1.174378</td>\n",
       "      <td>1.332442</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.223490</td>\n",
       "      <td>6.693311</td>\n",
       "      <td>0.498797</td>\n",
       "      <td>0.571543</td>\n",
       "      <td>0.510564</td>\n",
       "      <td>0.475796</td>\n",
       "      <td>0.449051</td>\n",
       "      <td>0.493982</td>\n",
       "      <td>0.491843</td>\n",
       "      <td>0.611928</td>\n",
       "      <td>0.929125</td>\n",
       "      <td>1.037176</td>\n",
       "      <td>1.420701</td>\n",
       "      <td>1.004814</td>\n",
       "      <td>1.259695</td>\n",
       "      <td>01:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.117807</td>\n",
       "      <td>6.511022</td>\n",
       "      <td>0.512481</td>\n",
       "      <td>0.600695</td>\n",
       "      <td>0.537577</td>\n",
       "      <td>0.474993</td>\n",
       "      <td>0.461086</td>\n",
       "      <td>0.491575</td>\n",
       "      <td>0.508960</td>\n",
       "      <td>0.591602</td>\n",
       "      <td>0.871623</td>\n",
       "      <td>0.925381</td>\n",
       "      <td>1.367478</td>\n",
       "      <td>1.026745</td>\n",
       "      <td>1.253811</td>\n",
       "      <td>01:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.099850</td>\n",
       "      <td>6.509422</td>\n",
       "      <td>0.517161</td>\n",
       "      <td>0.602835</td>\n",
       "      <td>0.533833</td>\n",
       "      <td>0.480877</td>\n",
       "      <td>0.458946</td>\n",
       "      <td>0.500134</td>\n",
       "      <td>0.526344</td>\n",
       "      <td>0.601498</td>\n",
       "      <td>0.860390</td>\n",
       "      <td>0.929660</td>\n",
       "      <td>1.309174</td>\n",
       "      <td>0.966301</td>\n",
       "      <td>1.101364</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.016551</td>\n",
       "      <td>6.474551</td>\n",
       "      <td>0.519256</td>\n",
       "      <td>0.600963</td>\n",
       "      <td>0.535972</td>\n",
       "      <td>0.484087</td>\n",
       "      <td>0.466167</td>\n",
       "      <td>0.506018</td>\n",
       "      <td>0.522332</td>\n",
       "      <td>0.546670</td>\n",
       "      <td>0.773736</td>\n",
       "      <td>0.918160</td>\n",
       "      <td>1.251137</td>\n",
       "      <td>0.938486</td>\n",
       "      <td>1.117679</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.003228</td>\n",
       "      <td>6.462118</td>\n",
       "      <td>0.521218</td>\n",
       "      <td>0.599358</td>\n",
       "      <td>0.544531</td>\n",
       "      <td>0.493715</td>\n",
       "      <td>0.464563</td>\n",
       "      <td>0.503343</td>\n",
       "      <td>0.521797</td>\n",
       "      <td>0.580637</td>\n",
       "      <td>0.767317</td>\n",
       "      <td>0.892217</td>\n",
       "      <td>1.205937</td>\n",
       "      <td>0.920032</td>\n",
       "      <td>1.156191</td>\n",
       "      <td>01:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.904054</td>\n",
       "      <td>6.412464</td>\n",
       "      <td>0.523625</td>\n",
       "      <td>0.598556</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.491308</td>\n",
       "      <td>0.466167</td>\n",
       "      <td>0.512169</td>\n",
       "      <td>0.527949</td>\n",
       "      <td>0.551752</td>\n",
       "      <td>0.755282</td>\n",
       "      <td>0.907194</td>\n",
       "      <td>1.258358</td>\n",
       "      <td>0.890612</td>\n",
       "      <td>1.107783</td>\n",
       "      <td>01:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>6.922832</td>\n",
       "      <td>6.421584</td>\n",
       "      <td>0.524160</td>\n",
       "      <td>0.603370</td>\n",
       "      <td>0.549612</td>\n",
       "      <td>0.488901</td>\n",
       "      <td>0.465633</td>\n",
       "      <td>0.512437</td>\n",
       "      <td>0.525007</td>\n",
       "      <td>0.561915</td>\n",
       "      <td>0.772667</td>\n",
       "      <td>0.909067</td>\n",
       "      <td>1.265846</td>\n",
       "      <td>0.944103</td>\n",
       "      <td>1.131051</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "# average emb\n",
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle( hyper_params[\"num_epochs1\"] , max_lr=slice(2e-3,2e-2), wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web': 'https://www.comet.ml/api/image/download?imageId=faf88ee246a3472193e4b9ac7b42a932&experimentKey=4819cf71cd32475fbe9dae77f7d9c3b6',\n",
       " 'api': 'https://www.comet.ml/api/rest/v1/image/get-image?imageId=faf88ee246a3472193e4b9ac7b42a932&experimentKey=4819cf71cd32475fbe9dae77f7d9c3b6',\n",
       " 'imageId': 'faf88ee246a3472193e4b9ac7b42a932'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = cls_learn.recorder.plot_losses()\n",
    "experiment.log_figure(figure_name=\"train loss 01\", figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.save('hotel.clas.2020avg.1.learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (11173 items)\n",
       "x: TextList\n",
       "xxbos we had the most wonderful time at the excellence xxperiod we opted for the excellence club and it was well worht it for the perks xxperiod roberto puello who was the club manager did an extra special job of making us feel welcomed xxperiod we took some great pictures that looked like postcards xxperiod the food was excellent the pizza was close to being our favorite xxperiod the shows were great especially the michael jackson and the 70 's 80 's night xxperiod the french resturant was probably our favorite xxperiod we will definately be back for an anniversary trip xxperiod we made some friends but had plenty of xxunk as well xxperiod the rooms were great and everytime we left and came back it was cleaned , towels restocked and the bar was as well xxperiod,xxbos a truly fantastic place xmas we have just returned from a 2 week xmas new year break at the excellence punta cana xxperiod the hotel is magnificent and we have never experienced such incredible service before xxperiod the restaurants are great , with plenty of choice xxperiod the whole complex is so good that we only did one trip and that was for my wife to swim with the dolphins xxperiod great beaches and pools xxperiod the staff fall over each other to look after you xxperiod we stayed in a junior suite on ground level and it was spotlessly clean , great maid service and room service on tap 24 / 7 xxperiod overall i would say this is probably the best hotel we have ever stayed in on xxunk i am extremely fussy xxperiod the journey to the resort is a little uncomfortable , with the roads in a bad state xxperiod but once you are checked in the pace of life xxunk down and you forget about the outside world xxperiod worthy of special mention and our sincere thanks are : pedro from the cielo bar / pool an absolute gentleman xxperiod miguel the bell boy and also carlos from the xxunk bar xxperiod we will definately be booking again , probably for next christmas xxperiod jim eileen ( xxunk uk ) xxperiod,xxbos too sick to enjoy the ride to and form the airport is not good xxperiod very bumpy road and you can really see the poor living conditions xxperiod when you get to the resort it is like another world xxperiod we were told not to drink the tap or shower water - keep your mouth shut in the shower etc xxperiod but we have since heard that not all properties in punta cana have a water problem xxperiod in some resorts they have a huge purification system and the tap water is safe xxperiod both my husband and i were sick with diarrhea while we were there and put it down to the unclean water used to prepare food xxperiod we will not go on any more holidays where we have to worry about our health xxperiod each night at about 10 pm when we were settling down to sleep the music started up and we like fresh air but we had to close our patio doors to mute the sound xxperiod we really wanted a quieter resort xxperiod it was disappointing xxperiod it was partially our fault for not doing our research properly xxperiod some people did not hear the bands play at night but we could nt change our swim up room for a quieter one xxperiod we will not be returning to dominican republic xxperiod,xxbos relaxing and fun , but xxperiod i rang in the new year at excellence punta cana xxperiod since new york was freezing at the time , i was just happy to be in the sun xxperiod my husband and i had a great time overall xxperiod however , i was a little surprised to see this hotel rating itself as a 5- star facility ( more like a 4 ) xxperiod all of the basics were covered , but some of the details that i would expect from a 5- star place were missing xxperiod the bed , although comfortable creaked loudly and i couldn ? ? tell whether or not the headboard was attached properly xxperiod the staff were friendly and helpful overall , but in keeping up with the island mentality , their sense of urgency was lacking ( we had to ask two different people for hand towels and our one small room service order took close to an hour to arrive ) xxperiod actually , as mentioned by another reviewer , the food was the worst part xxperiod the presentation was excellent ( clean buffet areas , nicely decorated theme restaurants ) , but the dinner food quality wasn ? ? the best xxperiod one night i didn ? ? even finish my meal ( at the ? ? xxunk ? xxperiod on - site ) xxperiod lunch ( indoor buffet or beach bbq ) was usually the best bet xxperiod actually , everything on the beach ? ? xxunk the wait xxunk great xxperiod considering all of this , the excellence was a great setting for relaxation xxperiod the grounds were beautiful , the weather was pleasant and there were many options for activities ( and spa treatments ) if you wanted them xxperiod i would definitely go back xxperiod one note - coordinate travel to and from the hotel beforehand if you can xxperiod the hotel is over an hour away from the airport , so you 'll want to skip the can i help you , sir xxperiod bidding wars between different transportation companies xxperiod one more note - the resort is on the atlantic side of the ocean xxperiod that means rough seas and cold ( er ) water xxperiod if you intend to spend your entire vacation in the ocean , you may want to skip this place xxperiod i went in a few times , but i quickly became content just relaxing by the water when i couldn ? ? go in it xxperiod,xxbos somewhat excellence we just returned from a one week stay at excellence punta cana from xxunk to xxunk xxperiod we are seasoned travelers and have stayed at a dozen all inclusives xxperiod we stayed at excellence punta cana in 2007 for two weeks , then excellence mayan riviera and then playa mujeres and now just returned from our second time to punta cana xxperiod we love excellence xxperiod so here goes the staff at excellence i rate 5 star all the way they are an amazing group of people friendly hard working and makes your stay wonderful xxperiod pool beach and grounds a solid five star ca not say more then wow xxperiod now for the rooms we traveled as three couples all oceanview excellence club seen all three rooms and they are all in need of work the rooms are showing there age all over the place a 4 rating is generous xxperiod food a 3 rating very incosistent cold most of the time under cooked over cooked , the new ala carte menus has less selection and was not impressed at all xxperiod deffinately a great decline since our last trip two years ago xxperiod excellence club we did it in 2007 and that is why we all choose to do it again i rate it a three the only thing good was the view and the ability for late check out as we had a late flight home xxperiod but for its service i rate it a 3 xxperiod excellence club was closed upon our late arrival so they gave us our keys and sent us on our way in pitch darkness we knew where we where going so we led the other two couples we traveled with too their rooms xxperiod their service was very inconsistent beach towels in all of our three club rooms only once xxperiod went to excellence club three times for a channel changer in the end tipped staff and they got one from an unoccupied room xxperiod turn down hit and miss had to ask for bathroom amenities , would not exchange any liquor bottles in room which they did the first time we where there xxperiod the club lounge it self which used to have plenty of food to choose from literally had nothing during the day and evening cheese some fruit and bread not at all like it was excellence club has xxunk there service big time xxperiod the staff tended better to you then excellence club staff xxperiod we requested top floor for all three rooms and after all our returns we thought that was not an unreasonable request and we did not get one top floor room xxperiod save your money and shame on you excellence club for your lack of service xxperiod all three rooms had very xxunk fridge stalking and was only resolved with a tip and even then once when givin a tip was not done xxperiod i will not be returning to excellence punta cana and it is sliding down the charts for a reason xxperiod in saying all this we traveled with two other couples and we all had a fabulous time we just feel we did not get what we xxunk for xxperiod anyone with questions please do not hesitate to contact me xxperiod\n",
       "y: MultiCategoryList\n",
       ",,0;2;3;5,,2\n",
       "Path: data;\n",
       "\n",
       "Valid: LabelList (3739 items)\n",
       "x: TextList\n",
       "xxbos definitely not a 5 star resort i ' m dumbfounded that this hotel gets good reviews and is so highly rated xxperiod it 's decidedly a 3 star property , not 5 stars as indicated xxperiod the rooms are very dated and run down , old crappy beds and pillows , an old tv and overall poorly maintained xxperiod the whole property is pretty run down and old - looking xxperiod the food is subpar , not one meal i had would be called great xxperiod the service is uneven and the staff is poorly trained and uninformed xxperiod many do not comprehend english xxperiod the beach is great , it 's the only redeeming factor xxperiod however the resort is a 1- hour taxi trip from the airport xxperiod,xxbos facilities need work xxperiod we visited excellence for 5 nights in december xxperiod our first room , # xxunk , had a safe that did not work and so - so air conditioning xxperiod when we went to the front desk to complain , we were told to go to the room and someone would be there within 15 minutes xxperiod 45 minutes later , the safe guy showed up , but nobody for the a / c xxperiod the safe guy could not fix it xxperiod when he left , the electricity went out xxperiod it went out a second time before we finally went to the front desk to change rooms xxperiod we had dinner that night in the lobster house xxperiod do not waste your time on this one xxperiod the lobster tails had about 2 bites of food included xxperiod while we were in there , the electricity went out again xxperiod room xxunk served us pretty well , until night # 3 when my partner got up to go to the bathroom and stepped into an inch of water xxperiod a hose had broken on the back of the toilet and flooded our room xxperiod it would ' ve been ok , but when we went to the front desk we were told that we needed to wait until noon to see if perhaps they could move us to another room xxperiod the front desk clerks were not xxunk to just move us xxperiod my partner was infuriated that they wanted us to wait 4 hours for a new room xxperiod finally , matias at the front desk finally arranged to have us moved to another upgraded room - xxunk xxperiod we walked in and saw the leak coming from the ceiling and nearly flipped xxperiod we finally got into # xxunk , which was a gorgeous suite with a beautiful view xxperiod on the positive side , the food at the other restaurants was very good xxperiod i particularly liked the french restaurant , while my partner liked the asian restaurant xxperiod the breakfast buffet was like nothing i 'd ever seen before - lots of choices xxperiod the ocean was way too rough to enjoy , particularly if you 're not a strong swimmer xxperiod much of the beach was black flagged the entire time we were there , so if you 're a big ocean fan , i do not recommend this resort xxperiod my favorite part , by far , though , were the beds next to the pools and ocean xxperiod they were amazing xxperiod i guess you could particularly say so since the beds in the rooms were hard as rocks xxperiod all in all , a good trip - highly recommend the zip line tour xxperiod it was worth every penny xxperiod,xxbos excellence was exactly that xxperiod my family and i stayed at the excellence punta cana from december 22 to december 29 of this year xxperiod it was an amazing time had by all that attended xxperiod we arrived at the resort around 4 am because of a delay at the airport in vancouver , but even at 4 am , the service of the bellhops and the front desk was up to par xxperiod our bags were unloaded and immediately tagged and set to one side of the lobby while we were handed cold scented towels to cool off with xxperiod check in was fairly expedient and we were in our rooms within twenty minutes xxperiod i had never been to an all inclusive resort before , and wasted no time enjoying the pleasures of the mini bar in the room , as well as the ample storage space for our things xxperiod room service even at 4 am was great , the girl on the phone said it would be about 40 minutes for the food , which seemed a little long , but i think they only say that to cover there butts , because it took about 25 minutes at most xxperiod the activities during the day were well thought out , though , there was some delays and cancellations due to weather conditions ( beach volleyball cancelled to due strong winds xxperiod the entertainment staff was amazing and extremely friendly , a special thanks to all my friends , ines ( my fiance , ) altagracia ( who lovingly xxunk to me as xxunk loco , which translates into crazy skinny guy , ) xxunk and johanna ( my disco dance partners , ) sexy cesar ( who taught me all the sexy dance moves i xxperiod now know , ) julio cesar ( the mc for the games and parties xxperiod the restaurants i ca not offer too much help with , i did not eat at all of them , but of the few i did eat at , i recommend toscana for the huge buffet everyday , breakfast here is well prepared and quite delicious ( although i do not recommend the scrambled eggs xxperiod the omlettes are delicious and you have to try one xxperiod for lunch , you have to check out the grill on the beach , different food everyday , always good , and makes the beach smell amazing xxperiod for dinner , i liked spice ( asian cuisine , ) agave ( mexican , but do not eat the xxunk from here , very rubbery , ) the pizza that is delivered to the pool and the beach is awesome , make sure you try that xxperiod the bars were awesome , you get accustomed to speaking the language when ordering drinks , instead of drinking your usual bacardi and coke , try the brugal extra anejo , they call it the dominican xxunk , and it 's obvious why once you try it xxperiod the stuff tastes amazing and it does magic for someone trying to loosen up on the dance floor xxperiod the disco is great too , although sometimes a little empty , but still worth checking out xxperiod the worst part of my trip was the vendors , they do not let up , and i am a very well mannered person , which makes it hard to shut them down over and over again , make sure you do not tell them you like anything until you know you 're going to buy it , otherwise you 'll have to beat xxperiod them off with a stick to get away xxperiod try to make it to the theatre for the shows at 10 pm every night , they are worth it xxperiod the ice breaker shows are fun too , it gets people into the swing of things xxperiod all in all , i would highly recommend this resort for anyone going on a honeymoon or a romantic time with the better half , there is not a very big single crowd , so parents beware taking your single sons and daughters to this resort if they 're looking to party with other singles xxperiod hope this helps you xxperiod adios amigos and xxunk xxperiod,xxbos great service , nice hotel , mediocre food xxperiod my husband and i stayed at excellence for five nights mid - november xxperiod we booked our trip at the very last minute so we were not able to do a ton of research on the dominican but the hotel receives high ratings xxunk the web xxperiod after the one hour ride from the airport we arrived at the hotel and were greeted by everyone we met xxperiod i have to say that the staff at the hotel were very nice and made every effort to learn our names and greet us by name each time they saw us xxperiod we opted to upgrade to the excellence club and we are still trying to decide if we think it was worth it or not xxperiod as part of the excellence club , you are ushered to the club 's private lobby for check - in but , really , it almost just creates an xxunk step in the check - in process and adds another person or two you feel like you should tip xxperiod the biggest benefits of the excellence club for us were the unlimited internet access , beach towels in the room ( they were hard to get otherwise ) , and the beach bag in our room xxperiod we did eat breakfast each morning in the excellence club which was nice because it was a small buffet and you did not have to deal with a crowd xxperiod the hotel itself was clean , the staff was very friendly , and nothing ever felt crowded xxperiod however , the food was not great xxperiod it was not bad - but it was not great xxperiod i ' m not a big eater but i was prepared to indulge on my vacation and there just was not anything i was crazy about xxperiod the presentation of the food was nice but it was just bland xxperiod i think that is the best way to describe it xxperiod the pizzas that were delivered to the pool area were good but it was unpredictable because you never knew when they would arrive xxperiod we went on two excursions - swimming with the sting - rays / sharks and the zip - line tour xxperiod we loved the zip - line excursion xxperiod the staff was great and our bus driver and tour guide were great xxperiod it was interesting to visit the sting - rays and swim with the sharks but the reef where we snorkeled was disappointing xxperiod the fish were very small and there was not much to see xxperiod the electricity went out in our room a handful of times , especially when i used the hairdryer xxperiod also , our ac was terrible xxperiod they tried to repair it but it just never got cool xxperiod our room was big , though , and clean xxperiod we always got housekeeping service twice a day and they refilled our mini - bar daily xxperiod in many of the reviews , people said they got sick xxperiod our representative at the hotel ( through aaa ) warned us that many people think they get sick from the water or the food but they do not realize that having too many drinks with coconut in them will also do it xxperiod coconut is a natural laxative so you need to limit your consumption xxperiod i would still pack the immodium just to be sure xxperiod i could not decide if i wanted to give this hotel a 3 / 5 or a 4 / 5 but i decided to go up because of the friendly staff and the cleanliness of our room xxperiod i do not think i would go back because of the food but we had a nice time while we were there xxperiod we met a lot of great people at the swim up bar xxperiod,xxbos very relaxing experience just returned from my 40th birthday romantic getaway with my husband xxperiod this was our first time in the dominican republic , and we have literally been to every single island in the caribbean xxperiod so i can assure you that my review will be short , sweet , and comprehensive xxperiod in general , we liked the dr , and the excellence was very nice xxperiod the top reasons why we liked excellence were : 1 ) no kids ( ie xxperiod , if i want to get away from my own kids , i definitely do not want to vacation with other peoples ' kids ) xxperiod it was so quiet xxperiod 2 ) the staff and the people in the dr in general xxperiod so genuinely friendly , helpful , and wonderful xxperiod believe me , this is not true in most other areas of the caribbean xxperiod the all - inclusive feature xxperiod loved being served , served , served xxperiod i wanted to sit on my [ - - ] all day and just be a xxunk pig xxperiod and this is the perfect place to do it xxperiod 4 ) the best selection of beach and pool lounge chairs , beds , and hammocks i ' ve ever seen xxperiod there were palapas everywhere , so there was no shortage of shade xxperiod there were so many beds , you did not have to worry about not getting one xxperiod i 'd never had the chance to sleep on a beach bed , because usually hotels have only a few , so you end up looking xxunk at the lucky few who get them xxperiod ok , so here 's what i did not love about excellence :1 ) beach is not swimmable xxperiod way too rough most of the time xxperiod for this reason alone , i 'd not return here xxperiod i ' m a beach fan , and love to swim in the warm caribbean sea xxperiod i ' m no food snob , but some of the food was downright bad xxperiod and you end up eating in the same spot for breakfast and lunch xxperiod even though they have like 7 restaurants - the majority of them are only open for dinner xxperiod we only stayed 4 nights , and we were definitely getting very tired of the breakfast / lunch selection by the 3rd day xxperiod overall , if you just want to relax by the pool and you do not care about not going in the beach , this is a very beautiful resort xxperiod the staff is wonderful xxperiod if you are looking for a place to party and be loud and crazy , this is not your place xxperiod\n",
       "y: MultiCategoryList\n",
       "0;4;5,2,,,\n",
       "Path: data;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(23008, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(23008, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Cls02ATT400(\n",
       "    (aspect): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=6, bias=True)\n",
       "      (7): Softmax(dim=1)\n",
       "    )\n",
       "    (s0): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (s1): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (s2): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (s3): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (s4): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (s5): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=MultiLabelCEL(), metrics=[<function multi_acc at 0x7f8332229440>, <function acc_0 at 0x7f8332229290>, <function acc_1 at 0x7f8332229710>, <function acc_2 at 0x7f8332229830>, <function acc_3 at 0x7f8332229950>, <function acc_4 at 0x7f8332229a70>, <function acc_5 at 0x7f8332229b90>, <function clas_mse0 at 0x7f8332229b00>, <function clas_mse1 at 0x7f8332229dd0>, <function clas_mse2 at 0x7f8332229ef0>, <function clas_mse3 at 0x7f833221f050>, <function clas_mse4 at 0x7f833221f170>, <function clas_mse5 at 0x7f833221f290>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), functools.partial(<class 'fastai.train.GradientClipping'>, clip=1.0)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(23008, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(23008, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Cls02ATT400(\n",
       "    (aspect): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=6, bias=True)\n",
       "      (7): Softmax(dim=1)\n",
       "    )\n",
       "    (s0): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (s1): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (s2): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (s3): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (s4): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (s5): Sequential(\n",
       "      (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=400, out_features=128, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_learn.load('hotel.clas.2020avg.1.learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cls_learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>acc_0</th>\n",
       "      <th>acc_1</th>\n",
       "      <th>acc_2</th>\n",
       "      <th>acc_3</th>\n",
       "      <th>acc_4</th>\n",
       "      <th>acc_5</th>\n",
       "      <th>clas_mse0</th>\n",
       "      <th>clas_mse1</th>\n",
       "      <th>clas_mse2</th>\n",
       "      <th>clas_mse3</th>\n",
       "      <th>clas_mse4</th>\n",
       "      <th>clas_mse5</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.063848</td>\n",
       "      <td>6.357745</td>\n",
       "      <td>0.529341</td>\n",
       "      <td>0.617967</td>\n",
       "      <td>0.544465</td>\n",
       "      <td>0.511343</td>\n",
       "      <td>0.460980</td>\n",
       "      <td>0.506352</td>\n",
       "      <td>0.534936</td>\n",
       "      <td>0.543103</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>0.851180</td>\n",
       "      <td>1.254084</td>\n",
       "      <td>0.940109</td>\n",
       "      <td>1.049002</td>\n",
       "      <td>01:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.907689</td>\n",
       "      <td>6.239329</td>\n",
       "      <td>0.536298</td>\n",
       "      <td>0.628857</td>\n",
       "      <td>0.548094</td>\n",
       "      <td>0.513612</td>\n",
       "      <td>0.471416</td>\n",
       "      <td>0.512250</td>\n",
       "      <td>0.543557</td>\n",
       "      <td>0.504991</td>\n",
       "      <td>0.737750</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>1.183303</td>\n",
       "      <td>0.911071</td>\n",
       "      <td>1.024047</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.932091</td>\n",
       "      <td>6.174832</td>\n",
       "      <td>0.541364</td>\n",
       "      <td>0.635209</td>\n",
       "      <td>0.549002</td>\n",
       "      <td>0.523593</td>\n",
       "      <td>0.475953</td>\n",
       "      <td>0.516334</td>\n",
       "      <td>0.548094</td>\n",
       "      <td>0.501361</td>\n",
       "      <td>0.721869</td>\n",
       "      <td>0.866606</td>\n",
       "      <td>1.168330</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.994102</td>\n",
       "      <td>01:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.643217</td>\n",
       "      <td>6.133434</td>\n",
       "      <td>0.547868</td>\n",
       "      <td>0.650635</td>\n",
       "      <td>0.556261</td>\n",
       "      <td>0.532668</td>\n",
       "      <td>0.475045</td>\n",
       "      <td>0.524047</td>\n",
       "      <td>0.548548</td>\n",
       "      <td>0.448730</td>\n",
       "      <td>0.703721</td>\n",
       "      <td>0.821234</td>\n",
       "      <td>1.179220</td>\n",
       "      <td>0.865245</td>\n",
       "      <td>0.972777</td>\n",
       "      <td>01:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.461899</td>\n",
       "      <td>6.019319</td>\n",
       "      <td>0.556564</td>\n",
       "      <td>0.650181</td>\n",
       "      <td>0.563975</td>\n",
       "      <td>0.541742</td>\n",
       "      <td>0.489564</td>\n",
       "      <td>0.532214</td>\n",
       "      <td>0.561706</td>\n",
       "      <td>0.455082</td>\n",
       "      <td>0.659256</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>1.063975</td>\n",
       "      <td>0.871143</td>\n",
       "      <td>0.897005</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.373526</td>\n",
       "      <td>5.964935</td>\n",
       "      <td>0.561252</td>\n",
       "      <td>0.656987</td>\n",
       "      <td>0.572595</td>\n",
       "      <td>0.549456</td>\n",
       "      <td>0.484120</td>\n",
       "      <td>0.542196</td>\n",
       "      <td>0.562160</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>0.640200</td>\n",
       "      <td>0.754537</td>\n",
       "      <td>1.092559</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.899274</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.188919</td>\n",
       "      <td>5.917856</td>\n",
       "      <td>0.563067</td>\n",
       "      <td>0.666062</td>\n",
       "      <td>0.575318</td>\n",
       "      <td>0.539020</td>\n",
       "      <td>0.489564</td>\n",
       "      <td>0.540381</td>\n",
       "      <td>0.568058</td>\n",
       "      <td>0.424682</td>\n",
       "      <td>0.627042</td>\n",
       "      <td>0.769964</td>\n",
       "      <td>1.039927</td>\n",
       "      <td>0.818512</td>\n",
       "      <td>0.845281</td>\n",
       "      <td>01:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.300423</td>\n",
       "      <td>5.923725</td>\n",
       "      <td>0.565714</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.568965</td>\n",
       "      <td>0.553085</td>\n",
       "      <td>0.487296</td>\n",
       "      <td>0.540381</td>\n",
       "      <td>0.580762</td>\n",
       "      <td>0.417877</td>\n",
       "      <td>0.612069</td>\n",
       "      <td>0.713249</td>\n",
       "      <td>1.085753</td>\n",
       "      <td>0.780399</td>\n",
       "      <td>0.794011</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.116552</td>\n",
       "      <td>5.882848</td>\n",
       "      <td>0.569419</td>\n",
       "      <td>0.664247</td>\n",
       "      <td>0.572595</td>\n",
       "      <td>0.558076</td>\n",
       "      <td>0.497278</td>\n",
       "      <td>0.544011</td>\n",
       "      <td>0.580309</td>\n",
       "      <td>0.416062</td>\n",
       "      <td>0.617514</td>\n",
       "      <td>0.724592</td>\n",
       "      <td>1.090290</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.792650</td>\n",
       "      <td>01:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.991771</td>\n",
       "      <td>5.899643</td>\n",
       "      <td>0.565638</td>\n",
       "      <td>0.666969</td>\n",
       "      <td>0.573049</td>\n",
       "      <td>0.549909</td>\n",
       "      <td>0.489111</td>\n",
       "      <td>0.537659</td>\n",
       "      <td>0.577132</td>\n",
       "      <td>0.424682</td>\n",
       "      <td>0.623866</td>\n",
       "      <td>0.755445</td>\n",
       "      <td>1.119782</td>\n",
       "      <td>0.784936</td>\n",
       "      <td>0.841652</td>\n",
       "      <td>01:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.973714</td>\n",
       "      <td>5.950313</td>\n",
       "      <td>0.564580</td>\n",
       "      <td>0.664701</td>\n",
       "      <td>0.568058</td>\n",
       "      <td>0.545372</td>\n",
       "      <td>0.489111</td>\n",
       "      <td>0.543103</td>\n",
       "      <td>0.577132</td>\n",
       "      <td>0.436025</td>\n",
       "      <td>0.629764</td>\n",
       "      <td>0.737296</td>\n",
       "      <td>1.068058</td>\n",
       "      <td>0.779946</td>\n",
       "      <td>0.792196</td>\n",
       "      <td>01:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.918071</td>\n",
       "      <td>5.906313</td>\n",
       "      <td>0.568436</td>\n",
       "      <td>0.669691</td>\n",
       "      <td>0.577132</td>\n",
       "      <td>0.544918</td>\n",
       "      <td>0.493194</td>\n",
       "      <td>0.548094</td>\n",
       "      <td>0.577586</td>\n",
       "      <td>0.414701</td>\n",
       "      <td>0.597096</td>\n",
       "      <td>0.723230</td>\n",
       "      <td>1.028131</td>\n",
       "      <td>0.747278</td>\n",
       "      <td>0.793557</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.891122</td>\n",
       "      <td>5.846008</td>\n",
       "      <td>0.569873</td>\n",
       "      <td>0.673775</td>\n",
       "      <td>0.576225</td>\n",
       "      <td>0.555354</td>\n",
       "      <td>0.495917</td>\n",
       "      <td>0.541742</td>\n",
       "      <td>0.576225</td>\n",
       "      <td>0.412432</td>\n",
       "      <td>0.611162</td>\n",
       "      <td>0.724592</td>\n",
       "      <td>1.032668</td>\n",
       "      <td>0.752269</td>\n",
       "      <td>0.813521</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.825956</td>\n",
       "      <td>5.884421</td>\n",
       "      <td>0.570100</td>\n",
       "      <td>0.666969</td>\n",
       "      <td>0.574864</td>\n",
       "      <td>0.556715</td>\n",
       "      <td>0.495009</td>\n",
       "      <td>0.548094</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.412886</td>\n",
       "      <td>0.586661</td>\n",
       "      <td>0.723230</td>\n",
       "      <td>1.026770</td>\n",
       "      <td>0.761797</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.787023</td>\n",
       "      <td>5.900925</td>\n",
       "      <td>0.569722</td>\n",
       "      <td>0.675136</td>\n",
       "      <td>0.577586</td>\n",
       "      <td>0.546733</td>\n",
       "      <td>0.492287</td>\n",
       "      <td>0.548548</td>\n",
       "      <td>0.578040</td>\n",
       "      <td>0.397913</td>\n",
       "      <td>0.582123</td>\n",
       "      <td>0.726407</td>\n",
       "      <td>1.038566</td>\n",
       "      <td>0.744102</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.845849</td>\n",
       "      <td>5.879955</td>\n",
       "      <td>0.570402</td>\n",
       "      <td>0.666515</td>\n",
       "      <td>0.574410</td>\n",
       "      <td>0.558076</td>\n",
       "      <td>0.495463</td>\n",
       "      <td>0.549002</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.417423</td>\n",
       "      <td>0.588476</td>\n",
       "      <td>0.707804</td>\n",
       "      <td>1.014973</td>\n",
       "      <td>0.764973</td>\n",
       "      <td>0.782214</td>\n",
       "      <td>01:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>5.746931</td>\n",
       "      <td>5.888724</td>\n",
       "      <td>0.569419</td>\n",
       "      <td>0.670145</td>\n",
       "      <td>0.572595</td>\n",
       "      <td>0.553993</td>\n",
       "      <td>0.496824</td>\n",
       "      <td>0.546733</td>\n",
       "      <td>0.576225</td>\n",
       "      <td>0.415608</td>\n",
       "      <td>0.611162</td>\n",
       "      <td>0.736388</td>\n",
       "      <td>1.027223</td>\n",
       "      <td>0.770417</td>\n",
       "      <td>0.806715</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FULL INDIPENDENT\n",
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>multi_acc</th>\n",
       "      <th>acc_0</th>\n",
       "      <th>acc_1</th>\n",
       "      <th>acc_2</th>\n",
       "      <th>acc_3</th>\n",
       "      <th>acc_4</th>\n",
       "      <th>acc_5</th>\n",
       "      <th>clas_mse0</th>\n",
       "      <th>clas_mse1</th>\n",
       "      <th>clas_mse2</th>\n",
       "      <th>clas_mse3</th>\n",
       "      <th>clas_mse4</th>\n",
       "      <th>clas_mse5</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.884711</td>\n",
       "      <td>6.356955</td>\n",
       "      <td>0.529197</td>\n",
       "      <td>0.613533</td>\n",
       "      <td>0.556566</td>\n",
       "      <td>0.492110</td>\n",
       "      <td>0.468842</td>\n",
       "      <td>0.514041</td>\n",
       "      <td>0.530088</td>\n",
       "      <td>0.524739</td>\n",
       "      <td>0.736828</td>\n",
       "      <td>0.900241</td>\n",
       "      <td>1.254079</td>\n",
       "      <td>0.916288</td>\n",
       "      <td>1.098155</td>\n",
       "      <td>03:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.747019</td>\n",
       "      <td>6.276711</td>\n",
       "      <td>0.537399</td>\n",
       "      <td>0.625836</td>\n",
       "      <td>0.570741</td>\n",
       "      <td>0.506285</td>\n",
       "      <td>0.472319</td>\n",
       "      <td>0.514576</td>\n",
       "      <td>0.534635</td>\n",
       "      <td>0.496924</td>\n",
       "      <td>0.684675</td>\n",
       "      <td>0.846483</td>\n",
       "      <td>1.188820</td>\n",
       "      <td>0.869484</td>\n",
       "      <td>1.046002</td>\n",
       "      <td>03:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.555628</td>\n",
       "      <td>6.235485</td>\n",
       "      <td>0.543104</td>\n",
       "      <td>0.644290</td>\n",
       "      <td>0.569938</td>\n",
       "      <td>0.508692</td>\n",
       "      <td>0.475796</td>\n",
       "      <td>0.519123</td>\n",
       "      <td>0.540786</td>\n",
       "      <td>0.492913</td>\n",
       "      <td>0.712490</td>\n",
       "      <td>0.891682</td>\n",
       "      <td>1.186146</td>\n",
       "      <td>0.899706</td>\n",
       "      <td>1.056700</td>\n",
       "      <td>03:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.544207</td>\n",
       "      <td>6.133196</td>\n",
       "      <td>0.550058</td>\n",
       "      <td>0.652046</td>\n",
       "      <td>0.576090</td>\n",
       "      <td>0.521262</td>\n",
       "      <td>0.477133</td>\n",
       "      <td>0.524739</td>\n",
       "      <td>0.549077</td>\n",
       "      <td>0.450655</td>\n",
       "      <td>0.675582</td>\n",
       "      <td>0.806098</td>\n",
       "      <td>1.116341</td>\n",
       "      <td>0.818401</td>\n",
       "      <td>0.956673</td>\n",
       "      <td>03:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.310172</td>\n",
       "      <td>6.068969</td>\n",
       "      <td>0.552465</td>\n",
       "      <td>0.659267</td>\n",
       "      <td>0.587590</td>\n",
       "      <td>0.526076</td>\n",
       "      <td>0.478203</td>\n",
       "      <td>0.525274</td>\n",
       "      <td>0.538379</td>\n",
       "      <td>0.449853</td>\n",
       "      <td>0.626906</td>\n",
       "      <td>0.768922</td>\n",
       "      <td>1.201391</td>\n",
       "      <td>0.802888</td>\n",
       "      <td>1.053758</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.332395</td>\n",
       "      <td>6.043277</td>\n",
       "      <td>0.557547</td>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.582509</td>\n",
       "      <td>0.538914</td>\n",
       "      <td>0.467505</td>\n",
       "      <td>0.533030</td>\n",
       "      <td>0.558973</td>\n",
       "      <td>0.429794</td>\n",
       "      <td>0.650441</td>\n",
       "      <td>0.737363</td>\n",
       "      <td>1.258358</td>\n",
       "      <td>0.793260</td>\n",
       "      <td>0.950522</td>\n",
       "      <td>03:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.268944</td>\n",
       "      <td>6.077346</td>\n",
       "      <td>0.557457</td>\n",
       "      <td>0.650441</td>\n",
       "      <td>0.579032</td>\n",
       "      <td>0.544263</td>\n",
       "      <td>0.482215</td>\n",
       "      <td>0.534100</td>\n",
       "      <td>0.554694</td>\n",
       "      <td>0.465365</td>\n",
       "      <td>0.618615</td>\n",
       "      <td>0.694571</td>\n",
       "      <td>1.117411</td>\n",
       "      <td>0.769190</td>\n",
       "      <td>0.875635</td>\n",
       "      <td>03:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.075140</td>\n",
       "      <td>5.978911</td>\n",
       "      <td>0.561469</td>\n",
       "      <td>0.664616</td>\n",
       "      <td>0.594009</td>\n",
       "      <td>0.536240</td>\n",
       "      <td>0.483017</td>\n",
       "      <td>0.533833</td>\n",
       "      <td>0.557101</td>\n",
       "      <td>0.431666</td>\n",
       "      <td>0.607649</td>\n",
       "      <td>0.771329</td>\n",
       "      <td>1.151377</td>\n",
       "      <td>0.775074</td>\n",
       "      <td>0.889543</td>\n",
       "      <td>03:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.119335</td>\n",
       "      <td>5.946039</td>\n",
       "      <td>0.564233</td>\n",
       "      <td>0.673977</td>\n",
       "      <td>0.586253</td>\n",
       "      <td>0.537309</td>\n",
       "      <td>0.491843</td>\n",
       "      <td>0.534902</td>\n",
       "      <td>0.561113</td>\n",
       "      <td>0.426585</td>\n",
       "      <td>0.608184</td>\n",
       "      <td>0.746724</td>\n",
       "      <td>1.081573</td>\n",
       "      <td>0.783365</td>\n",
       "      <td>0.903183</td>\n",
       "      <td>03:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.967346</td>\n",
       "      <td>5.967121</td>\n",
       "      <td>0.567576</td>\n",
       "      <td>0.676919</td>\n",
       "      <td>0.596684</td>\n",
       "      <td>0.542926</td>\n",
       "      <td>0.484087</td>\n",
       "      <td>0.544531</td>\n",
       "      <td>0.560310</td>\n",
       "      <td>0.412142</td>\n",
       "      <td>0.599626</td>\n",
       "      <td>0.727467</td>\n",
       "      <td>1.137203</td>\n",
       "      <td>0.756352</td>\n",
       "      <td>0.896764</td>\n",
       "      <td>03:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.850564</td>\n",
       "      <td>5.941073</td>\n",
       "      <td>0.568869</td>\n",
       "      <td>0.674244</td>\n",
       "      <td>0.595614</td>\n",
       "      <td>0.546135</td>\n",
       "      <td>0.491843</td>\n",
       "      <td>0.539716</td>\n",
       "      <td>0.565659</td>\n",
       "      <td>0.420968</td>\n",
       "      <td>0.589997</td>\n",
       "      <td>0.723990</td>\n",
       "      <td>1.094945</td>\n",
       "      <td>0.781492</td>\n",
       "      <td>0.908799</td>\n",
       "      <td>03:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.934150</td>\n",
       "      <td>5.875716</td>\n",
       "      <td>0.570206</td>\n",
       "      <td>0.672105</td>\n",
       "      <td>0.591335</td>\n",
       "      <td>0.552822</td>\n",
       "      <td>0.493180</td>\n",
       "      <td>0.546403</td>\n",
       "      <td>0.565392</td>\n",
       "      <td>0.406258</td>\n",
       "      <td>0.581439</td>\n",
       "      <td>0.705536</td>\n",
       "      <td>1.026478</td>\n",
       "      <td>0.741642</td>\n",
       "      <td>0.843809</td>\n",
       "      <td>03:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.759236</td>\n",
       "      <td>5.914033</td>\n",
       "      <td>0.569983</td>\n",
       "      <td>0.674244</td>\n",
       "      <td>0.594277</td>\n",
       "      <td>0.549880</td>\n",
       "      <td>0.493447</td>\n",
       "      <td>0.547473</td>\n",
       "      <td>0.560578</td>\n",
       "      <td>0.404654</td>\n",
       "      <td>0.587858</td>\n",
       "      <td>0.719176</td>\n",
       "      <td>1.040653</td>\n",
       "      <td>0.746724</td>\n",
       "      <td>0.890078</td>\n",
       "      <td>03:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.696476</td>\n",
       "      <td>5.866328</td>\n",
       "      <td>0.573059</td>\n",
       "      <td>0.678256</td>\n",
       "      <td>0.597753</td>\n",
       "      <td>0.558171</td>\n",
       "      <td>0.493447</td>\n",
       "      <td>0.543193</td>\n",
       "      <td>0.567531</td>\n",
       "      <td>0.399305</td>\n",
       "      <td>0.580904</td>\n",
       "      <td>0.677454</td>\n",
       "      <td>1.008558</td>\n",
       "      <td>0.740305</td>\n",
       "      <td>0.866809</td>\n",
       "      <td>03:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.638726</td>\n",
       "      <td>5.947060</td>\n",
       "      <td>0.571677</td>\n",
       "      <td>0.673442</td>\n",
       "      <td>0.595346</td>\n",
       "      <td>0.551752</td>\n",
       "      <td>0.493982</td>\n",
       "      <td>0.545868</td>\n",
       "      <td>0.569671</td>\n",
       "      <td>0.408131</td>\n",
       "      <td>0.591067</td>\n",
       "      <td>0.711955</td>\n",
       "      <td>1.073014</td>\n",
       "      <td>0.749131</td>\n",
       "      <td>0.860925</td>\n",
       "      <td>03:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.714550</td>\n",
       "      <td>5.891889</td>\n",
       "      <td>0.572078</td>\n",
       "      <td>0.677186</td>\n",
       "      <td>0.595079</td>\n",
       "      <td>0.552019</td>\n",
       "      <td>0.492110</td>\n",
       "      <td>0.544263</td>\n",
       "      <td>0.571811</td>\n",
       "      <td>0.411875</td>\n",
       "      <td>0.594009</td>\n",
       "      <td>0.700990</td>\n",
       "      <td>1.048676</td>\n",
       "      <td>0.753677</td>\n",
       "      <td>0.879914</td>\n",
       "      <td>03:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>5.696353</td>\n",
       "      <td>5.924479</td>\n",
       "      <td>0.573772</td>\n",
       "      <td>0.680931</td>\n",
       "      <td>0.595881</td>\n",
       "      <td>0.553089</td>\n",
       "      <td>0.497192</td>\n",
       "      <td>0.543461</td>\n",
       "      <td>0.572078</td>\n",
       "      <td>0.395293</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.702327</td>\n",
       "      <td>1.038780</td>\n",
       "      <td>0.745386</td>\n",
       "      <td>0.852902</td>\n",
       "      <td>03:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
      "/home/aeryen/anaconda3/envs/laam/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  parameters = list(filter(lambda p: p.grad is not None, parameters))\n"
     ]
    }
   ],
   "source": [
    "# 2020 AVG\n",
    "with experiment.train():\n",
    "    cls_learn.fit_one_cycle(17, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web': 'https://www.comet.ml/api/image/download?imageId=68ff75f1ddd344308ebed17658da5325&experimentKey=6b1da453714c49a19287bee3f478030c',\n",
       " 'api': 'https://www.comet.ml/api/rest/v1/image/get-image?imageId=68ff75f1ddd344308ebed17658da5325&experimentKey=6b1da453714c49a19287bee3f478030c',\n",
       " 'imageId': '68ff75f1ddd344308ebed17658da5325'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_learn.recorder.plot_losses()\n",
    "experiment.log_figure(figure_name=\"train loss 02\", figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web': 'https://www.comet.ml/api/image/download?imageId=c9d5dc3055134bb085b29b26b024fa25&experimentKey=985dcb33868d415f9bcf006757f1ae92',\n",
       " 'api': 'https://www.comet.ml/api/rest/v1/image/get-image?imageId=c9d5dc3055134bb085b29b26b024fa25&experimentKey=985dcb33868d415f9bcf006757f1ae92',\n",
       " 'imageId': 'c9d5dc3055134bb085b29b26b024fa25'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2020 AVG\n",
    "fig = cls_learn.recorder.plot_losses()\n",
    "experiment.log_figure(figure_name=\"train loss 02\", figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learn.save('clasATT02.learner.attind800.200109.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINISH EXPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/aeryen/2019nn/6b1da453714c49a19287bee3f478030c\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     sys.cpu.percent.01 [196]            : (0.8, 7.2)\n",
      "COMET INFO:     sys.cpu.percent.02 [196]            : (0.3, 11.0)\n",
      "COMET INFO:     sys.cpu.percent.03 [196]            : (0.2, 27.8)\n",
      "COMET INFO:     sys.cpu.percent.04 [196]            : (0.1, 9.1)\n",
      "COMET INFO:     sys.cpu.percent.05 [196]            : (0.2, 7.0)\n",
      "COMET INFO:     sys.cpu.percent.06 [196]            : (0.2, 30.5)\n",
      "COMET INFO:     sys.cpu.percent.07 [196]            : (0.2, 11.3)\n",
      "COMET INFO:     sys.cpu.percent.08 [196]            : (0.2, 21.4)\n",
      "COMET INFO:     sys.cpu.percent.09 [196]            : (0.2, 7.9)\n",
      "COMET INFO:     sys.cpu.percent.10 [196]            : (0.1, 16.8)\n",
      "COMET INFO:     sys.cpu.percent.11 [196]            : (0.5, 3.4)\n",
      "COMET INFO:     sys.cpu.percent.12 [196]            : (0.2, 1.9)\n",
      "COMET INFO:     sys.cpu.percent.avg [196]           : (0.43333333333333335, 8.883333333333335)\n",
      "COMET INFO:     sys.gpu.0.free_memory [223]         : (8409055232.0, 9561702400.0)\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization [223]     : (0.0, 84.0)\n",
      "COMET INFO:     sys.gpu.0.total_memory              : (25373310976.0, 25373310976.0)\n",
      "COMET INFO:     sys.gpu.0.used_memory [223]         : (15811608576.0, 16964255744.0)\n",
      "COMET INFO:     sys.load.avg [196]                  : (0.0, 0.98)\n",
      "COMET INFO:     sys.ram.total [196]                 : (16703758336.0, 16703758336.0)\n",
      "COMET INFO:     sys.ram.used [196]                  : (6780821504.0, 7315865600.0)\n",
      "COMET INFO:     train_acc_0 [54]                    : (0.5163339376449585, 0.6774047017097473)\n",
      "COMET INFO:     train_acc_1 [54]                    : (0.4736842215061188, 0.5775862336158752)\n",
      "COMET INFO:     train_acc_2 [54]                    : (0.4010889232158661, 0.5676043629646301)\n",
      "COMET INFO:     train_acc_3 [54]                    : (0.4337567985057831, 0.4972776770591736)\n",
      "COMET INFO:     train_acc_4 [54]                    : (0.4478221535682678, 0.5562613606452942)\n",
      "COMET INFO:     train_acc_5 [54]                    : (0.4795825779438019, 0.5852994322776794)\n",
      "COMET INFO:     train_clas_mse0 [54]                : (0.3979128897190094, 0.7627041935920715)\n",
      "COMET INFO:     train_clas_mse1 [54]                : (0.5821233987808228, 0.9990925788879395)\n",
      "COMET INFO:     train_clas_mse2 [54]                : (0.6887477040290833, 1.2663339376449585)\n",
      "COMET INFO:     train_clas_mse3 [54]                : (0.9863883852958679, 1.607985496520996)\n",
      "COMET INFO:     train_clas_mse4 [54]                : (0.7223230600357056, 1.2014518976211548)\n",
      "COMET INFO:     train_clas_mse5 [54]                : (0.7622504830360413, 1.289927363395691)\n",
      "COMET INFO:     train_curr_epoch [54]               : 16\n",
      "COMET INFO:     train_loss [1512]                   : (4.448082447052002, 10.6703462600708)\n",
      "COMET INFO:     train_multi_acc [54]                : (0.4590894877910614, 0.5725952982902527)\n",
      "COMET INFO:     train_sys.cpu.percent.01 [62]       : (2.2, 27.4)\n",
      "COMET INFO:     train_sys.cpu.percent.02 [62]       : (0.8, 24.1)\n",
      "COMET INFO:     train_sys.cpu.percent.03 [62]       : (0.8, 57.6)\n",
      "COMET INFO:     train_sys.cpu.percent.04 [62]       : (0.6, 50.7)\n",
      "COMET INFO:     train_sys.cpu.percent.05 [62]       : (0.6, 61.0)\n",
      "COMET INFO:     train_sys.cpu.percent.06 [62]       : (0.6, 48.4)\n",
      "COMET INFO:     train_sys.cpu.percent.07 [62]       : (0.5, 50.1)\n",
      "COMET INFO:     train_sys.cpu.percent.08 [62]       : (0.6, 57.9)\n",
      "COMET INFO:     train_sys.cpu.percent.09 [62]       : (0.5, 46.5)\n",
      "COMET INFO:     train_sys.cpu.percent.10 [62]       : (0.6, 52.2)\n",
      "COMET INFO:     train_sys.cpu.percent.11 [62]       : (0.8, 53.4)\n",
      "COMET INFO:     train_sys.cpu.percent.12 [62]       : (0.4, 54.5)\n",
      "COMET INFO:     train_sys.cpu.percent.avg [62]      : (1.9833333333333332, 10.533333333333335)\n",
      "COMET INFO:     train_sys.gpu.0.free_memory [70]    : (860684288.0, 17304387584.0)\n",
      "COMET INFO:     train_sys.gpu.0.gpu_utilization [70]: (64.0, 94.0)\n",
      "COMET INFO:     train_sys.gpu.0.used_memory [70]    : (8068923392.0, 24512626688.0)\n",
      "COMET INFO:     train_sys.load.avg [62]             : (0.26, 1.4)\n",
      "COMET INFO:     train_sys.ram.total [62]            : (16703758336.0, 16703758336.0)\n",
      "COMET INFO:     train_sys.ram.used [62]             : (7001280512.0, 9449467904.0)\n",
      "COMET INFO:     train_val_loss [54]                 : (5.845250129699707, 7.157726287841797)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     figures  : 3\n",
      "COMET INFO:     git-patch: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_num_file = [\"aspect_0.count\", \"test_aspect_0.count\"]\n",
    "rating_file = [\"aspect_0.rating\", \"test_aspect_0.rating\"]\n",
    "content_file = [\"aspect_0.txt\", \"test_aspect_0.txt\"]\n",
    "\n",
    "dataset_dir = \"./data/hotel_balance_LengthFix1_3000per/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_to_doc(sent_list, sent_count):\n",
    "    start_index = 0\n",
    "    docs = []\n",
    "    for s in sent_count:\n",
    "#         doc = \" xxPERIOD \".join(sent_list[start_index:start_index + s])\n",
    "#         doc = doc + \" xxPERIOD \"\n",
    "        docs.append(sent_list[start_index:start_index + s])\n",
    "        start_index = start_index + s\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = 0\n",
    "TEST_DATA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 30, 25, 33, 29]\n",
      "   0  1  2  3  4  5\n",
      "0  1  0  0  3  1  1\n",
      "1  2  2  1  2  3  3\n",
      "2  4  4  4  3  4  4\n",
      "3  3  2  3  3  3  4\n",
      "4  3  4  3  4  4  4\n"
     ]
    }
   ],
   "source": [
    "# Load Count\n",
    "sent_count_test = list(open(dataset_dir + sent_num_file[TEST_DATA], \"r\").readlines())\n",
    "sent_count_test = [int(s) for s in sent_count_test if (len(s) > 0 and s != \"\\n\")]\n",
    "print( sent_count_test[0:5] )\n",
    "\n",
    "# Load Ratings\n",
    "aspect_rating_test = list(open(dataset_dir + rating_file[TEST_DATA], \"r\").readlines())\n",
    "aspect_rating_test = [s for s in aspect_rating_test if (len(s) > 0 and s != \"\\n\")]\n",
    "\n",
    "aspect_rating_test = [s.split(\" \") for s in aspect_rating_test]\n",
    "aspect_rating_test = np.array(aspect_rating_test)[:, 0:-1]\n",
    "aspect_rating_test = aspect_rating_test.astype(np.int) - 1\n",
    "aspect_rating_test = pd.DataFrame(aspect_rating_test)\n",
    "print( aspect_rating_test.head() )\n",
    "\n",
    "# Load Sents\n",
    "sents_test = list(open(dataset_dir + content_file[TEST_DATA], \"r\").readlines())\n",
    "sents_test = [s.strip() for s in sents_test]\n",
    "\n",
    "# Sents to Doc\n",
    "docs_test = concat_to_doc(sents_test, sent_count_test)\n",
    "\n",
    "docs_test = pd.DataFrame({doc:docs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[definitely not a 5 star resort i 'm dumbfound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[facilities need work, we visited excellence f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[excellence was exactly that, my family and i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[great service , nice hotel , mediocre food, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[very relaxing experience just returned from m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5                                                  6\n",
       "0  1  0  0  3  1  1  [definitely not a 5 star resort i 'm dumbfound...\n",
       "1  2  2  1  2  3  3  [facilities need work, we visited excellence f...\n",
       "2  4  4  4  3  4  4  [excellence was exactly that, my family and i ...\n",
       "3  3  2  3  3  3  4  [great service , nice hotel , mediocre food, m...\n",
       "4  3  4  3  4  4  4  [very relaxing experience just returned from m..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.concat( [aspect_rating_test, docs_test], axis=1, ignore_index=True )\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clas_acc(asp_index):\n",
    "    def asp_acc(preds, targs):\n",
    "        preds = torch.max(preds, dim=2)[1]\n",
    "        targs = targs.contiguous().long()\n",
    "        return (preds[:,asp_index]==targs[:,asp_index]).float().mean()\n",
    "    return asp_acc\n",
    "def get_clas_mse(asp_index):\n",
    "    def asp_mse(preds, targs):\n",
    "        preds = torch.max(preds, dim=2)[1].float()[:,asp_index]\n",
    "        targs = targs.contiguous().float()[:,asp_index]\n",
    "        return torch.nn.functional.mse_loss(preds, targs)\n",
    "    return asp_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(self,\n",
    "              ds_type:DatasetType,\n",
    "              activ:nn.Module=None,\n",
    "              with_loss:bool=False,\n",
    "              n_batch:Optional[int]=None,\n",
    "              pbar:Optional[PBar]=None,\n",
    "              ordered:bool=False) -> List[Tensor]:\n",
    "    \"Return predictions and targets on the valid, train, or test set, depending on `ds_type`.\"\n",
    "    self.model.reset()\n",
    "    if ordered: np.random.seed(42)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outs = []\n",
    "        asps = []\n",
    "        for xb,yb in progress_bar(cls_learn.dl(ds_type)):\n",
    "            out,raw_enc,enc,asp = cls_learn.model(xb)\n",
    "            outs.append(out)\n",
    "            for doc in asp:\n",
    "                asps.append( to_float(doc.cpu()))\n",
    "\n",
    "    outs = to_float(torch.cat(outs).cpu())\n",
    "    \n",
    "    if ordered and hasattr(self.dl(ds_type), 'sampler'):\n",
    "        np.random.seed(42)\n",
    "        sampler = [i for i in self.dl(ds_type).sampler]\n",
    "        reverse_sampler = np.argsort(sampler)\n",
    "        \n",
    "        outs = outs[reverse_sampler]\n",
    "        asps = [asps[i] for i in reverse_sampler]\n",
    "    return (outs,asps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='117' class='' max='117', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [117/117 00:10<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outs,asps = get_preds(self=cls_learn, ds_type=DatasetType.Test, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3739, 7, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 3, 1, 1],\n",
       "        [2, 2, 1, 2, 3, 3],\n",
       "        [4, 4, 4, 3, 4, 4],\n",
       "        ...,\n",
       "        [0, 0, 0, 3, 0, 2],\n",
       "        [0, 0, 0, 2, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor( aspect_rating_test.values )\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9473)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mloss = MultiLabelCEL()\n",
    "mloss.forward(outs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASP0</th>\n",
       "      <th>ASP1</th>\n",
       "      <th>ASP2</th>\n",
       "      <th>ASP3</th>\n",
       "      <th>ASP4</th>\n",
       "      <th>ASP5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.675314</td>\n",
       "      <td>0.60444</td>\n",
       "      <td>0.54774</td>\n",
       "      <td>0.482749</td>\n",
       "      <td>0.548275</td>\n",
       "      <td>0.566997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ASP0     ASP1     ASP2      ASP3      ASP4      ASP5\n",
       "0  0.675314  0.60444  0.54774  0.482749  0.548275  0.566997"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict( {\"ASP\"+str(ai):[get_clas_acc(ai)(outs, target).item()] for ai in range(6)} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASP0</th>\n",
       "      <th>ASP1</th>\n",
       "      <th>ASP2</th>\n",
       "      <th>ASP3</th>\n",
       "      <th>ASP4</th>\n",
       "      <th>ASP5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.415084</td>\n",
       "      <td>0.605777</td>\n",
       "      <td>0.731479</td>\n",
       "      <td>1.059107</td>\n",
       "      <td>0.768922</td>\n",
       "      <td>0.863065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ASP0      ASP1      ASP2      ASP3      ASP4      ASP5\n",
       "0  0.415084  0.605777  0.731479  1.059107  0.768922  0.863065"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict( {\"ASP\"+str(ai):[get_clas_mse(ai)(outs, target).item()] for ai in range(6)} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth:\n",
      "[1, 0, 0, 3, 1, 1]\n",
      "prediction:\n",
      "tensor([1, 0, 0, 2, 0, 0])\n",
      "doc:\n",
      "definitely not a 5 star resort i 'm dumbfounded that this hotel gets good reviews and is so highly rated\n",
      "          +++ overall +++ value +++ [0.179 0.118 0.069 0.066 0.034 0.069]\n",
      "it 's decidedly a 3 star property , not 5 stars as indicated\n",
      "          +++ overall +++ value +++ [0.202 0.148 0.103 0.077 0.054 0.087]\n",
      "the rooms are very dated and run down , old crappy beds and pillows , an old tv and overall poorly maintained\n",
      "          +++ room +++ room +++ [0.089 0.07  0.502 0.028 0.193 0.034]\n",
      "the whole property is pretty run down and old - looking\n",
      "          +++ room +++ room +++ [0.123 0.088 0.34  0.05  0.146 0.044]\n",
      "the food is subpar , not one meal i had would be called great\n",
      "          +++ overall +++ value +++ [0.139 0.098 0.064 0.042 0.046 0.074]\n",
      "the service is uneven and the staff is poorly trained and uninformed\n",
      "          +++ service +++ service +++ [0.083 0.059 0.021 0.022 0.041 0.66 ]\n",
      "many do not comprehend english\n",
      "          +++ service +++ service +++ [0.089 0.058 0.021 0.017 0.03  0.504]\n",
      "the beach is great , it 's the only redeeming factor\n",
      "          +++ location +++ location +++ [0.038 0.027 0.01  0.418 0.008 0.014]\n",
      "however the resort is a 1- hour taxi trip from the airport\n",
      "          +++ location +++ location +++ [0.052 0.034 0.015 0.486 0.004 0.022]\n",
      "===========\n",
      "truth:\n",
      "[2, 2, 1, 2, 3, 3]\n",
      "prediction:\n",
      "tensor([1, 1, 0, 3, 3, 1])\n",
      "doc:\n",
      "facilities need work\n",
      "          +++ overall +++ value +++ [0.167 0.114 0.097 0.053 0.06  0.103]\n",
      "we visited excellence for 5 nights in december\n",
      "          +++ overall +++ value +++ [0.21  0.121 0.066 0.068 0.045 0.098]\n",
      "our first room , #1112, had a safe that did not work and so - so air conditioning\n",
      "          +++ room +++ room +++ [0.098 0.064 0.222 0.028 0.035 0.035]\n",
      "when we went to the front desk to complain , we were told to go to the room and someone would be there within 15 minutes\n",
      "          +++ service +++ service +++ [0.046 0.027 0.034 0.014 0.011 0.078]\n",
      "45 minutes later , the safe guy showed up , but nobody for the a/c\n",
      "          +++ service +++ service +++ [0.069 0.043 0.053 0.016 0.014 0.075]\n",
      "the safe guy could not fix it\n",
      "          +++ service +++ service +++ [0.06  0.035 0.043 0.017 0.01  0.065]\n",
      "when he left , the electricity went out\n",
      "          +++ room +++ room +++ [0.052 0.032 0.057 0.019 0.01  0.037]\n",
      "it went out a second time before we finally went to the front desk to change rooms\n",
      "          +++ service +++ service +++ [0.057 0.037 0.048 0.015 0.014 0.066]\n",
      "we had dinner that night in the lobster house\n",
      "          +++ overall +++ service +++ [0.043 0.027 0.024 0.024 0.012 0.032]\n",
      "do not waste your time on this one\n",
      "          +++ overall +++ value +++ [0.112 0.075 0.058 0.037 0.021 0.062]\n",
      "the lobster tails had about 2 bites of food included\n",
      "          +++ overall +++ value +++ [0.079 0.053 0.043 0.026 0.029 0.052]\n",
      "while we were in there , the electricity went out again\n",
      "          +++ overall +++ room +++ [0.057 0.037 0.04  0.018 0.01  0.029]\n",
      "room 3002 served us pretty well , until night #3 when my partner got up to go to the bathroom and stepped into an inch of water\n",
      "          +++ room +++ room +++ [0.078 0.055 0.103 0.028 0.029 0.057]\n",
      "a hose had broken on the back of the toilet and flooded our room\n",
      "          +++ room +++ room +++ [0.11  0.077 0.24  0.035 0.073 0.059]\n",
      "it would 've been ok , but when we went to the front desk we were told that we needed to wait until noon to see if perhaps they could move us to another room\n",
      "          +++ service +++ service +++ [0.059 0.042 0.057 0.013 0.018 0.075]\n",
      "the front desk clerks were not empowered to just move us\n",
      "          +++ service +++ service +++ [0.059 0.039 0.04  0.012 0.014 0.107]\n",
      "my partner was infuriated that they wanted us to wait 4 hours for a new room\n",
      "          +++ service +++ service +++ [0.055 0.038 0.044 0.012 0.014 0.087]\n",
      "finally , matias at the front desk finally arranged to have us moved to another upgraded room - 3109\n",
      "          +++ overall +++ service +++ [0.045 0.026 0.032 0.016 0.006 0.045]\n",
      "we walked in and saw the leak coming from the ceiling and nearly flipped\n",
      "          +++ room +++ room +++ [0.077 0.048 0.099 0.03  0.016 0.033]\n",
      "we finally got into #3110, which was a gorgeous suite with a beautiful view\n",
      "          +++ room +++ room +++ [0.065 0.045 0.232 0.049 0.065 0.05 ]\n",
      "on the positive side , the food at the other restaurants was very good\n",
      "          +++ overall +++ value +++ [0.181 0.121 0.037 0.067 0.045 0.08 ]\n",
      "i particularly liked the french restaurant , while my partner liked the asian restaurant\n",
      "          +++ overall +++ service +++ [0.097 0.068 0.035 0.055 0.036 0.072]\n",
      "the breakfast buffet was like nothing i 'd ever seen before - lots of choices\n",
      "          +++ overall +++ value +++ [0.174 0.115 0.051 0.066 0.052 0.076]\n",
      "the ocean was way too rough to enjoy , particularly if you 're not a strong swimmer\n",
      "          +++ location +++ location +++ [0.053 0.032 0.012 0.216 0.013 0.015]\n",
      "much of the beach was black flagged the entire time we were there , so if you 're a big ocean fan , i do not recommend this resort\n",
      "          +++ location +++ location +++ [0.058 0.037 0.011 0.387 0.011 0.014]\n",
      "my favorite part , by far , though , were the beds next to the pools and ocean\n",
      "          +++ room +++ room +++ [0.074 0.058 0.116 0.072 0.06  0.051]\n",
      "they were amazing\n",
      "          +++ room +++ room +++ [0.049 0.041 0.159 0.048 0.073 0.039]\n",
      "i guess you could particularly say so since the beds in the rooms were hard as rocks\n",
      "          +++ room +++ room +++ [0.086 0.062 0.271 0.045 0.09  0.04 ]\n",
      "all in all , a good trip - highly recommend the zip line tour\n",
      "          +++ location +++ location +++ [0.169 0.133 0.028 0.175 0.027 0.074]\n",
      "it was worth every penny\n",
      "          +++ overall +++ value +++ [0.241 0.199 0.045 0.132 0.039 0.114]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 3, 4, 4]\n",
      "prediction:\n",
      "tensor([4, 4, 4, 4, 4, 4])\n",
      "doc:\n",
      "excellence was exactly that\n",
      "          +++ overall +++ service +++ [0.248 0.166 0.084 0.098 0.068 0.181]\n",
      "my family and i stayed at the excellence punta cana from december 22 to december 29 of this year\n",
      "          +++ overall +++ value +++ [0.244 0.136 0.074 0.088 0.048 0.135]\n",
      "it was an amazing time had by all that attended\n",
      "          +++ overall +++ service +++ [0.215 0.147 0.083 0.089 0.067 0.191]\n",
      "we arrived at the resort around 4 am because of a delay at the airport in vancouver , but even at 4 am , the service of the bellhops and the front desk was up to par\n",
      "          +++ service +++ service +++ [0.082 0.051 0.036 0.052 0.028 0.38 ]\n",
      "our bags were unloaded and immediately tagged and set to one side of the lobby while we were handed cold scented towels to cool off with\n",
      "          +++ service +++ service +++ [0.06  0.038 0.048 0.055 0.031 0.12 ]\n",
      "check in was fairly expedient and we were in our rooms within twenty minutes\n",
      "          +++ service +++ service +++ [0.088 0.059 0.08  0.085 0.036 0.115]\n",
      "i had never been to an all inclusive resort before , and wasted no time enjoying the pleasures of the mini bar in the room , as well as the ample storage space for our things\n",
      "          +++ room +++ room +++ [0.114 0.086 0.198 0.076 0.083 0.121]\n",
      "room service even at 4 am was great , the girl on the phone said it would be about 40 minutes for the food , which seemed a little long , but i think they only say that to cover there butts , because it took about 25 minutes at most\n",
      "          +++ service +++ service +++ [0.083 0.06  0.047 0.036 0.049 0.4  ]\n",
      "the activities during the day were well thought out , though , there was some delays and cancellations due to weather conditions ( beach volleyball cancelled to due strong winds\n",
      "          +++ overall +++ location +++ [0.084 0.054 0.025 0.063 0.026 0.059]\n",
      "the entertainment staff was amazing and extremely friendly , a special thanks to all my friends , ines ( my fiance , ) altagracia ( who lovingly reffered to me as flaco loco , which translates into crazy skinny guy , ) eliza and johanna ( my disco dance partners , ) sexy cesar ( who taught me all the sexy dance moves i\n",
      "          +++ service +++ service +++ [0.07  0.043 0.026 0.036 0.029 0.116]\n",
      "now know , ) julio cesar ( the mc for the games and parties\n",
      "          +++ service +++ service +++ [0.054 0.036 0.021 0.037 0.029 0.112]\n",
      "the restaurants i ca not offer too much help with , i did not eat at all of them , but of the few i did eat at , i recommend toscana for the huge buffet everyday , breakfast here is well prepared and quite delicious ( although i do not recommend the scrambled eggs\n",
      "          +++ overall +++ service +++ [0.13  0.08  0.03  0.044 0.039 0.08 ]\n",
      "the omlettes are delicious and you have to try one\n",
      "          +++ overall +++ service +++ [0.107 0.071 0.035 0.049 0.042 0.087]\n",
      "for lunch , you have to check out the grill on the beach , different food everyday , always good , and makes the beach smell amazing\n",
      "          +++ overall +++ service +++ [0.12  0.084 0.037 0.07  0.058 0.101]\n",
      "for dinner , i liked spice ( asian cuisine , ) agave ( mexican , but do not eat the calimari from here , very rubbery , ) the pizza that is delivered to the pool and the beach is awesome , make sure you try that\n",
      "          +++ overall +++ service +++ [0.078 0.054 0.027 0.045 0.03  0.074]\n",
      "the bars were awesome , you get accustomed to speaking the language when ordering drinks , instead of drinking your usual bacardi and coke , try the brugal extra anejo , they call it the dominican babymaker , and it 's obvious why once you try it\n",
      "          +++ service +++ service +++ [0.071 0.05  0.034 0.042 0.037 0.127]\n",
      "the stuff tastes amazing and it does magic for someone trying to loosen up on the dance floor\n",
      "          +++ service +++ service +++ [0.077 0.053 0.036 0.041 0.042 0.138]\n",
      "the disco is great too , although sometimes a little empty , but still worth checking out\n",
      "          +++ overall +++ service +++ [0.076 0.053 0.029 0.046 0.029 0.075]\n",
      "the worst part of my trip was the vendors , they do not let up , and i am a very well mannered person , which makes it hard to shut them down over and over again , make sure you do not tell them you like anything until you know you 're going to buy it , otherwise you 'll have to beat\n",
      "          +++ service +++ service +++ [0.075 0.043 0.013 0.035 0.018 0.103]\n",
      "them off with a stick to get away\n",
      "          +++ overall +++ service +++ [0.061 0.037 0.011 0.03  0.017 0.051]\n",
      "try to make it to the theatre for the shows at 10pm every night , they are worth it\n",
      "          +++ overall +++ service +++ [0.069 0.046 0.017 0.043 0.024 0.053]\n",
      "the ice breaker shows are fun too , it gets people into the swing of things\n",
      "          +++ overall +++ service +++ [0.046 0.03  0.013 0.038 0.02  0.042]\n",
      "all in all , i would highly recommend this resort for anyone going on a honeymoon or a romantic time with the better half , there is not a very big single crowd , so parents beware taking your single sons and daughters to this resort if they 're looking to party with other singles\n",
      "          +++ overall +++ value +++ [0.276 0.191 0.059 0.094 0.044 0.121]\n",
      "hope this helps you\n",
      "          +++ overall +++ value +++ [0.242 0.178 0.05  0.089 0.046 0.146]\n",
      "adios amigos and amigas\n",
      "          +++ overall +++ service +++ [0.217 0.162 0.055 0.088 0.053 0.165]\n",
      "===========\n",
      "truth:\n",
      "[3, 2, 3, 3, 3, 4]\n",
      "prediction:\n",
      "tensor([3, 3, 3, 4, 4, 4])\n",
      "doc:\n",
      "great service , nice hotel , mediocre food\n",
      "          +++ service +++ service +++ [0.196 0.151 0.059 0.079 0.062 0.279]\n",
      "my husband and i stayed at excellence for five nights mid - november\n",
      "          +++ overall +++ value +++ [0.238 0.171 0.104 0.098 0.063 0.136]\n",
      "we booked our trip at the very last minute so we were not able to do a ton of research on the dominican but the hotel receives high ratings thorughout the web\n",
      "          +++ overall +++ location +++ [0.092 0.052 0.026 0.074 0.012 0.053]\n",
      "after the one hour ride from the airport we arrived at the hotel and were greeted by everyone we met\n",
      "          +++ service +++ service +++ [0.064 0.033 0.023 0.052 0.015 0.227]\n",
      "i have to say that the staff at the hotel were very nice and made every effort to learn our names and greet us by name each time they saw us\n",
      "          +++ service +++ service +++ [0.071 0.053 0.037 0.04  0.052 0.597]\n",
      "we opted to upgrade to the excellence club and we are still trying to decide if we think it was worth it or not\n",
      "          +++ overall +++ service +++ [0.18  0.123 0.082 0.077 0.04  0.132]\n",
      "as part of the excellence club , you are ushered to the club 's private lobby for check - in but , really , it almost just creates an unneccesary step in the check - in process and adds another person or two you feel like you should tip\n",
      "          +++ service +++ service +++ [0.098 0.066 0.057 0.053 0.034 0.284]\n",
      "the biggest benefits of the excellence club for us were the unlimited internet access , beach towels in the room ( they were hard to get otherwise ) , and the beach bag in our room\n",
      "          +++ service +++ service +++ [0.082 0.061 0.103 0.067 0.046 0.103]\n",
      "we did eat breakfast each morning in the excellence club which was nice because it was a small buffet and you did not have to deal with a crowd\n",
      "          +++ overall +++ value +++ [0.129 0.088 0.075 0.066 0.037 0.086]\n",
      "the hotel itself was clean , the staff was very friendly , and nothing ever felt crowded\n",
      "          +++ service +++ service +++ [0.097 0.088 0.118 0.067 0.158 0.386]\n",
      "however , the food was not great\n",
      "          +++ overall +++ value +++ [0.213 0.142 0.031 0.049 0.034 0.09 ]\n",
      "it was not bad - but it was not great\n",
      "          +++ overall +++ value +++ [0.197 0.129 0.028 0.054 0.034 0.071]\n",
      "i 'm not a big eater but i was prepared to indulge on my vacation and there just was not anything i was crazy about\n",
      "          +++ overall +++ value +++ [0.212 0.134 0.041 0.06  0.048 0.073]\n",
      "the presentation of the food was nice but it was just bland\n",
      "          +++ overall +++ value +++ [0.197 0.127 0.024 0.044 0.035 0.072]\n",
      "i think that is the best way to describe it\n",
      "          +++ overall +++ value +++ [0.152 0.101 0.037 0.066 0.039 0.073]\n",
      "the pizzas that were delivered to the pool area were good but it was unpredictable because you never knew when they would arrive\n",
      "          +++ overall +++ service +++ [0.14  0.097 0.03  0.049 0.039 0.123]\n",
      "we went on two excursions - swimming with the sting - rays/sharks and the zip - line tour\n",
      "          +++ location +++ location +++ [0.029 0.016 0.004 0.059 0.01  0.013]\n",
      "we loved the zip - line excursion\n",
      "          +++ location +++ location +++ [0.027 0.016 0.006 0.061 0.01  0.018]\n",
      "the staff was great and our bus driver and tour guide were great\n",
      "          +++ service +++ service +++ [0.041 0.03  0.014 0.051 0.023 0.12 ]\n",
      "it was interesting to visit the sting - rays and swim with the sharks but the reef where we snorkeled was disappointing\n",
      "          +++ location +++ location +++ [0.027 0.015 0.003 0.104 0.007 0.011]\n",
      "the fish were very small and there was not much to see\n",
      "          +++ location +++ location +++ [0.047 0.03  0.01  0.088 0.014 0.025]\n",
      "the electricity went out in our room a handful of times , especially when i used the hairdryer\n",
      "          +++ room +++ room +++ [0.053 0.04  0.055 0.024 0.022 0.035]\n",
      "also , our ac was terrible\n",
      "          +++ room +++ room +++ [0.093 0.06  0.22  0.025 0.034 0.028]\n",
      "they tried to repair it but it just never got cool\n",
      "          +++ room +++ room +++ [0.098 0.072 0.223 0.032 0.059 0.066]\n",
      "our room was big , though , and clean\n",
      "          +++ room +++ room +++ [0.066 0.054 0.412 0.035 0.133 0.041]\n",
      "we always got housekeeping service twice a day and they refilled our mini - bar daily\n",
      "          +++ service +++ service +++ [0.078 0.064 0.174 0.048 0.163 0.241]\n",
      "in many of the reviews , people said they got sick\n",
      "          +++ service +++ service +++ [0.109 0.073 0.037 0.044 0.062 0.146]\n",
      "our representative at the hotel ( through aaa ) warned us that many people think they get sick from the water or the food but they do not realize that having too many drinks with coconut in them will also do it\n",
      "          +++ service +++ service +++ [0.101 0.064 0.031 0.04  0.038 0.109]\n",
      "coconut is a natural laxative so you need to limit your consumption\n",
      "          +++ overall +++ service +++ [0.085 0.054 0.023 0.049 0.039 0.058]\n",
      "i would still pack the immodium just to be sure\n",
      "          +++ overall +++ value +++ [0.106 0.073 0.029 0.065 0.037 0.063]\n",
      "i could not decide if i wanted to give this hotel a 3/5 or a 4/5 but i decided to go up because of the friendly staff and the cleanliness of our room\n",
      "          +++ clean +++ clean +++ [0.096 0.081 0.196 0.06  0.224 0.21 ]\n",
      "i do not think i would go back because of the food but we had a nice time while we were there\n",
      "          +++ overall +++ value +++ [0.315 0.222 0.031 0.093 0.043 0.116]\n",
      "we met a lot of great people at the swim up bar\n",
      "          +++ service +++ service +++ [0.078 0.056 0.018 0.048 0.028 0.116]\n",
      "===========\n",
      "truth:\n",
      "[3, 4, 3, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([3, 4, 4, 4, 4, 4])\n",
      "doc:\n",
      "very relaxing experience just returned from my 40th birthday romantic getaway with my husband\n",
      "          +++ overall +++ value +++ [0.237 0.169 0.097 0.103 0.049 0.11 ]\n",
      "this was our first time in the dominican republic , and we have literally been to every single island in the caribbean\n",
      "          +++ overall +++ value +++ [0.211 0.141 0.054 0.126 0.042 0.103]\n",
      "so i can assure you that my review will be short , sweet , and comprehensive\n",
      "          +++ overall +++ location +++ [0.192 0.126 0.054 0.131 0.038 0.104]\n",
      "in general , we liked the dr , and the excellence was very nice\n",
      "          +++ overall +++ value +++ [0.228 0.181 0.079 0.13  0.061 0.143]\n",
      "the top reasons why we liked excellence were : 1) no kids ( ie\n",
      "          +++ overall +++ service +++ [0.075 0.049 0.043 0.046 0.028 0.056]\n",
      ", if i want to get away from my own kids , i definitely do not want to vacation with other peoples ' kids )\n",
      "          +++ overall +++ service +++ [0.102 0.073 0.042 0.07  0.042 0.091]\n",
      "it was so quiet\n",
      "          +++ overall +++ room +++ [0.071 0.052 0.07  0.061 0.038 0.052]\n",
      "2) the staff and the people in the dr in general\n",
      "          +++ service +++ service +++ [0.095 0.08  0.067 0.059 0.097 0.473]\n",
      "so genuinely friendly , helpful , and wonderful\n",
      "          +++ service +++ service +++ [0.048 0.039 0.027 0.025 0.062 0.745]\n",
      "believe me , this is not true in most other areas of the caribbean\n",
      "          +++ service +++ service +++ [0.074 0.055 0.028 0.057 0.042 0.18 ]\n",
      "the all - inclusive feature\n",
      "          +++ overall +++ location +++ [0.061 0.038 0.013 0.05  0.016 0.033]\n",
      "loved being served , served , served\n",
      "          +++ service +++ service +++ [0.089 0.065 0.047 0.055 0.053 0.137]\n",
      "i wanted to sit on my [ - - ] all day and just be a gluttonous pig\n",
      "          +++ service +++ service +++ [0.112 0.077 0.036 0.052 0.042 0.12 ]\n",
      "and this is the perfect place to do it\n",
      "          +++ overall +++ value +++ [0.248 0.189 0.079 0.114 0.063 0.175]\n",
      "4) the best selection of beach and pool lounge chairs , beds , and hammocks i 've ever seen\n",
      "          +++ overall +++ service +++ [0.096 0.081 0.055 0.092 0.072 0.095]\n",
      "there were palapas everywhere , so there was no shortage of shade\n",
      "          +++ location +++ location +++ [0.074 0.057 0.033 0.078 0.037 0.052]\n",
      "there were so many beds , you did not have to worry about not getting one\n",
      "          +++ overall +++ room +++ [0.088 0.058 0.073 0.061 0.049 0.052]\n",
      "i 'd never had the chance to sleep on a beach bed , because usually hotels have only a few , so you end up looking longingly at the lucky few who get them\n",
      "          +++ room +++ room +++ [0.102 0.073 0.149 0.057 0.06  0.059]\n",
      "ok , so here 's what i did not love about excellence :1) beach is not swimmable\n",
      "          +++ location +++ location +++ [0.014 0.008 0.003 0.213 0.005 0.006]\n",
      "way too rough most of the time\n",
      "          +++ location +++ location +++ [0.007 0.004 0.001 0.425 0.002 0.002]\n",
      "for this reason alone , i 'd not return here\n",
      "          +++ overall +++ value +++ [0.301 0.215 0.046 0.08  0.053 0.099]\n",
      "i 'm a beach fan , and love to swim in the warm caribbean sea\n",
      "          +++ location +++ location +++ [0.047 0.031 0.006 0.284 0.009 0.015]\n",
      "i 'm no food snob , but some of the food was downright bad\n",
      "          +++ overall +++ value +++ [0.139 0.08  0.011 0.032 0.025 0.067]\n",
      "and you end up eating in the same spot for breakfast and lunch\n",
      "          +++ overall +++ value +++ [0.173 0.109 0.03  0.039 0.032 0.066]\n",
      "even though they have like 7 restaurants - the majority of them are only open for dinner\n",
      "          +++ overall +++ value +++ [0.142 0.093 0.033 0.035 0.029 0.067]\n",
      "we only stayed 4 nights , and we were definitely getting very tired of the breakfast/lunch selection by the 3rd day\n",
      "          +++ overall +++ value +++ [0.209 0.135 0.048 0.041 0.032 0.084]\n",
      "overall , if you just want to relax by the pool and you do not care about not going in the beach , this is a very beautiful resort\n",
      "          +++ location +++ location +++ [0.233 0.186 0.045 0.285 0.039 0.078]\n",
      "the staff is wonderful\n",
      "          +++ service +++ service +++ [0.053 0.041 0.024 0.028 0.057 0.712]\n",
      "if you are looking for a place to party and be loud and crazy , this is not your place\n",
      "          +++ overall +++ location +++ [0.254 0.177 0.054 0.189 0.025 0.095]\n",
      "===========\n",
      "truth:\n",
      "[1, 0, 2, 3, 1, 0]\n",
      "prediction:\n",
      "tensor([0, 0, 0, 3, 0, 0])\n",
      "doc:\n",
      "5- star views\n",
      "          +++ overall +++ location +++ [0.159 0.117 0.052 0.117 0.025 0.068]\n",
      "2- star service i do not know where to start\n",
      "          +++ service +++ service +++ [0.175 0.12  0.046 0.047 0.039 0.221]\n",
      "the roaches in the room , the rude waiters , bartenders , front desk , the dead flies that stayed on our friends ' mirror the entire stay , the average at best food ( only one morning in the bathroom for longer than you would want ) , the 6,7,8 times i had to trip the breakers so my wife could use the\n",
      "          +++ room +++ room +++ [0.093 0.073 0.263 0.042 0.175 0.092]\n",
      "hair dryer without our power going out , or the waste of money the excellence club turned out to be\n",
      "          +++ room +++ room +++ [0.171 0.125 0.195 0.058 0.116 0.13 ]\n",
      "i guess i 'll start with the good\n",
      "          +++ overall +++ value +++ [0.127 0.09  0.072 0.045 0.036 0.08 ]\n",
      "the beach was fabulous\n",
      "          +++ location +++ location +++ [0.038 0.03  0.022 0.195 0.021 0.023]\n",
      "the resort itself , d?cor , pool , beach access was great\n",
      "          +++ room +++ room +++ [0.101 0.085 0.161 0.107 0.107 0.102]\n",
      "ok now for the rest of the trip\n",
      "          +++ overall +++ location +++ [0.162 0.119 0.079 0.134 0.053 0.08 ]\n",
      "we booked the excellence after changing from another resort we booked\n",
      "          +++ overall +++ value +++ [0.09  0.054 0.036 0.035 0.016 0.038]\n",
      "we booked the other one a little quickly and then read some really bad reviews\n",
      "          +++ overall +++ location +++ [0.056 0.027 0.013 0.031 0.007 0.029]\n",
      "so we were able to get out of that one and do a little more homework\n",
      "          +++ overall +++ location +++ [0.054 0.029 0.015 0.04  0.009 0.029]\n",
      "we read about the excellence from trip advisor and were really excited to go\n",
      "          +++ overall +++ service +++ [0.106 0.056 0.033 0.047 0.019 0.06 ]\n",
      "now , we 've been to the caribbean plenty and are low maintenance travelers\n",
      "          +++ overall +++ service +++ [0.075 0.041 0.036 0.045 0.02  0.061]\n",
      "we 'll check in and the hotel usually does not hear from us until we leave\n",
      "          +++ service +++ service +++ [0.089 0.053 0.044 0.058 0.02  0.092]\n",
      "one thing we usually like to do is get to the front desk and see about upgrading rooms\n",
      "          +++ service +++ service +++ [0.062 0.041 0.05  0.044 0.017 0.071]\n",
      "we were originally booked in a garden view room\n",
      "          +++ overall +++ room +++ [0.041 0.026 0.041 0.031 0.013 0.032]\n",
      "our first question at the front desk was , ??o you have any ocean view rooms available\n",
      "          +++ overall +++ service +++ [0.058 0.036 0.037 0.032 0.009 0.042]\n",
      "he said , ??es , let me tell you about the excellence club\n",
      "          +++ service +++ service +++ [0.056 0.035 0.037 0.035 0.011 0.057]\n",
      "the excellence club rooms are identical to every other room in the resort ( but with a plasma tv ) and you have access to what?? basically another room where you can eat ( the same food that?? served everywhere else ) , have premium drinks , and check out dvds\n",
      "          +++ room +++ room +++ [0.086 0.062 0.124 0.066 0.043 0.078]\n",
      "he showed us where we would be staying\n",
      "          +++ service +++ service +++ [0.054 0.03  0.04  0.043 0.013 0.064]\n",
      "it was perfect , right in front of the pool over looking the ocean\n",
      "          +++ room +++ room +++ [0.067 0.053 0.112 0.079 0.042 0.066]\n",
      "much to our dismay , we show up and still have a garden view\n",
      "          +++ room +++ room +++ [0.077 0.052 0.1   0.069 0.036 0.068]\n",
      "we call the front desk and ask where the ocean view room is and he commences to telling us how wonderful the excellence club is\n",
      "          +++ service +++ service +++ [0.062 0.037 0.046 0.051 0.015 0.089]\n",
      "so great , the hustle is on\n",
      "          +++ location +++ location +++ [0.055 0.035 0.046 0.141 0.012 0.045]\n",
      "after spending much of the first night arguing back and forth while he?? ??ooking into it?\n",
      "          +++ service +++ service +++ [0.071 0.044 0.053 0.053 0.016 0.081]\n",
      "we finally gave up and waited until the morning\n",
      "          +++ overall +++ service +++ [0.064 0.033 0.029 0.032 0.007 0.052]\n",
      "they finally moved us to a pool front room but i still don?? why we had to pay $400 extra per couple for a plasma tv and movies\n",
      "          +++ room +++ room +++ [0.087 0.061 0.111 0.044 0.029 0.057]\n",
      "the movies ended up being ok since one morning i stayed in bed with an upset stomach and my wife did the same thing a couple of nights\n",
      "          +++ room +++ room +++ [0.1   0.071 0.128 0.035 0.037 0.042]\n",
      "the resort was pretty much empty so we were a little confused why there were no ocean front rooms available\n",
      "          +++ overall +++ location +++ [0.086 0.059 0.077 0.083 0.023 0.058]\n",
      "the restaurants were empty , the bars were empty\n",
      "          +++ overall +++ value +++ [0.1   0.065 0.047 0.054 0.021 0.061]\n",
      "i guess enjoying their ocean front rooms\n",
      "          +++ overall +++ location +++ [0.092 0.063 0.082 0.084 0.022 0.049]\n",
      "another strange phenomenon was with it being so empty , why were the waits so long\n",
      "          +++ overall +++ room +++ [0.091 0.055 0.057 0.057 0.011 0.045]\n",
      "waiting for service , waiting for a table , waiting for a drink , waiting at the front desk\n",
      "          +++ service +++ service +++ [0.093 0.06  0.035 0.02  0.021 0.202]\n",
      "the place was empty\n",
      "          +++ service +++ service +++ [0.116 0.071 0.039 0.045 0.02  0.127]\n",
      "there were a few nice waiters but most of them were rude , acted as if we were bothering them and sometimes just stood there and looked at us like we were stupid or something\n",
      "          +++ service +++ service +++ [0.081 0.051 0.02  0.021 0.024 0.561]\n",
      "i was amazed\n",
      "          +++ service +++ service +++ [0.088 0.055 0.027 0.028 0.021 0.275]\n",
      "i would recommend bringing an electrician with you as well because you??l need to get the power turned on if you want to dry your hair and run the air conditioner at the same time\n",
      "          +++ overall +++ room +++ [0.11  0.074 0.106 0.035 0.033 0.061]\n",
      "watch your step walking underneath the vent as well so you won?? slip in the leaking water from the vent\n",
      "          +++ room +++ room +++ [0.099 0.061 0.157 0.048 0.038 0.03 ]\n",
      "we had roaches in our bar , the couple that went with us noticed dead flies stuck to their mirror in clear sight and they stayed there the whole time we were there\n",
      "          +++ room +++ room +++ [0.09  0.06  0.182 0.048 0.158 0.062]\n",
      "average at best\n",
      "          +++ room +++ room +++ [0.135 0.093 0.174 0.045 0.112 0.05 ]\n",
      "the quality wasn?? very good\n",
      "          +++ room +++ room +++ [0.116 0.084 0.123 0.044 0.08  0.063]\n",
      "we spent the whole trip scared of it after we were in the bathroom the next morning\n",
      "          +++ room +++ room +++ [0.103 0.067 0.116 0.038 0.051 0.034]\n",
      "we called room service one night and they were out of pepperonis for the pizza\n",
      "          +++ service +++ service +++ [0.068 0.047 0.031 0.017 0.026 0.181]\n",
      "( that was more funny than annoying ) and our friends went to the italian restaurant and ate dinner\n",
      "          +++ overall +++ service +++ [0.09  0.058 0.034 0.027 0.022 0.09 ]\n",
      "after , they ordered dessert : ??e??e out of tiramisu\n",
      "          +++ service +++ service +++ [0.054 0.039 0.023 0.019 0.018 0.07 ]\n",
      "??e have one cheesecake?\n",
      "          +++ overall +++ service +++ [0.055 0.04  0.028 0.017 0.02  0.049]\n",
      "ok , what kind\n",
      "          +++ overall +++ value +++ [0.066 0.051 0.027 0.016 0.018 0.046]\n",
      "??o , we have one piece of cheesecake left\n",
      "          +++ overall +++ service +++ [0.049 0.035 0.022 0.016 0.019 0.044]\n",
      "ran out of dessert\n",
      "          +++ overall +++ service +++ [0.049 0.034 0.018 0.014 0.014 0.044]\n",
      "like i said before , we??e low maintenance travelers\n",
      "          +++ service +++ service +++ [0.081 0.057 0.036 0.02  0.026 0.101]\n",
      "we go to a cheaper place and have to wait or deal with annoyances , we don?? mind\n",
      "          +++ overall +++ service +++ [0.1   0.07  0.038 0.024 0.019 0.082]\n",
      "you get what you pay for\n",
      "          +++ overall +++ value +++ [0.102 0.074 0.031 0.024 0.02  0.071]\n",
      "but this is supposed to be a 4-5 star establishment and it seemed as though they didn?? know what the heck they were doing\n",
      "          +++ overall +++ service +++ [0.124 0.092 0.043 0.03  0.028 0.098]\n",
      "it was just a comedy of issues from the time we showed up to the time we left\n",
      "          +++ overall +++ value +++ [0.137 0.094 0.051 0.034 0.022 0.074]\n",
      "the whole trip was ruined and that?? a few thousand dollars we wish we had back\n",
      "          +++ overall +++ value +++ [0.282 0.189 0.053 0.059 0.031 0.11 ]\n",
      "there are so many other options available down there , look elsewhere\n",
      "          +++ overall +++ value +++ [0.248 0.17  0.043 0.041 0.025 0.071]\n",
      "i know we??l never return\n",
      "          +++ overall +++ value +++ [0.343 0.236 0.038 0.046 0.032 0.108]\n",
      "===========\n",
      "truth:\n",
      "[3, 3, 3, 4, 3, 4]\n",
      "prediction:\n",
      "tensor([4, 4, 4, 4, 4, 4])\n",
      "doc:\n",
      "we just got back yesterday from a one week stay at the excellence resort in punta cana\n",
      "          +++ overall +++ value +++ [0.259 0.148 0.078 0.084 0.051 0.107]\n",
      "i had done my research online and found that with the exception of the food , there were almost 100% positive things that people had to say about this resort\n",
      "          +++ overall +++ value +++ [0.272 0.178 0.063 0.088 0.05  0.122]\n",
      "it all lived up to be true\n",
      "          +++ overall +++ value +++ [0.252 0.177 0.083 0.097 0.064 0.14 ]\n",
      "the resort was magical\n",
      "          +++ service +++ service +++ [0.163 0.131 0.155 0.105 0.111 0.179]\n",
      "my wife and i went with 2 friends of ours and were blown away\n",
      "          +++ overall +++ service +++ [0.19  0.128 0.091 0.095 0.052 0.129]\n",
      "the pool and beach were spectacular\n",
      "          +++ room +++ room +++ [0.082 0.068 0.114 0.081 0.105 0.086]\n",
      "as tropical beach rooms go , the rooms were nicely appointed with an open feel\n",
      "          +++ room +++ room +++ [0.053 0.045 0.426 0.033 0.114 0.04 ]\n",
      "bathtub in the room , shower in the bathroom , was a nice tough\n",
      "          +++ room +++ room +++ [0.052 0.039 0.398 0.026 0.078 0.028]\n",
      "the only negative thing i can say about this experience was the food\n",
      "          +++ overall +++ value +++ [0.245 0.158 0.046 0.066 0.042 0.086]\n",
      "even though at all the restaurants you ordered from menus , the food was prepared banquet style\n",
      "          +++ overall +++ value +++ [0.173 0.109 0.046 0.053 0.037 0.081]\n",
      "what i mean by this is that it was not as hot and fresh as food would typically be that is prepared for you\n",
      "          +++ overall +++ value +++ [0.175 0.113 0.048 0.043 0.031 0.07 ]\n",
      "if you are a person that typically eats in nice restaurants in cities like new york , san francisco , etc\n",
      "          +++ overall +++ value +++ [0.149 0.099 0.056 0.08  0.04  0.077]\n",
      "you will probably rate this food around a c+ or b -\n",
      "          +++ overall +++ value +++ [0.244 0.169 0.05  0.064 0.041 0.099]\n",
      "though i am not typically a buffet type person , i would highly recommend you go to their buffet for both breakfast and lunch\n",
      "          +++ overall +++ value +++ [0.189 0.13  0.056 0.078 0.049 0.092]\n",
      "you will not be disappointed as they will prepare eggs , meats , pastas individually for you\n",
      "          +++ overall +++ value +++ [0.194 0.129 0.055 0.079 0.06  0.106]\n",
      "lastly on the food , steer clear of the lobster house\n",
      "          +++ overall +++ location +++ [0.087 0.058 0.024 0.065 0.028 0.049]\n",
      "it was the only restaurant that highly disappointed us\n",
      "          +++ overall +++ value +++ [0.103 0.069 0.026 0.042 0.026 0.064]\n",
      "every place else ( the mexican , french , italian , etc\n",
      "          +++ overall +++ value +++ [0.112 0.073 0.062 0.066 0.04  0.07 ]\n",
      ") provided the ambiance , a decent meal , wine , etc\n",
      "          +++ overall +++ service +++ [0.12  0.084 0.046 0.068 0.044 0.084]\n",
      "for a very nice evening\n",
      "          +++ overall +++ value +++ [0.112 0.077 0.034 0.062 0.042 0.062]\n",
      "outside of the food , the resort and its people were truly amazing\n",
      "          +++ service +++ service +++ [0.079 0.054 0.034 0.038 0.069 0.45 ]\n",
      "we will definitely go back\n",
      "          +++ overall +++ value +++ [0.287 0.236 0.07  0.095 0.08  0.207]\n",
      "this review was written by an ad agency executive who travels almost half the year in mostly big cities like la , new york , las vegas and san francisco\n",
      "          +++ overall +++ service +++ [0.159 0.091 0.036 0.093 0.024 0.118]\n",
      "i am lucky enough to entertain clients in very fine restaurants and would probably be considered a food snob\n",
      "          +++ overall +++ value +++ [0.208 0.134 0.044 0.058 0.034 0.111]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([4, 4, 4, 4, 4, 4])\n",
      "doc:\n",
      "life does not get better than this\n",
      "          +++ overall +++ value +++ [0.19  0.109 0.038 0.052 0.028 0.075]\n",
      "my husband and i stayed at excellence punta cana from 10th nov - 24th nov\n",
      "          +++ overall +++ value +++ [0.233 0.141 0.083 0.088 0.052 0.117]\n",
      "this was our first holiday in 2 years , so we were looking for somewhere where we can just relax on a beautiful beach or by the pool\n",
      "          +++ overall +++ location +++ [0.153 0.087 0.037 0.134 0.023 0.067]\n",
      "we did not expect much with regards to the food after having read poor reviews and to be honest some of them had us a little worried\n",
      "          +++ overall +++ value +++ [0.177 0.091 0.022 0.042 0.022 0.062]\n",
      "i am pleased to say that excellence punta cana exceeded our expectations by a mile\n",
      "          +++ overall +++ value +++ [0.293 0.209 0.061 0.101 0.053 0.14 ]\n",
      "for the first couple of day , i just walked around with my mouth open\n",
      "          +++ overall +++ value +++ [0.107 0.061 0.031 0.055 0.022 0.055]\n",
      "the place is absolutely breathtaking\n",
      "          +++ room +++ room +++ [0.102 0.08  0.13  0.101 0.116 0.104]\n",
      "we loved our room (3101) , clean , spacious and quiet\n",
      "          +++ room +++ room +++ [0.064 0.057 0.415 0.041 0.14  0.061]\n",
      "the beach is beautiful , the sea is so much fun to swim in ( we both like the waves\n",
      "          +++ location +++ location +++ [0.047 0.037 0.028 0.089 0.028 0.038]\n",
      ") , the pool is huge and sparkling clean with plenty of beds/loungers\n",
      "          +++ overall +++ room +++ [0.091 0.078 0.081 0.08  0.076 0.08 ]\n",
      "as i said we did not expect much and we 're not sure why people are complaining so much ( bearing in mind that we are very fussy eaters )\n",
      "          +++ overall +++ value +++ [0.12  0.084 0.061 0.072 0.063 0.068]\n",
      "for a start it is an all - inclusive resort not a michelin - star restaurant\n",
      "          +++ overall +++ value +++ [0.183 0.133 0.056 0.07  0.044 0.084]\n",
      "the choices were plentiful and the food was fresh , well presented and well cooked\n",
      "          +++ overall +++ service +++ [0.19  0.143 0.085 0.096 0.097 0.154]\n",
      "we were there for 2 weeks and could have probably stayed another 2 weeks without getting bored of the food\n",
      "          +++ overall +++ value +++ [0.234 0.154 0.053 0.073 0.048 0.098]\n",
      "the entertainment team do a great job , the bar staff are fun , beach  pool servers always around , the waiting staff friendly and attentive , the housekeeping ladies kept our room spotlessly clean and tidy\n",
      "          +++ service +++ service +++ [0.083 0.073 0.179 0.053 0.202 0.226]\n",
      "i 'm not going to single out any staff in particular because i think every single one played a big part in having made our stay so enjoyable , and in particular those behind the scenes who works so hard but do not get the tips or the credit they derserve\n",
      "          +++ service +++ service +++ [0.099 0.078 0.045 0.042 0.071 0.458]\n",
      "we 've had a wonderful time and although we usually prefer not to return to the same place ( so as not to spoil it ) , excellence punta cana is one of those places that we would certainly not hesitate going back to\n",
      "          +++ overall +++ value +++ [0.351 0.254 0.051 0.087 0.057 0.158]\n",
      "in fact we would have quite liked to move in\n",
      "          +++ overall +++ value +++ [0.273 0.207 0.077 0.098 0.059 0.146]\n",
      "there are so much more to say , but i could go on forever\n",
      "          +++ overall +++ value +++ [0.281 0.208 0.074 0.097 0.059 0.157]\n",
      "this place is a taste of heaven , go with the right attitude and you will not be dissapointed\n",
      "          +++ overall +++ value +++ [0.263 0.196 0.072 0.096 0.067 0.19 ]\n",
      "everything that they offer must surely satisfy 90% of guests\n",
      "          +++ service +++ service +++ [0.203 0.146 0.07  0.081 0.066 0.221]\n",
      "as for the other 10% , there is no satisfaction no matter what you do\n",
      "          +++ overall +++ service +++ [0.187 0.135 0.064 0.073 0.055 0.151]\n",
      "they should probably rather stay at home and certainly not travel to a 3rd world country\n",
      "          +++ overall +++ value +++ [0.217 0.142 0.054 0.062 0.035 0.136]\n",
      "ps : 1\n",
      "          +++ overall +++ location +++ [0.074 0.054 0.026 0.072 0.024 0.049]\n",
      "enjoy the ride from the airport (50minutes ) , its an excursion on its own , saves having to pay to go on one in precious holiday time\n",
      "          +++ location +++ location +++ [0.044 0.029 0.01  0.154 0.01  0.022]\n",
      "find the soft serve machine by cafe kafe bar\n",
      "          +++ location +++ location +++ [0.046 0.034 0.024 0.07  0.023 0.064]\n",
      "nancy 's shop is the best , she wo not rip you off\n",
      "          +++ service +++ service +++ [0.047 0.032 0.015 0.037 0.017 0.068]\n",
      "( with the other vendors everything always starts at $200, and you can get it down to $15, nancy gives a reasonable price from the start )4\n",
      "          +++ service +++ service +++ [0.038 0.026 0.014 0.025 0.017 0.04 ]\n",
      "go to the sports bar for ice cold water\n",
      "          +++ service +++ service +++ [0.041 0.031 0.017 0.027 0.019 0.045]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([4, 4, 4, 4, 4, 4])\n",
      "doc:\n",
      "everything is excellent at excellence punta cana we just got back from our honeymoon\n",
      "          +++ overall +++ service +++ [0.242 0.159 0.103 0.092 0.066 0.159]\n",
      "we had an amazing time at excellence punta cana\n",
      "          +++ overall +++ value +++ [0.266 0.194 0.093 0.097 0.075 0.169]\n",
      "we felt we were in paradise\n",
      "          +++ overall +++ service +++ [0.191 0.127 0.097 0.088 0.064 0.159]\n",
      "the hotel and service was excellent\n",
      "          +++ service +++ service +++ [0.099 0.084 0.074 0.061 0.106 0.468]\n",
      "the whole staff was very friendly and so polite\n",
      "          +++ service +++ service +++ [0.061 0.05  0.036 0.032 0.066 0.645]\n",
      "my husband and i did not want to leave the resort\n",
      "          +++ service +++ service +++ [0.134 0.098 0.061 0.059 0.07  0.314]\n",
      "they made us feel very special\n",
      "          +++ service +++ service +++ [0.074 0.054 0.043 0.042 0.064 0.377]\n",
      "we have never experience a trip like this one\n",
      "          +++ service +++ service +++ [0.185 0.129 0.081 0.08  0.065 0.203]\n",
      "we loved it so much we are gathering a group of couple to go again early next summer\n",
      "          +++ overall +++ service +++ [0.24  0.174 0.085 0.092 0.063 0.177]\n",
      "we never experience a problem while our stay\n",
      "          +++ overall +++ service +++ [0.21  0.152 0.073 0.081 0.068 0.184]\n",
      "all of our belongings we safe , nothing was missing\n",
      "          +++ service +++ service +++ [0.167 0.109 0.06  0.065 0.055 0.183]\n",
      "ca not wait to go back\n",
      "          +++ overall +++ value +++ [0.325 0.235 0.065 0.088 0.068 0.177]\n",
      "much love to maria isabel and carlos from excellence club concierge - - from mr\n",
      "          +++ service +++ service +++ [0.1   0.056 0.032 0.036 0.033 0.248]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([4, 4, 4, 4, 4, 4])\n",
      "doc:\n",
      "the pool and beach were beautiful this hotel catered to every persons needs\n",
      "          +++ service +++ service +++ [0.171 0.136 0.125 0.091 0.094 0.183]\n",
      "i love the fact that it is all inclusive food and alcohol\n",
      "          +++ overall +++ service +++ [0.116 0.087 0.034 0.073 0.03  0.092]\n",
      "just because the hotel says it is all inclusive does not mean you dont have to tip\n",
      "          +++ service +++ service +++ [0.137 0.108 0.046 0.056 0.054 0.273]\n",
      "the people there work 12 days straight with 2 days off\n",
      "          +++ service +++ service +++ [0.103 0.072 0.036 0.035 0.039 0.251]\n",
      "they work up to 16 hours a day to make your experience there comfortable\n",
      "          +++ service +++ service +++ [0.105 0.081 0.033 0.044 0.063 0.452]\n",
      "so i made sure i tipped everyone who helped me\n",
      "          +++ service +++ service +++ [0.108 0.084 0.041 0.048 0.06  0.385]\n",
      "it is 35 pesos to one american dollar so you can do the math\n",
      "          +++ service +++ service +++ [0.074 0.051 0.024 0.041 0.022 0.11 ]\n",
      "the people that work in entertainment , cesar , mariel , ines , whinny were all very good\n",
      "          +++ service +++ service +++ [0.085 0.059 0.018 0.038 0.04  0.354]\n",
      "they taught me and my husband many things about there culture such as dancing\n",
      "          +++ service +++ service +++ [0.052 0.03  0.011 0.04  0.016 0.075]\n",
      "also there was a bartender named juan who made the best drinks there\n",
      "          +++ service +++ service +++ [0.048 0.034 0.014 0.04  0.022 0.132]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "asp_inc_overall = True\n",
    "if not asp_inc_overall: \n",
    "    nasp_analysis = hyper_params[\"num_aspect\"] - 1\n",
    "else:\n",
    "    nasp_analysis = hyper_params[\"num_aspect\"]\n",
    "    \n",
    "np.set_printoptions(precision=3)\n",
    "asp_name = [\"overall\", \"value\", \"room\", \"location\", \"clean\", \"service\"]\n",
    "for i in range(10):\n",
    "    print(\"truth:\")\n",
    "    print(df_test.iloc[i,0:6].values.flatten().tolist() )\n",
    "    print(\"prediction:\")\n",
    "    print( torch.argmax(outs[i][0:6],dim=1) )\n",
    "    print(\"doc:\")\n",
    "    dasp = torch.argmax(asps[i][:,0:nasp_analysis],dim=1).numpy()\n",
    "    if asp_inc_overall: dasp_noall = torch.argmax(asps[i][:,1:6],dim=1).numpy()\n",
    "#     dasp_dist = torch.nn.functional.softmax(asps[i][:,0:nasp_analysis], dim=1).numpy()\n",
    "    dasp_dist = asps[i][:,0:nasp_analysis].numpy()\n",
    "    for senti,s in enumerate(df_test.iloc[i,6]):\n",
    "        print(s)\n",
    "        if asp_inc_overall:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]] + \" +++ \" + asp_name[dasp_noall[senti]+1] + \" +++ \" + str(dasp_dist[senti]) )\n",
    "        else:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]+1] + \" +++ \" + str(dasp_dist[senti]) )\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize regression output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth:\n",
      "[1, 0, 0, 3, 1, 1]\n",
      "prediction:\n",
      "tensor([[4.1020e-04, 9.9668e-01, 2.9009e-03, 3.9855e-06, 1.4823e-06],\n",
      "        [8.2763e-03, 9.6310e-01, 2.8592e-02, 1.9740e-05, 1.5135e-05],\n",
      "        [9.9990e-01, 1.0335e-04, 4.0020e-08, 2.1092e-08, 5.1732e-09],\n",
      "        [2.2873e-01, 1.3853e-01, 2.5908e-01, 3.5016e-01, 2.3494e-02],\n",
      "        [9.5705e-01, 4.2916e-02, 3.6786e-05, 1.6780e-07, 1.2092e-07],\n",
      "        [9.9998e-01, 1.9123e-05, 1.6604e-09, 5.1853e-12, 3.6041e-11]])\n",
      "doc:\n",
      "definitely not a 5 star resort i 'm dumbfounded that this hotel gets good reviews and is so highly rated\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "it 's decidedly a 3 star property , not 5 stars as indicated\n",
      "          +++ value +++ [0.22 0.2  0.2  0.19 0.19]\n",
      "the rooms are very dated and run down , old crappy beds and pillows , an old tv and overall poorly maintained\n",
      "          +++ location +++ [0.17 0.17 0.28 0.17 0.21]\n",
      "the whole property is pretty run down and old - looking\n",
      "          +++ location +++ [0.19 0.18 0.25 0.18 0.21]\n",
      "the food is subpar , not one meal i had would be called great\n",
      "          +++ value +++ [0.21 0.19 0.21 0.19 0.2 ]\n",
      "the service is uneven and the staff is poorly trained and uninformed\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "many do not comprehend english\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "the beach is great , it 's the only redeeming factor\n",
      "          +++ clean +++ [0.18 0.18 0.18 0.28 0.18]\n",
      "however the resort is a 1- hour taxi trip from the airport\n",
      "          +++ clean +++ [0.19 0.19 0.19 0.25 0.19]\n",
      "===========\n",
      "truth:\n",
      "[2, 2, 1, 2, 3, 3]\n",
      "prediction:\n",
      "tensor([[1.4483e-04, 9.8034e-01, 1.8719e-02, 7.9833e-04, 1.3543e-06],\n",
      "        [3.2097e-04, 8.8404e-01, 1.1110e-01, 4.4965e-03, 4.1651e-05],\n",
      "        [9.1542e-02, 4.8621e-01, 2.8853e-02, 3.6794e-01, 2.5458e-02],\n",
      "        [2.4023e-05, 1.3841e-03, 6.8200e-01, 3.0687e-01, 9.7190e-03],\n",
      "        [1.4971e-03, 2.3923e-02, 1.7517e-01, 7.2813e-01, 7.1284e-02],\n",
      "        [2.5925e-03, 6.4041e-01, 2.5299e-01, 1.0233e-01, 1.6784e-03]])\n",
      "doc:\n",
      "facilities need work\n",
      "          +++ value +++ [0.22 0.2  0.2  0.19 0.19]\n",
      "we visited excellence for 5 nights in december\n",
      "          +++ value +++ [0.22 0.2  0.2  0.19 0.19]\n",
      "our first room , #1112, had a safe that did not work and so - so air conditioning\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.19]\n",
      "when we went to the front desk to complain , we were told to go to the room and someone would be there within 15 minutes\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "45 minutes later , the safe guy showed up , but nobody for the a/c\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "the safe guy could not fix it\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "when he left , the electricity went out\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "it went out a second time before we finally went to the front desk to change rooms\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "we had dinner that night in the lobster house\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "do not waste your time on this one\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "the lobster tails had about 2 bites of food included\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "while we were in there , the electricity went out again\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "room 3002 served us pretty well , until night #3 when my partner got up to go to the bathroom and stepped into an inch of water\n",
      "          +++ location +++ [0.2  0.19 0.21 0.19 0.19]\n",
      "a hose had broken on the back of the toilet and flooded our room\n",
      "          +++ location +++ [0.19 0.19 0.24 0.19 0.19]\n",
      "it would 've been ok , but when we went to the front desk we were told that we needed to wait until noon to see if perhaps they could move us to another room\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "the front desk clerks were not empowered to just move us\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "my partner was infuriated that they wanted us to wait 4 hours for a new room\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "finally , matias at the front desk finally arranged to have us moved to another upgraded room - 3109\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "we walked in and saw the leak coming from the ceiling and nearly flipped\n",
      "          +++ location +++ [0.2  0.19 0.22 0.2  0.19]\n",
      "we finally got into #3110, which was a gorgeous suite with a beautiful view\n",
      "          +++ location +++ [0.19 0.19 0.24 0.19 0.19]\n",
      "on the positive side , the food at the other restaurants was very good\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "i particularly liked the french restaurant , while my partner liked the asian restaurant\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "the breakfast buffet was like nothing i 'd ever seen before - lots of choices\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "the ocean was way too rough to enjoy , particularly if you 're not a strong swimmer\n",
      "          +++ clean +++ [0.2  0.2  0.19 0.22 0.19]\n",
      "much of the beach was black flagged the entire time we were there , so if you 're a big ocean fan , i do not recommend this resort\n",
      "          +++ clean +++ [0.21 0.19 0.19 0.22 0.19]\n",
      "my favorite part , by far , though , were the beds next to the pools and ocean\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.19]\n",
      "they were amazing\n",
      "          +++ location +++ [0.2  0.19 0.21 0.2  0.2 ]\n",
      "i guess you could particularly say so since the beds in the rooms were hard as rocks\n",
      "          +++ location +++ [0.19 0.19 0.24 0.19 0.19]\n",
      "all in all , a good trip - highly recommend the zip line tour\n",
      "          +++ value +++ [0.21 0.2  0.19 0.21 0.19]\n",
      "it was worth every penny\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 3, 4, 4]\n",
      "prediction:\n",
      "tensor([[1.8609e-07, 1.7810e-10, 5.0119e-11, 3.3600e-06, 1.0000e+00],\n",
      "        [1.1452e-06, 2.3374e-08, 2.8158e-09, 1.1409e-05, 9.9999e-01],\n",
      "        [5.4073e-07, 2.6483e-07, 5.1241e-07, 2.1868e-05, 9.9998e-01],\n",
      "        [1.2492e-07, 2.9025e-06, 8.7353e-06, 1.2448e-04, 9.9986e-01],\n",
      "        [2.3340e-05, 7.1462e-07, 3.3719e-06, 2.6458e-06, 9.9997e-01],\n",
      "        [1.5828e-06, 2.5712e-07, 6.6358e-07, 1.8245e-05, 9.9998e-01]])\n",
      "doc:\n",
      "excellence was exactly that\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "my family and i stayed at the excellence punta cana from december 22 to december 29 of this year\n",
      "          +++ value +++ [0.22 0.2  0.2  0.19 0.19]\n",
      "it was an amazing time had by all that attended\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.2 ]\n",
      "we arrived at the resort around 4 am because of a delay at the airport in vancouver , but even at 4 am , the service of the bellhops and the front desk was up to par\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "our bags were unloaded and immediately tagged and set to one side of the lobby while we were handed cold scented towels to cool off with\n",
      "          +++ service +++ [0.2  0.19 0.2  0.2  0.21]\n",
      "check in was fairly expedient and we were in our rooms within twenty minutes\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "i had never been to an all inclusive resort before , and wasted no time enjoying the pleasures of the mini bar in the room , as well as the ample storage space for our things\n",
      "          +++ location +++ [0.2  0.19 0.21 0.19 0.2 ]\n",
      "room service even at 4 am was great , the girl on the phone said it would be about 40 minutes for the food , which seemed a little long , but i think they only say that to cover there butts , because it took about 25 minutes at most\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the activities during the day were well thought out , though , there was some delays and cancellations due to weather conditions ( beach volleyball cancelled to due strong winds\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      "the entertainment staff was amazing and extremely friendly , a special thanks to all my friends , ines ( my fiance , ) altagracia ( who lovingly reffered to me as flaco loco , which translates into crazy skinny guy , ) eliza and johanna ( my disco dance partners , ) sexy cesar ( who taught me all the sexy dance moves i\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "now know , ) julio cesar ( the mc for the games and parties\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "the restaurants i ca not offer too much help with , i did not eat at all of them , but of the few i did eat at , i recommend toscana for the huge buffet everyday , breakfast here is well prepared and quite delicious ( although i do not recommend the scrambled eggs\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.2 ]\n",
      "the omlettes are delicious and you have to try one\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.2 ]\n",
      "for lunch , you have to check out the grill on the beach , different food everyday , always good , and makes the beach smell amazing\n",
      "          +++ value +++ [0.21 0.2  0.19 0.21 0.2 ]\n",
      "for dinner , i liked spice ( asian cuisine , ) agave ( mexican , but do not eat the calimari from here , very rubbery , ) the pizza that is delivered to the pool and the beach is awesome , make sure you try that\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.2 ]\n",
      "the bars were awesome , you get accustomed to speaking the language when ordering drinks , instead of drinking your usual bacardi and coke , try the brugal extra anejo , they call it the dominican babymaker , and it 's obvious why once you try it\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "the stuff tastes amazing and it does magic for someone trying to loosen up on the dance floor\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "the disco is great too , although sometimes a little empty , but still worth checking out\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "the worst part of my trip was the vendors , they do not let up , and i am a very well mannered person , which makes it hard to shut them down over and over again , make sure you do not tell them you like anything until you know you 're going to buy it , otherwise you 'll have to beat\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "them off with a stick to get away\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "try to make it to the theatre for the shows at 10pm every night , they are worth it\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "the ice breaker shows are fun too , it gets people into the swing of things\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "all in all , i would highly recommend this resort for anyone going on a honeymoon or a romantic time with the better half , there is not a very big single crowd , so parents beware taking your single sons and daughters to this resort if they 're looking to party with other singles\n",
      "          +++ value +++ [0.23 0.2  0.19 0.19 0.19]\n",
      "hope this helps you\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "adios amigos and amigas\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "===========\n",
      "truth:\n",
      "[3, 2, 3, 3, 3, 4]\n",
      "prediction:\n",
      "tensor([[3.6580e-08, 7.1439e-08, 7.8256e-05, 9.9992e-01, 1.5043e-06],\n",
      "        [9.7199e-08, 4.7277e-07, 8.6615e-04, 9.9884e-01, 2.9803e-04],\n",
      "        [5.7395e-06, 2.4570e-04, 1.4986e-02, 9.8184e-01, 2.9182e-03],\n",
      "        [2.7667e-07, 3.0203e-06, 5.5198e-04, 2.7141e-01, 7.2803e-01],\n",
      "        [1.1826e-04, 3.1205e-05, 2.0831e-03, 4.8128e-01, 5.1648e-01],\n",
      "        [2.5054e-07, 5.7264e-07, 2.2535e-05, 8.4117e-03, 9.9156e-01]])\n",
      "doc:\n",
      "great service , nice hotel , mediocre food\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "my husband and i stayed at excellence for five nights mid - november\n",
      "          +++ value +++ [0.22 0.2  0.2  0.19 0.19]\n",
      "we booked our trip at the very last minute so we were not able to do a ton of research on the dominican but the hotel receives high ratings thorughout the web\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      "after the one hour ride from the airport we arrived at the hotel and were greeted by everyone we met\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "i have to say that the staff at the hotel were very nice and made every effort to learn our names and greet us by name each time they saw us\n",
      "          +++ service +++ [0.2  0.2  0.2  0.2  0.21]\n",
      "we opted to upgrade to the excellence club and we are still trying to decide if we think it was worth it or not\n",
      "          +++ value +++ [0.22 0.2  0.2  0.19 0.19]\n",
      "as part of the excellence club , you are ushered to the club 's private lobby for check - in but , really , it almost just creates an unneccesary step in the check - in process and adds another person or two you feel like you should tip\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "the biggest benefits of the excellence club for us were the unlimited internet access , beach towels in the room ( they were hard to get otherwise ) , and the beach bag in our room\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "we did eat breakfast each morning in the excellence club which was nice because it was a small buffet and you did not have to deal with a crowd\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "the hotel itself was clean , the staff was very friendly , and nothing ever felt crowded\n",
      "          +++ service +++ [0.2  0.2  0.19 0.2  0.21]\n",
      "however , the food was not great\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "it was not bad - but it was not great\n",
      "          +++ value +++ [0.23 0.2  0.19 0.19 0.19]\n",
      "i 'm not a big eater but i was prepared to indulge on my vacation and there just was not anything i was crazy about\n",
      "          +++ value +++ [0.23 0.2  0.19 0.2  0.19]\n",
      "the presentation of the food was nice but it was just bland\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "i think that is the best way to describe it\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "the pizzas that were delivered to the pool area were good but it was unpredictable because you never knew when they would arrive\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.19]\n",
      "we went on two excursions - swimming with the sting - rays/sharks and the zip - line tour\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "we loved the zip - line excursion\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "the staff was great and our bus driver and tour guide were great\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "it was interesting to visit the sting - rays and swim with the sharks but the reef where we snorkeled was disappointing\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      "the fish were very small and there was not much to see\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      "the electricity went out in our room a handful of times , especially when i used the hairdryer\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "also , our ac was terrible\n",
      "          +++ location +++ [0.2  0.19 0.23 0.19 0.19]\n",
      "they tried to repair it but it just never got cool\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.19]\n",
      "our room was big , though , and clean\n",
      "          +++ location +++ [0.18 0.17 0.29 0.17 0.19]\n",
      "we always got housekeeping service twice a day and they refilled our mini - bar daily\n",
      "          +++ service +++ [0.2  0.19 0.2  0.19 0.21]\n",
      "in many of the reviews , people said they got sick\n",
      "          +++ service +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "our representative at the hotel ( through aaa ) warned us that many people think they get sick from the water or the food but they do not realize that having too many drinks with coconut in them will also do it\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "coconut is a natural laxative so you need to limit your consumption\n",
      "          +++ clean +++ [0.2  0.2  0.19 0.21 0.2 ]\n",
      "i would still pack the immodium just to be sure\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.2 ]\n",
      "i could not decide if i wanted to give this hotel a 3/5 or a 4/5 but i decided to go up because of the friendly staff and the cleanliness of our room\n",
      "          +++ service +++ [0.19 0.18 0.22 0.18 0.23]\n",
      "i do not think i would go back because of the food but we had a nice time while we were there\n",
      "          +++ value +++ [0.24 0.2  0.18 0.19 0.19]\n",
      "we met a lot of great people at the swim up bar\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "===========\n",
      "truth:\n",
      "[3, 4, 3, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([[8.4084e-07, 6.6263e-06, 3.7345e-04, 9.9915e-01, 4.6555e-04],\n",
      "        [1.1268e-05, 1.6399e-05, 1.9504e-03, 9.2403e-01, 7.3995e-02],\n",
      "        [8.9798e-07, 2.3409e-06, 1.2049e-04, 3.6899e-01, 6.3089e-01],\n",
      "        [3.0712e-06, 1.4998e-05, 5.3127e-05, 1.4099e-02, 9.8583e-01],\n",
      "        [9.3287e-06, 5.6259e-08, 5.9345e-07, 8.7584e-05, 9.9990e-01],\n",
      "        [3.9845e-10, 8.1773e-11, 1.4880e-09, 4.4981e-06, 1.0000e+00]])\n",
      "doc:\n",
      "very relaxing experience just returned from my 40th birthday romantic getaway with my husband\n",
      "          +++ value +++ [0.21 0.2  0.19 0.21 0.19]\n",
      "this was our first time in the dominican republic , and we have literally been to every single island in the caribbean\n",
      "          +++ clean +++ [0.21 0.2  0.19 0.21 0.19]\n",
      "so i can assure you that my review will be short , sweet , and comprehensive\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.2 ]\n",
      "in general , we liked the dr , and the excellence was very nice\n",
      "          +++ value +++ [0.22 0.2  0.18 0.19 0.2 ]\n",
      "the top reasons why we liked excellence were : 1) no kids ( ie\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      ", if i want to get away from my own kids , i definitely do not want to vacation with other peoples ' kids )\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.19]\n",
      "it was so quiet\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "2) the staff and the people in the dr in general\n",
      "          +++ service +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "so genuinely friendly , helpful , and wonderful\n",
      "          +++ service +++ [0.2  0.2  0.2  0.2  0.21]\n",
      "believe me , this is not true in most other areas of the caribbean\n",
      "          +++ clean +++ [0.2  0.2  0.19 0.21 0.2 ]\n",
      "the all - inclusive feature\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "loved being served , served , served\n",
      "          +++ service +++ [0.2  0.2  0.2  0.2  0.21]\n",
      "i wanted to sit on my [ - - ] all day and just be a gluttonous pig\n",
      "          +++ value +++ [0.2  0.2  0.19 0.2  0.2 ]\n",
      "and this is the perfect place to do it\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "4) the best selection of beach and pool lounge chairs , beds , and hammocks i 've ever seen\n",
      "          +++ clean +++ [0.2  0.19 0.2  0.21 0.2 ]\n",
      "there were palapas everywhere , so there was no shortage of shade\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      "there were so many beds , you did not have to worry about not getting one\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "i 'd never had the chance to sleep on a beach bed , because usually hotels have only a few , so you end up looking longingly at the lucky few who get them\n",
      "          +++ location +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "ok , so here 's what i did not love about excellence :1) beach is not swimmable\n",
      "          +++ clean +++ [0.19 0.19 0.19 0.24 0.19]\n",
      "way too rough most of the time\n",
      "          +++ clean +++ [0.19 0.19 0.19 0.24 0.19]\n",
      "for this reason alone , i 'd not return here\n",
      "          +++ value +++ [0.25 0.21 0.18 0.19 0.17]\n",
      "i 'm a beach fan , and love to swim in the warm caribbean sea\n",
      "          +++ clean +++ [0.19 0.19 0.19 0.24 0.19]\n",
      "i 'm no food snob , but some of the food was downright bad\n",
      "          +++ value +++ [0.21 0.2  0.19 0.21 0.19]\n",
      "and you end up eating in the same spot for breakfast and lunch\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "even though they have like 7 restaurants - the majority of them are only open for dinner\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.19]\n",
      "we only stayed 4 nights , and we were definitely getting very tired of the breakfast/lunch selection by the 3rd day\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "overall , if you just want to relax by the pool and you do not care about not going in the beach , this is a very beautiful resort\n",
      "          +++ value +++ [0.22 0.2  0.18 0.21 0.18]\n",
      "the staff is wonderful\n",
      "          +++ service +++ [0.2  0.2  0.2  0.2  0.21]\n",
      "if you are looking for a place to party and be loud and crazy , this is not your place\n",
      "          +++ clean +++ [0.22 0.2  0.19 0.22 0.18]\n",
      "===========\n",
      "truth:\n",
      "[1, 0, 2, 3, 1, 0]\n",
      "prediction:\n",
      "tensor([[4.2200e-02, 9.5585e-01, 1.7459e-03, 1.8237e-04, 1.9602e-05],\n",
      "        [9.2379e-02, 8.9690e-01, 1.0464e-02, 1.4482e-04, 1.0815e-04],\n",
      "        [6.7303e-01, 3.0013e-01, 1.5393e-02, 1.0660e-02, 7.9320e-04],\n",
      "        [2.1633e-04, 1.2704e-03, 9.9103e-02, 7.3378e-01, 1.6563e-01],\n",
      "        [4.5772e-01, 4.4981e-01, 8.1876e-02, 8.7199e-03, 1.8734e-03],\n",
      "        [9.0938e-01, 8.4787e-02, 4.9554e-03, 3.2411e-04, 5.5284e-04]])\n",
      "doc:\n",
      "5- star views\n",
      "          +++ clean +++ [0.21 0.2  0.19 0.22 0.18]\n",
      "2- star service i do not know where to start\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "the roaches in the room , the rude waiters , bartenders , front desk , the dead flies that stayed on our friends ' mirror the entire stay , the average at best food ( only one morning in the bathroom for longer than you would want ) , the 6,7,8 times i had to trip the breakers so my wife could use the\n",
      "          +++ location +++ [0.19 0.18 0.23 0.18 0.21]\n",
      "hair dryer without our power going out , or the waste of money the excellence club turned out to be\n",
      "          +++ location +++ [0.21 0.19 0.22 0.19 0.2 ]\n",
      "i guess i 'll start with the good\n",
      "          +++ value +++ [0.22 0.2  0.2  0.19 0.19]\n",
      "the beach was fabulous\n",
      "          +++ clean +++ [0.19 0.19 0.19 0.24 0.19]\n",
      "the resort itself , d?cor , pool , beach access was great\n",
      "          +++ location +++ [0.2  0.19 0.21 0.21 0.2 ]\n",
      "ok now for the rest of the trip\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "we booked the excellence after changing from another resort we booked\n",
      "          +++ value +++ [0.22 0.2  0.2  0.2  0.19]\n",
      "we booked the other one a little quickly and then read some really bad reviews\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "so we were able to get out of that one and do a little more homework\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "we read about the excellence from trip advisor and were really excited to go\n",
      "          +++ value +++ [0.22 0.2  0.2  0.19 0.19]\n",
      "now , we 've been to the caribbean plenty and are low maintenance travelers\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "we 'll check in and the hotel usually does not hear from us until we leave\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "one thing we usually like to do is get to the front desk and see about upgrading rooms\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "we were originally booked in a garden view room\n",
      "          +++ value +++ [0.2  0.2  0.2  0.2  0.19]\n",
      "our first question at the front desk was , ??o you have any ocean view rooms available\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "he said , ??es , let me tell you about the excellence club\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "the excellence club rooms are identical to every other room in the resort ( but with a plasma tv ) and you have access to what?? basically another room where you can eat ( the same food that?? served everywhere else ) , have premium drinks , and check out dvds\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "he showed us where we would be staying\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "it was perfect , right in front of the pool over looking the ocean\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.19]\n",
      "much to our dismay , we show up and still have a garden view\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "we call the front desk and ask where the ocean view room is and he commences to telling us how wonderful the excellence club is\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "so great , the hustle is on\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.19]\n",
      "after spending much of the first night arguing back and forth while he?? ??ooking into it?\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "we finally gave up and waited until the morning\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "they finally moved us to a pool front room but i still don?? why we had to pay $400 extra per couple for a plasma tv and movies\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "the movies ended up being ok since one morning i stayed in bed with an upset stomach and my wife did the same thing a couple of nights\n",
      "          +++ location +++ [0.21 0.2  0.21 0.2  0.19]\n",
      "the resort was pretty much empty so we were a little confused why there were no ocean front rooms available\n",
      "          +++ location +++ [0.2  0.2  0.21 0.2  0.19]\n",
      "the restaurants were empty , the bars were empty\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "i guess enjoying their ocean front rooms\n",
      "          +++ value +++ [0.21 0.2  0.21 0.2  0.19]\n",
      "another strange phenomenon was with it being so empty , why were the waits so long\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "waiting for service , waiting for a table , waiting for a drink , waiting at the front desk\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "the place was empty\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.19]\n",
      "there were a few nice waiters but most of them were rude , acted as if we were bothering them and sometimes just stood there and looked at us like we were stupid or something\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "i was amazed\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "i would recommend bringing an electrician with you as well because you??l need to get the power turned on if you want to dry your hair and run the air conditioner at the same time\n",
      "          +++ location +++ [0.21 0.2  0.21 0.19 0.2 ]\n",
      "watch your step walking underneath the vent as well so you won?? slip in the leaking water from the vent\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.19]\n",
      "we had roaches in our bar , the couple that went with us noticed dead flies stuck to their mirror in clear sight and they stayed there the whole time we were there\n",
      "          +++ location +++ [0.19 0.18 0.23 0.18 0.21]\n",
      "average at best\n",
      "          +++ location +++ [0.19 0.18 0.25 0.18 0.2 ]\n",
      "the quality wasn?? very good\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.2 ]\n",
      "we spent the whole trip scared of it after we were in the bathroom the next morning\n",
      "          +++ location +++ [0.2  0.19 0.22 0.19 0.2 ]\n",
      "we called room service one night and they were out of pepperonis for the pizza\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "( that was more funny than annoying ) and our friends went to the italian restaurant and ate dinner\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "after , they ordered dessert : ??e??e out of tiramisu\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.2 ]\n",
      "??e have one cheesecake?\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "ok , what kind\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.19]\n",
      "??o , we have one piece of cheesecake left\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "ran out of dessert\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "like i said before , we??e low maintenance travelers\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "we go to a cheaper place and have to wait or deal with annoyances , we don?? mind\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.19]\n",
      "you get what you pay for\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.19]\n",
      "but this is supposed to be a 4-5 star establishment and it seemed as though they didn?? know what the heck they were doing\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "it was just a comedy of issues from the time we showed up to the time we left\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "the whole trip was ruined and that?? a few thousand dollars we wish we had back\n",
      "          +++ value +++ [0.23 0.2  0.19 0.19 0.19]\n",
      "there are so many other options available down there , look elsewhere\n",
      "          +++ value +++ [0.23 0.2  0.19 0.19 0.19]\n",
      "i know we??l never return\n",
      "          +++ value +++ [0.25 0.21 0.18 0.18 0.18]\n",
      "===========\n",
      "truth:\n",
      "[3, 3, 3, 4, 3, 4]\n",
      "prediction:\n",
      "tensor([[1.3337e-06, 2.8296e-08, 4.6831e-08, 1.3547e-03, 9.9864e-01],\n",
      "        [5.4394e-07, 1.9599e-07, 1.5283e-07, 2.3779e-04, 9.9976e-01],\n",
      "        [4.6940e-06, 3.3177e-08, 1.4304e-07, 1.3275e-03, 9.9867e-01],\n",
      "        [3.0501e-08, 5.4341e-07, 3.2844e-07, 1.8999e-05, 9.9998e-01],\n",
      "        [3.5019e-06, 5.1406e-08, 2.6429e-07, 1.9837e-06, 9.9999e-01],\n",
      "        [2.0109e-08, 4.3648e-08, 1.8418e-07, 5.4377e-05, 9.9995e-01]])\n",
      "doc:\n",
      "we just got back yesterday from a one week stay at the excellence resort in punta cana\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.2 ]\n",
      "i had done my research online and found that with the exception of the food , there were almost 100% positive things that people had to say about this resort\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "it all lived up to be true\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "the resort was magical\n",
      "          +++ value +++ [0.21 0.2  0.2  0.19 0.2 ]\n",
      "my wife and i went with 2 friends of ours and were blown away\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "the pool and beach were spectacular\n",
      "          +++ clean +++ [0.19 0.19 0.19 0.22 0.19]\n",
      "as tropical beach rooms go , the rooms were nicely appointed with an open feel\n",
      "          +++ location +++ [0.18 0.18 0.28 0.18 0.19]\n",
      "bathtub in the room , shower in the bathroom , was a nice tough\n",
      "          +++ location +++ [0.18 0.18 0.27 0.18 0.18]\n",
      "the only negative thing i can say about this experience was the food\n",
      "          +++ value +++ [0.23 0.2  0.19 0.19 0.19]\n",
      "even though at all the restaurants you ordered from menus , the food was prepared banquet style\n",
      "          +++ value +++ [0.22 0.2  0.2  0.2  0.19]\n",
      "what i mean by this is that it was not as hot and fresh as food would typically be that is prepared for you\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "if you are a person that typically eats in nice restaurants in cities like new york , san francisco , etc\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.19]\n",
      "you will probably rate this food around a c+ or b -\n",
      "          +++ value +++ [0.23 0.2  0.19 0.19 0.19]\n",
      "though i am not typically a buffet type person , i would highly recommend you go to their buffet for both breakfast and lunch\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "you will not be disappointed as they will prepare eggs , meats , pastas individually for you\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.2 ]\n",
      "lastly on the food , steer clear of the lobster house\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.19]\n",
      "it was the only restaurant that highly disappointed us\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "every place else ( the mexican , french , italian , etc\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      ") provided the ambiance , a decent meal , wine , etc\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "for a very nice evening\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.19]\n",
      "outside of the food , the resort and its people were truly amazing\n",
      "          +++ service +++ [0.2  0.2  0.19 0.2  0.21]\n",
      "we will definitely go back\n",
      "          +++ value +++ [0.23 0.2  0.18 0.19 0.2 ]\n",
      "this review was written by an ad agency executive who travels almost half the year in mostly big cities like la , new york , las vegas and san francisco\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      "i am lucky enough to entertain clients in very fine restaurants and would probably be considered a food snob\n",
      "          +++ value +++ [0.22 0.2  0.19 0.2  0.19]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([[6.3634e-09, 6.6011e-11, 1.1526e-11, 2.5008e-07, 1.0000e+00],\n",
      "        [2.9541e-08, 1.1291e-08, 5.9148e-10, 1.6055e-06, 1.0000e+00],\n",
      "        [1.1582e-07, 1.2325e-08, 1.7565e-08, 6.7471e-06, 9.9999e-01],\n",
      "        [3.5451e-08, 1.1549e-06, 7.6374e-07, 1.3465e-05, 9.9998e-01],\n",
      "        [1.9543e-06, 2.7467e-08, 1.3066e-07, 2.8922e-07, 1.0000e+00],\n",
      "        [1.2131e-08, 8.6012e-09, 2.5163e-08, 2.2591e-06, 1.0000e+00]])\n",
      "doc:\n",
      "life does not get better than this\n",
      "          +++ value +++ [0.23 0.21 0.18 0.2  0.18]\n",
      "my husband and i stayed at excellence punta cana from 10th nov - 24th nov\n",
      "          +++ value +++ [0.22 0.2  0.2  0.19 0.19]\n",
      "this was our first holiday in 2 years , so we were looking for somewhere where we can just relax on a beautiful beach or by the pool\n",
      "          +++ value +++ [0.21 0.2  0.19 0.21 0.19]\n",
      "we did not expect much with regards to the food after having read poor reviews and to be honest some of them had us a little worried\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "i am pleased to say that excellence punta cana exceeded our expectations by a mile\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "for the first couple of day , i just walked around with my mouth open\n",
      "          +++ clean +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the place is absolutely breathtaking\n",
      "          +++ clean +++ [0.19 0.19 0.2  0.22 0.2 ]\n",
      "we loved our room (3101) , clean , spacious and quiet\n",
      "          +++ location +++ [0.19 0.18 0.26 0.18 0.2 ]\n",
      "the beach is beautiful , the sea is so much fun to swim in ( we both like the waves\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      ") , the pool is huge and sparkling clean with plenty of beds/loungers\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      "as i said we did not expect much and we 're not sure why people are complaining so much ( bearing in mind that we are very fussy eaters )\n",
      "          +++ value +++ [0.21 0.19 0.2  0.2  0.19]\n",
      "for a start it is an all - inclusive resort not a michelin - star restaurant\n",
      "          +++ value +++ [0.23 0.2  0.19 0.19 0.19]\n",
      "the choices were plentiful and the food was fresh , well presented and well cooked\n",
      "          +++ value +++ [0.21 0.2  0.2  0.19 0.21]\n",
      "we were there for 2 weeks and could have probably stayed another 2 weeks without getting bored of the food\n",
      "          +++ value +++ [0.23 0.2  0.19 0.19 0.19]\n",
      "the entertainment team do a great job , the bar staff are fun , beach  pool servers always around , the waiting staff friendly and attentive , the housekeeping ladies kept our room spotlessly clean and tidy\n",
      "          +++ service +++ [0.19 0.18 0.2  0.19 0.24]\n",
      "i 'm not going to single out any staff in particular because i think every single one played a big part in having made our stay so enjoyable , and in particular those behind the scenes who works so hard but do not get the tips or the credit they derserve\n",
      "          +++ service +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "we 've had a wonderful time and although we usually prefer not to return to the same place ( so as not to spoil it ) , excellence punta cana is one of those places that we would certainly not hesitate going back to\n",
      "          +++ value +++ [0.24 0.21 0.18 0.18 0.19]\n",
      "in fact we would have quite liked to move in\n",
      "          +++ value +++ [0.23 0.2  0.19 0.19 0.19]\n",
      "there are so much more to say , but i could go on forever\n",
      "          +++ value +++ [0.23 0.2  0.19 0.19 0.19]\n",
      "this place is a taste of heaven , go with the right attitude and you will not be dissapointed\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.2 ]\n",
      "everything that they offer must surely satisfy 90% of guests\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.2 ]\n",
      "as for the other 10% , there is no satisfaction no matter what you do\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "they should probably rather stay at home and certainly not travel to a 3rd world country\n",
      "          +++ value +++ [0.22 0.2  0.2  0.2  0.19]\n",
      "ps : 1\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      "enjoy the ride from the airport (50minutes ) , its an excursion on its own , saves having to pay to go on one in precious holiday time\n",
      "          +++ clean +++ [0.2  0.2  0.19 0.22 0.2 ]\n",
      "find the soft serve machine by cafe kafe bar\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      "nancy 's shop is the best , she wo not rip you off\n",
      "          +++ clean +++ [0.2  0.2  0.2  0.21 0.2 ]\n",
      "( with the other vendors everything always starts at $200, and you can get it down to $15, nancy gives a reasonable price from the start )4\n",
      "          +++ clean +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "go to the sports bar for ice cold water\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([[1.3482e-09, 5.5344e-15, 2.4204e-15, 3.8888e-11, 1.0000e+00],\n",
      "        [2.2262e-07, 2.2185e-10, 1.8590e-12, 3.9325e-09, 1.0000e+00],\n",
      "        [1.7217e-08, 2.9531e-09, 2.4548e-09, 2.3951e-09, 1.0000e+00],\n",
      "        [2.0244e-08, 9.9657e-07, 4.6473e-08, 2.5267e-08, 1.0000e+00],\n",
      "        [6.9858e-06, 2.6199e-08, 1.4574e-07, 5.6835e-09, 9.9999e-01],\n",
      "        [1.7755e-07, 9.0359e-10, 3.1757e-10, 1.1514e-08, 1.0000e+00]])\n",
      "doc:\n",
      "everything is excellent at excellence punta cana we just got back from our honeymoon\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "we had an amazing time at excellence punta cana\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.2 ]\n",
      "we felt we were in paradise\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "the hotel and service was excellent\n",
      "          +++ service +++ [0.2  0.2  0.2  0.2  0.21]\n",
      "the whole staff was very friendly and so polite\n",
      "          +++ service +++ [0.2  0.2  0.2  0.2  0.21]\n",
      "my husband and i did not want to leave the resort\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "they made us feel very special\n",
      "          +++ service +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "we have never experience a trip like this one\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "we loved it so much we are gathering a group of couple to go again early next summer\n",
      "          +++ value +++ [0.22 0.2  0.19 0.19 0.19]\n",
      "we never experience a problem while our stay\n",
      "          +++ value +++ [0.21 0.2  0.2  0.19 0.19]\n",
      "all of our belongings we safe , nothing was missing\n",
      "          +++ value +++ [0.22 0.2  0.2  0.19 0.19]\n",
      "ca not wait to go back\n",
      "          +++ value +++ [0.23 0.21 0.19 0.19 0.19]\n",
      "much love to maria isabel and carlos from excellence club concierge - - from mr\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.19]\n",
      "===========\n",
      "truth:\n",
      "[4, 4, 4, 4, 4, 4]\n",
      "prediction:\n",
      "tensor([[1.4498e-09, 8.5083e-13, 4.1866e-13, 5.5214e-08, 1.0000e+00],\n",
      "        [2.2207e-08, 6.0916e-10, 9.8872e-11, 2.4354e-06, 1.0000e+00],\n",
      "        [1.1146e-08, 1.9252e-08, 2.9254e-08, 6.0871e-07, 1.0000e+00],\n",
      "        [4.5215e-09, 4.1306e-07, 8.1024e-07, 1.8986e-05, 9.9998e-01],\n",
      "        [1.8055e-06, 1.4050e-08, 1.3543e-07, 7.4386e-08, 1.0000e+00],\n",
      "        [2.4027e-08, 2.5745e-10, 9.9839e-10, 1.1573e-07, 1.0000e+00]])\n",
      "doc:\n",
      "the pool and beach were beautiful this hotel catered to every persons needs\n",
      "          +++ value +++ [0.21 0.2  0.2  0.19 0.2 ]\n",
      "i love the fact that it is all inclusive food and alcohol\n",
      "          +++ value +++ [0.21 0.2  0.2  0.2  0.2 ]\n",
      "just because the hotel says it is all inclusive does not mean you dont have to tip\n",
      "          +++ value +++ [0.21 0.2  0.19 0.2  0.2 ]\n",
      "the people there work 12 days straight with 2 days off\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "they work up to 16 hours a day to make your experience there comfortable\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "so i made sure i tipped everyone who helped me\n",
      "          +++ service +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "it is 35 pesos to one american dollar so you can do the math\n",
      "          +++ clean +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "the people that work in entertainment , cesar , mariel , ines , whinny were all very good\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "they taught me and my husband many things about there culture such as dancing\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "also there was a bartender named juan who made the best drinks there\n",
      "          +++ value +++ [0.2 0.2 0.2 0.2 0.2]\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "asp_inc_overall = False\n",
    "if not asp_inc_overall: \n",
    "    nasp_analysis = hyper_params[\"num_aspect\"] - 1\n",
    "else:\n",
    "    nasp_analysis = hyper_params[\"num_aspect\"]\n",
    "    \n",
    "np.set_printoptions(precision=3)\n",
    "asp_name = [\"overall\", \"value\", \"room\", \"location\", \"clean\", \"service\"]\n",
    "for i in range(10):\n",
    "    print(\"truth:\")\n",
    "    print( df_test.iloc[i,0:6].values.flatten().tolist() )\n",
    "    print(\"prediction:\")\n",
    "    print( outs[i][0:6] )\n",
    "    print(\"doc:\")\n",
    "    dasp = torch.argmax(asps[i][:,0:nasp_analysis],dim=1).numpy()\n",
    "    if asp_inc_overall: dasp_noall = torch.argmax(asps[i][:,1:6],dim=1).numpy()\n",
    "    dasp_dist = torch.nn.functional.softmax(asps[i][:,0:nasp_analysis], dim=1).numpy()\n",
    "#     dasp_dist = asps[i][:,0:nasp_analysis].numpy()\n",
    "    for senti,s in enumerate(df_test.iloc[i,6]):\n",
    "        print(s)\n",
    "        if asp_inc_overall:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]] + \" +++ \" + asp_name[dasp_noall[senti]+1] + \" +++ \" + str(dasp_dist[senti]) )\n",
    "        else:\n",
    "            print(\"          +++ \"+ asp_name[dasp[senti]+1] + \" +++ \" + str( np.around(dasp_dist[senti], decimals=2) ) )\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hotel_asp(asp_pred, asp_true, asp_inc_overall):\n",
    "    asp_to_id = {\"value\":0, \"room\":1, \"location\":2, \"cleanliness\":3, \"service\":4, \"none\":-1}\n",
    "    asp_true = np.array( [asp_to_id[l] for l in asp_true] )\n",
    "    print(\"total true: \" + str(len(asp_true)) )\n",
    "    print(\"total not none: \" + str(sum(asp_true>0)) )\n",
    "    \n",
    "    asp_pred_index = []\n",
    "    if asp_inc_overall:\n",
    "        for i in range(1000):\n",
    "            asp_pred_index.append( asp_pred[i][:,1:6].numpy().argsort() )\n",
    "    else:\n",
    "        for i in range(1000):\n",
    "            asp_pred_index.append( asp_pred[i][:,0:5].numpy().argsort() )\n",
    "    asp_pred_index = np.concatenate( asp_pred_index , axis=0)\n",
    "    \n",
    "    result_index = []\n",
    "    for i,lbl in enumerate(asp_true):\n",
    "        if(lbl==-1):\n",
    "            result_index.append(-1)\n",
    "        else:\n",
    "            at = np.where(asp_pred_index[i,] == lbl)\n",
    "            result_index.append(at[0][0])\n",
    "    result_index = np.array(result_index)\n",
    "    \n",
    "    print(\"Top 1 ACC:\")\n",
    "    print( sum(result_index>=4) / sum(result_index>=0) )\n",
    "    print(\"Top 2 ACC:\")\n",
    "    print( sum(result_index>=3) / sum(result_index>=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "yifan_label = open(dataset_dir + \"test_aspect_0.yifanmarjan.aspect\", \"r\").readlines()\n",
    "yifan_label = [s.split()[0] for s in yifan_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true: 1000\n",
      "total not none: 454\n",
      "Top 1 ACC:\n",
      "0.6763005780346821\n",
      "Top 2 ACC:\n",
      "0.9152215799614644\n"
     ]
    }
   ],
   "source": [
    "eval_hotel_asp(asps, yifan_label, asp_inc_overall=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_label = open(dataset_dir + \"test_aspect_0.fan.aspect\", \"r\").readlines()\n",
    "fan_label = [s.split()[0] for s in fan_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total true: 621\n",
      "total not none: 288\n",
      "Top 1 ACC:\n",
      "0.6814159292035398\n",
      "Top 2 ACC:\n",
      "0.887905604719764\n"
     ]
    }
   ],
   "source": [
    "eval_hotel_asp(asps, fan_label, asp_inc_overall=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
